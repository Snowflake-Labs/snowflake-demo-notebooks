{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "arf7ixegn5jhpeldnvma",
   "authorId": "6841714608330",
   "authorName": "CHANINN",
   "authorEmail": "chanin.nantasenamat@snowflake.com",
   "sessionId": "39cecc10-d792-4cd7-8dae-4096a745bba0",
   "lastEditTime": 1760806281013
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7a29c7-92eb-4d12-b8cd-d81075f6e7b4",
   "metadata": {
    "name": "md_title",
    "collapsed": false
   },
   "source": "# Build Your First Machine Learning Project - Part 1 | `Data Wrangling`\n\nIn this course, we'll build our first machine learning project end-to-end in Python. \n\n### Course Content\nWe'll cover this in five separate notebooks/apps:\n1. **Data Operations** - Ingest data, data wrangling and write to Snowflake using Modin (`modin.pandas`) and Snowpark (`snowflake-snowpark-python`)\n2. **Exploratory Data Analysis (EDA)** - Explore data, summary statistics, data visualization using `Altair` and `Streamlit`\n3. **Machine learning (ML)** - Prepare data and features for build models using different ML algorithms (Logistic Regression, Random Forest and Support Vector Machine) with `scikit-learn`\n4. **Experiment Tracking** - Initiate experiment tracking when building and trying out different hyperparameters with `ExperimentTracking()` from `snowflake-ml-python`\n5. **Data App** - Build a sharable data app with `Streamlit`\n\n### What We'll Cover (in this Notebook):\n\n1. **Data Loading and Preparation** - Load the bear dataset and prepare it for analysis using Modin (`modin.pandas`) and Snowpark (`snowflake-snowpark-python`)\n2. **Basic Statistics** - Calculate and visualize summary statistics of the dataset\n3. **Feature Distribution Analysis** - Explore the distribution of individual features across different bear species with `Altair` and `Streamlit`\n4. **Correlation Analysis** - Investigate relationships between numeric features using correlation heatmaps with `Altair` and `Streamlit`\n5. **Feature Relationships** - Visualize relationships between pairs of features using interactive scatter plots with `Altair` and `Streamlit`\n6. **Categorical Analysis** - Examine the distribution of categorical features including species classification with `Altair` and `Streamlit`\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "2b7a332e-de36-413b-bd23-38d754f043b6",
   "metadata": {
    "name": "md_notebook_setup",
    "collapsed": false
   },
   "source": "# Notebook Setup"
  },
  {
   "cell_type": "markdown",
   "id": "67d6e16b-e886-4422-af18-cd4078840540",
   "metadata": {
    "name": "md_packages",
    "collapsed": false
   },
   "source": "## Install Prerequisite Libraries\n\nSnowflake Notebooks includes common Python libraries by default. To add more, use the **Packages** dropdown in the top right. \n\nLet's add the following package:\n- `modin` - Perform data operations (read/write) and wrangling just like pandas with the [Snowpark pandas API](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/modin/index)\n- `scikit-learn` - Perform data splits and build machine learning models\n\nNote: When using an AI/ML container, Snowpark and relevant machine learning packages comes pre-installed."
  },
  {
   "cell_type": "markdown",
   "id": "46b03d87-4884-4a09-9a88-fa511e50b9e5",
   "metadata": {
    "name": "md_settings",
    "collapsed": false
   },
   "source": "## Notebook Settings\n\n1. Click on the three dots on the top-right hand corner and select \"Notebook settings\"\n2. In the \"Notebook settings\" modal that appears, by default the General tab is activated, click on \"Run on container\" and under \"Compute pool\" choose a CPU compute node.\n3. From the \"Notebook settings\" modal, click on the \"External access\" tab, select a policy that allows the notebook external access (*i.e.* this will allow access to data stored on GitHub)."
  },
  {
   "cell_type": "markdown",
   "id": "fb1bbba1-0802-4b35-9921-cb899b41bdff",
   "metadata": {
    "name": "md_data_setup",
    "collapsed": false
   },
   "source": "# Data Setup\n\nIn this step, we'll perform the following:\n1. Load tabular data from a GitHub repo\n2. Create a Snowflake stage for storing image data"
  },
  {
   "cell_type": "markdown",
   "id": "41a7defc-5f38-4387-9f17-abd83c26bb11",
   "metadata": {
    "name": "md_bear_data",
    "collapsed": false
   },
   "source": "## Bear dataset\n\nThe dataset is a classic multi-class classification problem where the goal of the machine learning task is to classify each entry as belonging to one of four species that a bear belongs to based on its features.\n\nThe bear dataset is comprised of 200 bear samples and each entry is described by 6 different features pertaining to the bear's physical characteristics (also known as parameters, independent variables or X variables) and is assigned to one of four bear species (A, B, C and D).\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "623130bf-a2bc-4c09-b387-edec0b100ef7",
   "metadata": {
    "name": "md_load_data",
    "collapsed": false
   },
   "source": "## Load Data\n\nHere, we'll load in the first portion of the data set that comprises of the ID, bear species, and 6 feature columns:\n- id\n- species\n- body_mass_kg\n- shoulder_hump_height_cm\n- claw_length_cm\n- snout_length_cm\n- forearm_circumference_cm\n- ear_length_cm\n\nAs for the second portion, we'll prepare those from features that we'll extract from a collection of bear images that corresponds to each of the row IDs."
  },
  {
   "cell_type": "code",
   "id": "ec516c45-bb11-4da7-a77c-dc974486f471",
   "metadata": {
    "language": "python",
    "name": "py_load_data",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import modin.pandas as pd\nimport snowflake.snowpark.modin.plugin\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/dataprofessor/bear-dataset/refs/heads/master/bear_raw_data.csv\")\ndf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4b2f5a5f-92ed-48f2-ab70-3faa7fe7de79",
   "metadata": {
    "name": "md_load_images",
    "collapsed": false
   },
   "source": "## Load Images\n\nAs previously mentioned, each row from the data set has a unique ID for each bear along with its own corresponding image named using the ID (e.g. `GRZ_01`, `GRZ_02`, `GRZ_03`, etc.)"
  },
  {
   "cell_type": "markdown",
   "id": "1a8a8f38-9016-48a1-a5bc-6e8998b9fcd0",
   "metadata": {
    "name": "md_create_stage",
    "collapsed": false
   },
   "source": "### Create Snowflake Stage to Store Image Uploads\n\nBefore we can work with any image, we'll need to create a Snowflake stage for storing the images.\n\nWe can do this via the Snowsight UI or with a SQL statement.\n\nEssentially, this is implemented in 3 steps:\n1. Create the stage using `CREATE STAGE stage_name`\n2. Enable `DIRECTORY` so that files are shown in the stage\n3. Use server-side encryption so that we can use Cortex functions on images stored in stage"
  },
  {
   "cell_type": "markdown",
   "id": "cba1d444-028a-4521-9155-39ef87166d94",
   "metadata": {
    "name": "md_set_database",
    "collapsed": false
   },
   "source": "Before creating the stage, let's switch to our working database (here I'll use `chaninn_demo_data`; feel free to replace this with another database of your choice).\n\nI'll also specify that we'll use the `stages` schema."
  },
  {
   "cell_type": "code",
   "id": "9b376cf0-01b6-42ed-bb56-6219f09e2996",
   "metadata": {
    "language": "sql",
    "name": "sql_set_database",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "USE DATABASE chaninn_demo_data;\nUSE SCHEMA stages;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a37704b-4be8-4bfa-9097-a70eca993a12",
   "metadata": {
    "language": "sql",
    "name": "sql_create_stage"
   },
   "outputs": [],
   "source": "CREATE STAGE IF NOT EXISTS input_stage\n    DIRECTORY = ( ENABLE = true )\n    ENCRYPTION = ( TYPE = 'SNOWFLAKE_SSE' );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fb623e81-46ca-4de4-b70d-84122153f603",
   "metadata": {
    "name": "md_upload_images",
    "collapsed": false
   },
   "source": "Next, head over to the Database explorer in Snowsight and upload the 200 bear images to the `BEAR` stage located under the `CHANINN_DEMO_DATA` database and `STAGE` schema.\n\nAfterwards, head back to this notebook and run the `ls` command to query the `@bear` stage."
  },
  {
   "cell_type": "code",
   "id": "13cd20fb-be18-4b01-bc27-dc0859a88f18",
   "metadata": {
    "language": "sql",
    "name": "sql_list_stage_files",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ls @bear",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "726d70d6-89b1-4738-9bb3-861734872be4",
   "metadata": {
    "name": "md_display_image",
    "collapsed": false
   },
   "source": "## Display bear images\n\nNow that we have all of the image uploaded, let's have a look at them."
  },
  {
   "cell_type": "code",
   "id": "71460384-3f4d-489e-bea6-eb0353efae1d",
   "metadata": {
    "language": "python",
    "name": "py_display_image"
   },
   "outputs": [],
   "source": "import streamlit as st\nfrom snowflake.snowpark.context import get_active_session\n\nst.title(\"Bear species\")\n\n# Create a single row with 4 columns\ncols = st.columns(4)\n\n# Bear species and their captions\nbears = [\n    (\"ABB\", \"American Black Bear\"),\n    (\"EUR\", \"Eurasian Brown Bear\"), \n    (\"GRZ\", \"Grizzly Bear\"),\n    (\"KDK\", \"Kodiak Bear\")\n]\n\n# Display images in grid using loop\nfor col, (species, caption) in zip(cols, bears):\n    with col:\n        st.image(\n            f'https://github.com/dataprofessor/bear-dataset/blob/master/images/{species}_01.png?raw=true',\n            caption=f\"{caption} ({species}_01.png)\"\n        )\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "33228af2-76b0-4826-9186-127de28368ff",
   "metadata": {
    "name": "md_image_analysis",
    "collapsed": false
   },
   "source": "# Image Analysis\n\nWhy are we analyzing the images? As mentioned earlier on, we're going to add additional features to the dataset by analyzing the bear images to figure out the following:\n- Fur color\n- Facial profile\n- Paw pad texture\n\nThease 3 features are added to the data set loaded above in the `py_load_data` cell and stored in the `df` variable."
  },
  {
   "cell_type": "markdown",
   "id": "ec088e67-621c-456c-9a3c-216ac6ce7d68",
   "metadata": {
    "name": "md_llm_inference",
    "collapsed": false
   },
   "source": "## LLM inference on Image\n\nTo perform an LLM inference, we're performing the following 4 things:\n1. Use the `AI_COMPLETE()` SQL function to analyze the image\n2. Use `claude-3-5-sonnet` LLM model to make the inference\n3. Specify the prompt that will provide the necessary instructions on how to analyze the image\n4. Use `TO_FILE()` to specify the image file to work on, while providing the stage and file names as input parameters."
  },
  {
   "cell_type": "code",
   "id": "f8daea86-3f27-4986-9a94-d28b85de8bbd",
   "metadata": {
    "language": "sql",
    "name": "sql_analyze_single_image",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT AI_COMPLETE('claude-3-5-sonnet',\n    'What is the fur color of the bear?',\n    TO_FILE('@bear', 'ABB_01.png'));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "446d00bb-49ca-4a54-85e4-65706815b522",
   "metadata": {
    "name": "md_dynamic_queries",
    "collapsed": false
   },
   "source": "## From static to dynamic queries\n\nNow, we'll essentially do the same thing as shown above but structuring it in a such a way that will allow us to pass Python variables to the SQL query, making it more dynamic and reusable within a programmatic workflow.\n\nPractically, this will allow us to process a large set of 200 images iteratively."
  },
  {
   "cell_type": "code",
   "id": "ae741218-bc19-4ad7-aabe-204af52677f3",
   "metadata": {
    "language": "python",
    "name": "py_analyze_single_image"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nprompt = 'What is the fur color of the bear?'\nimage = 'ABB_01.png'\n\nquery = f\"\"\"\nSELECT AI_COMPLETE('claude-3-5-sonnet',\n    '{prompt}',\n    TO_FILE('@bear', '{image}'));\n\"\"\"\n\nsession.sql(query)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2440f8b-35f1-40c5-86ad-8511ef9c14e5",
   "metadata": {
    "name": "md_analyze_fur_color",
    "collapsed": false
   },
   "source": "Expanding on the above query, we'll now change the prompt to allow us to infer the bear's fur color given the bear image."
  },
  {
   "cell_type": "code",
   "id": "13650baf-7e6a-4206-9485-2ac2e77562d0",
   "metadata": {
    "language": "python",
    "name": "py_analyze_fur_color"
   },
   "outputs": [],
   "source": "prompt = \"\"\"\nAnalyze the provided image of a bear. Describe only the fur color of the bear \nby choosing the most appropriate term from the following list. The response \nshould be a single value.\n- Light Brown\n- Medium Brown\n- Blond\n- Dark Brown\n- Grizzled (A mix of colors with silver-tipped hairs)\n- Reddish Brown\n- Blackish Brown\n- Black\n- Brown\n- Cinnamon\n\"\"\"\n\nimage = 'ABB_01.png'\nquery = f\"\"\"\nSELECT AI_COMPLETE('claude-3-5-sonnet',\n    '{prompt}',\n    TO_FILE('@bear', '{image}'));\n\"\"\"\n\nsession.sql(query)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d094f33-256f-4020-9653-61c82cb20984",
   "metadata": {
    "name": "md_iterative_analysis",
    "collapsed": false
   },
   "source": "## Iterative Image Analysis\n\nHere, we'll apply Cortex AISQL to analyze the image and determine the bear's features.\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "4da5078c-59bb-40d0-88b9-e3ad5dfbfb54",
   "metadata": {
    "name": "md_fur_color_iterative",
    "collapsed": false
   },
   "source": "### Fur Color\n\nWe'll start with analyzing the fur color for all 200 images."
  },
  {
   "cell_type": "code",
   "id": "76332a84-3993-4c4d-b140-ea3fac3409be",
   "metadata": {
    "language": "python",
    "name": "py_fur_color_iterative",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\nprompt = \"\"\"\nAnalyze the provided image of a bear. Describe only the fur color of the bear\nby choosing the most appropriate term from the following list. The response\nshould be a single value.\n- Light Brown\n- Medium Brown\n- Blond\n- Dark Brown\n- Grizzled (A mix of colors with silver-tipped hairs)\n- Reddish Brown\n- Blackish Brown\n- Black\n- Brown\n- Cinnamon\n\"\"\"\n\n# Get a list of all image files in the stage\n# staged_files_df = session.sql(\"LIST @bear\").collect()\n\n# Sample the first N rows\nnrows = len(df)\nstaged_files_df = session.sql(\"LIST @bear\").collect()[:nrows]\n\n# Create a list of image filenames to iterate over\nimage_files = [row['name'] for row in staged_files_df if row['name'].lower().endswith(('.png', '.jpg', '.jpeg', '.gif'))]\n\n# Create an empty list to store the results\nresults_list = []\n\n# Loop through each image file and execute the AI function\nfor image_path in image_files:\n    # Extract just the filename from the full path\n    image_name = image_path.split('/')[-1]\n\n    # Dynamically build the query for each image\n    query = f\"\"\"\n    SELECT AI_COMPLETE('claude-3-5-sonnet',\n        '{prompt}',\n        TO_FILE('@bear', '{image_name}'));\n    \"\"\"\n\n    # Execute the query and collect the result\n    result = session.sql(query).collect()\n    \n    # Append a tuple of the filename and the result to the list\n    results_list.append((image_name, result[0][0]))\n\n    print(f\"Analysis for {image_name}: {result[0][0]}\")\n\n# You can now work with the `results_list`\nprint(\"\\n--- All Results Collected ---\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7702407b-f531-451e-aaec-3861aff5d338",
   "metadata": {
    "language": "python",
    "name": "py_display_results",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "results_list",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78399c92-cc39-4b1a-a949-128f9a5fc51d",
   "metadata": {
    "name": "md_clean_image_names",
    "collapsed": false
   },
   "source": "Let's remove `.PNG` from the image name to obtain the ID."
  },
  {
   "cell_type": "code",
   "id": "77989241-dee3-4e46-98c9-b1d74e9dd6d7",
   "metadata": {
    "language": "python",
    "name": "py_clean_image_names"
   },
   "outputs": [],
   "source": "fur_with_id = [\n    (image_name.replace('.png', ''), color)\n    for image_name, color in results_list\n]\n\nfur_with_id",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b8887e9-1294-493e-a7b2-181736517f2b",
   "metadata": {
    "name": "md_convert_to_dataframe",
    "collapsed": false
   },
   "source": "Here, we'll convert our collected fur color analysis results into a structured Snowpark DataFrame, which we'll add to the full data set later on.\n"
  },
  {
   "cell_type": "code",
   "id": "ac0d07f9-578a-418e-85a1-d410aa2288ea",
   "metadata": {
    "language": "python",
    "name": "py_convert_to_dataframe",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import types as T\n\nschema = T.StructType([T.StructField(\"id\", T.StringType()), T.StructField(\"color\", T.StringType())])\n\n# Convert the results_list to a Snowpark DataFrame\ndf_results = session.create_dataframe(fur_with_id, schema=schema)\ndf_fur = pd.DataFrame(df_results.to_pandas())\ndf_fur",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cea3f1ec-3454-4348-97f0-ce7471d57ec2",
   "metadata": {
    "name": "md_analyze_facial_profile",
    "collapsed": false
   },
   "source": "### Facial Profile\n\nNext, we'll analyze the facial profile of the bears, which is a key distinguishing feature. The facial profile can be either:\n- **Dished**: Concave profile, where the bridge of the nose dips)\n- **Straight**: Flat profile, with no dip from the forehead to the nose)"
  },
  {
   "cell_type": "code",
   "id": "80409a89-8ce4-48d5-8f08-fda603f8473f",
   "metadata": {
    "language": "python",
    "name": "py_analyze_facial_profile"
   },
   "outputs": [],
   "source": "# Define the prompt for facial profile analysis\nprompt = \"\"\"\nAnalyze the provided image of a bear. Describe only the facial profile of the bear. \nThe response must be one of the following two values as a single word with no explanation:\n- Dished (Concave profile, where the bridge of the nose dips)\n- Straight (Flat profile, with no dip from the forehead to the nose)\n\"\"\"\n\n# Get a list of first N image files (for testing)\nstaged_files_df = session.sql(\"LIST @bear\").collect()[:nrows]\n\n# Create a list of image filenames\nimage_files = [row['name'] for row in staged_files_df if row['name'].lower().endswith(('.png', '.jpg', '.jpeg', '.gif'))]\n\n# Create an empty list to store results\nresults_list = []\n\n# Process each image\nfor image_path in image_files:\n    image_name = image_path.split('/')[-1]\n    \n    query = f\"\"\"\n    SELECT AI_COMPLETE('claude-3-5-sonnet',\n        '{prompt}',\n        TO_FILE('@bear', '{image_name}'));\n    \"\"\"\n    \n    result = session.sql(query).collect()\n    # Extract ID by removing .png and store with result\n    id_value = image_name.replace('.png', '')\n    results_list.append((id_value, result[0][0]))\n    print(f\"Analysis for {image_name}: {result[0][0]}\")\n\n# Create Snowpark DataFrame with results\nschema = T.StructType([\n    T.StructField(\"ID\", T.StringType()), \n    T.StructField(\"FACIAL_PROFILE\", T.StringType())\n])\ndf_results = session.create_dataframe(results_list, schema=schema)\n\ndf_facial_profile = pd.DataFrame(df_results.to_pandas())\ndf_facial_profile",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c5974d98-05de-40b1-9a09-5739ca167a1f",
   "metadata": {
    "name": "md_analyze_paw_pad",
    "collapsed": false
   },
   "source": "### Paw Pad Texture\n\nNext, we'll analyze the texture of the bears' paw pads, which is another distinguishing characteristic. The paw pad texture can be either:\n- **Smooth**: Less textured and relatively flat, for walking\n- **Rough**: More textured and grooved, for gripping and climbing"
  },
  {
   "cell_type": "code",
   "id": "5a1e094b-336b-4cd8-9634-2ee0fc89edcf",
   "metadata": {
    "language": "python",
    "name": "py_analyze_paw_pad"
   },
   "outputs": [],
   "source": "# Define the prompt for paw pad texture analysis\nprompt = \"\"\"\nAnalyze the provided image of a bear. Describe only the paw pad texture of the bear. \nThe response must be one of the following two values as a single word with no explanation:\n- Smooth (Less textured and relatively flat, for walking)\n- Rough (More textured and grooved, for gripping and climbing)\n\"\"\"\n\n# Get a list of first N image files (for testing)\nstaged_files_df = session.sql(\"LIST @bear\").collect()[:nrows]\n\n# Create a list of image filenames\nimage_files = [row['name'] for row in staged_files_df if row['name'].lower().endswith(('.png', '.jpg', '.jpeg', '.gif'))]\n\n# Create an empty list to store results\nresults_list = []\n\n# Process each image\nfor image_path in image_files:\n    image_name = image_path.split('/')[-1]\n    \n    query = f\"\"\"\n    SELECT AI_COMPLETE('claude-3-5-sonnet',\n        '{prompt}',\n        TO_FILE('@bear', '{image_name}'));\n    \"\"\"\n    \n    result = session.sql(query).collect()\n    # Extract ID by removing .png and store with result\n    id_value = image_name.replace('.png', '')\n    results_list.append((id_value, result[0][0]))\n    print(f\"Analysis for {image_name}: {result[0][0]}\")\n\n# Create Snowpark DataFrame with results\nschema = T.StructType([\n    T.StructField(\"ID\", T.StringType()), \n    T.StructField(\"Paw_Pad_Texture\", T.StringType())\n])\ndf_results = session.create_dataframe(results_list, schema=schema)\n\ndf_paw_pad = pd.DataFrame(df_results.to_pandas())\ndf_paw_pad",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d348f459-ed3f-4cbd-a1e2-f4a4170dc695",
   "metadata": {
    "name": "md_data_operations",
    "collapsed": false
   },
   "source": "# Data Operations\n\nIn this section, we'll perform essential data operations to:\n- Combine the extracted features (fur color, facial profile, and paw pad texture) with the original dataset \n- Write the final dataset to a Snowflake table\n"
  },
  {
   "cell_type": "code",
   "id": "a1b3c87a-6a09-4b78-b561-187370388659",
   "metadata": {
    "language": "python",
    "name": "py_categorical_columns"
   },
   "outputs": [],
   "source": "# Read categorical columns\nimport modin.pandas as pd\nimport snowflake.snowpark.modin.plugin\n\n# Load the categorical feature data from CSV files\ndf_fur_color = pd.read_csv(\"https://raw.githubusercontent.com/dataprofessor/bear-dataset/refs/heads/master/fur_color.csv\")\ndf_facial_profile = pd.read_csv(\"https://raw.githubusercontent.com/dataprofessor/bear-dataset/refs/heads/master/facial_profile.csv\")\ndf_paw_pad = pd.read_csv(\"https://raw.githubusercontent.com/dataprofessor/bear-dataset/refs/heads/master/paw_pad_texture.csv\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "31830327-f269-44c7-931e-ebbbb3964571",
   "metadata": {
    "name": "md_combine_features",
    "collapsed": false
   },
   "source": "## Combining Features\n\nNow that we have extracted all three features (fur color, facial profile, and paw pad texture) from the bear images, let's combine them with our original dataset. \n\nThe final combined dataset will include:\n\n- Original physical measurements (body mass, shoulder hump height, etc.)\n- Fur color analysis\n- Facial profile classification\n- Paw pad texture assessment\n\nThis comprehensive dataset will give us a more complete picture of each bear's characteristics for our analysis.\n"
  },
  {
   "cell_type": "code",
   "id": "36e31897-4d61-4a33-a616-cf07c26243a4",
   "metadata": {
    "language": "python",
    "name": "py_combine_features"
   },
   "outputs": [],
   "source": "# Combining df_fur, df_facial_profile and df_paw_pad to df\n# Standardize column names to match\ndf['id'] = df['id'].str.upper()  # Ensure IDs are in uppercase\ndf_fur_color['id'] = df_fur_color['id'].str.upper()\ndf_facial_profile['id'] = df_facial_profile['id'].str.upper()\ndf_paw_pad['id'] = df_paw_pad['id'].str.upper()\n\n# Perform sequential merges to combine all features using proper indexing\ndf_combined = df.merge(df_fur_color, on='id', how='inner')\ndf_combined = df_combined.merge(df_facial_profile, on='id', how='inner')\ndf_combined = df_combined.merge(df_paw_pad, on='id', how='inner')\n\n# Display the combined DataFrame\ndf_combined",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae42bfbd-bbe4-4082-b4c4-1d344d3d97e9",
   "metadata": {
    "name": "md_write_data",
    "collapsed": false
   },
   "source": "## Write data to a database table"
  },
  {
   "cell_type": "markdown",
   "id": "7a4520f2-864f-4ea6-a9e4-2f51617417fb",
   "metadata": {
    "name": "md_db_schema",
    "collapsed": false
   },
   "source": "### Determine current database and schema\n\nBut before we write to a Snowflake database table, let's figure out the current location where this notebook is located, which in turn is where are database table will reside in."
  },
  {
   "cell_type": "code",
   "id": "86228219-bd35-47ab-a47b-6db331080d6d",
   "metadata": {
    "language": "sql",
    "name": "sql_db_schema"
   },
   "outputs": [],
   "source": "SELECT CURRENT_DATABASE(), CURRENT_SCHEMA();",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "083b6ab0-9919-493a-a778-8a70499511cc",
   "metadata": {
    "language": "sql",
    "name": "sql_set_db_schema"
   },
   "outputs": [],
   "source": "USE DATABASE chaninn_demo_data;\nUSE SCHEMA public;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "268c13f1-0614-4acb-affa-b4978262fc85",
   "metadata": {
    "language": "python",
    "name": "py_write_data",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df_combined.to_snowflake(\n    \"BEAR\",\n    if_exists=\"replace\",\n    index=False\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c7812b6e-8bea-487f-a135-03a4df9a6b1c",
   "metadata": {
    "name": "md_query_data",
    "collapsed": false
   },
   "source": "## Query data from table"
  },
  {
   "cell_type": "code",
   "id": "676033c3-95e6-40d0-b661-f0ab9240402d",
   "metadata": {
    "language": "sql",
    "name": "sqL_query_data"
   },
   "outputs": [],
   "source": "SELECT * FROM CHANINN_DEMO_DATA.PUBLIC.BEAR;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a3ea6562-fdad-490e-af12-2da61f65901f",
   "metadata": {
    "name": "md_resources",
    "collapsed": false
   },
   "source": "# Resources\nIf you'd like to take a deeper dive into Snowpark pandas:\n- [pandas on Snowflake](https://docs.snowflake.com/en/developer-guide/snowpark/python/pandas-on-snowflake)\n- [Snowpark pandas API](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/modin/index)\n- [Snowflake Cortex AISQL](https://docs.snowflake.com/user-guide/snowflake-cortex/aisql)\n- [YouTube Playlist on Snowflake Notebooks](https://www.youtube.com/watch?v=YB1B6vcMaGE&list=PLavJpcg8cl1Efw8x_fBKmfA2AMwjUaeBI)"
  }
 ]
}