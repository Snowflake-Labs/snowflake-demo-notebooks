{
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "lastEditStatus": {
      "notebookId": "ttiamxck5hsih3j2t2yy",
      "authorId": "6841714608330",
      "authorName": "CHANINN",
      "authorEmail": "chanin.nantasenamat@snowflake.com",
      "sessionId": "e5a7616c-7dc6-4447-9aeb-83ca65f30652",
      "lastEditTime": 1760806292188
    }
  },
  "nbformat_minor": 2,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "md_title",
        "collapsed": false,
        "codeCollapsed": false
      },
      "source": "# Build Your First Machine Learning Project - Part 3 | `Machine Learning Algorithms`\n\nIn this notebook, we'll prepare the Bear data set for machine learning model building.\n\n### What We'll Cover:\n\n1. **Data Loading** - Load the bear dataset using Snowpark (`snowflake-snowpark-python`)\n2. **Data Preparation** - Scale features and prepare data for model training using `scikit-learn`\n3. **Model Training** - Train multiple machine learning models using `scikit-learn`:\n   - Logistic Regression (`LogisticRegression`)\n   - Random Forest (`RandomForestClassifier`)\n   - Support Vector Machine (`SVC`)\n4. **Performance Comparison** - Compare models using accuracy and MCC metric (`scikit-learn`)\n5. **Model Interpretability** - Analyze feature importance and model coefficients to understand predictions (`pandas` built-in charts that is powered by `matplotlib`)\n",
      "id": "ce110000-1111-2222-3333-ffffff000000"
    },
    {
      "cell_type": "markdown",
      "id": "76e9f814-8b1a-49d3-936a-acdb445b0035",
      "metadata": {
        "name": "md_notebook_setup",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# Notebook Setup"
    },
    {
      "cell_type": "markdown",
      "id": "656cd9d2-0ab9-416f-afa1-d58d1ae3aeb4",
      "metadata": {
        "name": "md_settings",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Notebook Settings\n\n1. Click on the three dots on the top-right hand corner and select \"Notebook settings\"\n2. In the \"Notebook settings\" modal that appears, by default the General tab is activated, click on \"Run on container\" and under \"Compute pool\" choose a CPU compute node.\n3. From the \"Notebook settings\" modal, click on the \"External access\" tab, select a policy that allows the notebook external access (i.e. this will allow access to data stored on GitHub)."
    },
    {
      "cell_type": "markdown",
      "id": "4bd53bfa-0dd9-42b7-8d28-fa18d37a899f",
      "metadata": {
        "name": "md_packages",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Install Prerequisite Libraries\n\nSnowflake Notebooks includes common Python libraries by default. To add more, use the **Packages** dropdown in the top right. \n\nLet's add the following package:\n- `modin` - Perform data operations (read/write) and wrangling just like pandas with the [Snowpark pandas API](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/modin/index)\n- `scikit-learn` - Perform data splits and build machine learning models\n- `snowflake-ml-python` - a collection of ML functionalities from Snowflake. Here, we'll use model metrics logging functionality.\n\nNote: When using an AI/ML container, Snowpark and relevant machine learning packages comes pre-installed."
    },
    {
      "cell_type": "code",
      "id": "96b3fcdd-13c8-4ad7-9204-189399b0222c",
      "metadata": {
        "language": "python",
        "name": "py_packages"
      },
      "outputs": [],
      "source": "! pip install snowflake-ml-python",
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "md_session",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## 1. Establish Snowflake Connection\n\nWe'll start by getting an active session via the `get_active_session()` method.",
      "id": "ce110000-1111-2222-3333-ffffff000001"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "name": "py_session",
        "language": "python"
      },
      "outputs": [],
      "source": "# Get active Snowflake session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nprint(f\"âœ… Connected using active Snowflake session!\")",
      "id": "ce110000-1111-2222-3333-ffffff000002"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "md_data_operations",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## 2. Data Operations\n\nIn this section, we'll proceed to loading, preparing the features/class, explore missing data and data splitting.",
      "id": "ce110000-1111-2222-3333-ffffff000003"
    },
    {
      "cell_type": "markdown",
      "id": "91d1d3b7-d04d-456a-bb9d-bbbb483c3cd5",
      "metadata": {
        "name": "md_load_data",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### 2.1. Load Data\n\nData is read from the `BEAR` table stored in Snowflake via the `read_snowflake()` method."
    },
    {
      "cell_type": "code",
      "id": "ee88a4f6-9c58-4ba6-9665-7fdf1552d5b6",
      "metadata": {
        "language": "python",
        "name": "py_load_data"
      },
      "outputs": [],
      "source": "from snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Read data from Snowflake table using Snowpark\ndf_snowpark = session.table(\"CHANINN_DEMO_DATA.PUBLIC.BEAR\")\n\n# Convert to pandas for compatibility with visualization libraries\nbear_df = df_snowpark.to_pandas()\nbear_df",
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "md_prepare_data",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### 2.2. Prepare features and class\n\nThe DataFrame is separate into 2. Features are assigned to the `X` variable while the class is assigned to `y`.",
      "id": "ce110000-1111-2222-3333-ffffff000005"
    },
    {
      "cell_type": "code",
      "id": "4cce8f1c-60a9-4e35-aff8-bbad6de198b2",
      "metadata": {
        "language": "python",
        "name": "py_prepare_data"
      },
      "outputs": [],
      "source": "X = bear_df.drop(columns=['species', 'id'])\ny = bear_df['species']",
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "id": "cedbba72-f144-42d2-97b1-2e4be15a0a47",
      "metadata": {
        "name": "md_missing_data",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### 2.3. Check for Missing data"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "name": "py_missing_data",
        "language": "python",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "# Data quality checks\nmissing_features = X.isnull().sum().sum()\nmissing_target = y.isnull().sum()\n\nprint(f\"\\nðŸ” Data Quality:\")\nprint(f\"   Missing feature values: {missing_features}\")\nprint(f\"   Missing target values: {missing_target}\")",
      "id": "ce110000-1111-2222-3333-ffffff000006"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "md_data_splitting",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### 2.4. Data Splitting\nThe data is separated to Training-Testing sets using 80/20 ratio using `scikit-learn`:\n- 80% is used as the **Training set** - used to train an ML model\n- 20% is used as the **Testing set** - used as a test for the ML model",
      "id": "ce110000-1111-2222-3333-ffffff000007"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "name": "py_data_splitting",
        "language": "python"
      },
      "outputs": [],
      "source": "# Import scikit-learn modules at first use\nfrom sklearn.model_selection import train_test_split\n\n# Split data using scikit-learn (recommended by Snowflake for ML operations)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2, \n    random_state=42, \n    stratify=y  # Maintain target distribution\n)\n\nprint(\"âœ… Data splitting completed!\")\nprint('-' * 35) \n\nprint(\"ðŸ“Š Data Split Summary:\")\nprint(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"Testing set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"Number of features: {X_train.shape[1]}\")\nprint('-' * 35) \n\n# Check class distribution in splits\nprint(\"\\nðŸŽ¯ Class Distribution:\")\nprint(\"Training set:\", y_train.value_counts().sort_index().to_dict())\nprint(\"Testing set:\", y_test.value_counts().sort_index().to_dict())\nprint('-' * 35) ",
      "id": "ce110000-1111-2222-3333-ffffff000008"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "md_feature_scaling",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### 2.5. Feature Scaling\n\nFeature scaling is a data preprocessing technique used to standardize the range of independent variables or features of data. This helps to ensure that features with larger value ranges (e.g. one variable can have a range of 10,000 to 1,000,000 while others could be 0.1 to 0.8) do not disproportionately influence the model's learning process.\n\nHere, we're using `scikit-learn` to perform feature scaling by standardizing all variables by mean centering (mean = 0) unit variance (SD = 1).",
      "id": "ce110000-1111-2222-3333-ffffff000009"
    },
    {
      "cell_type": "code",
      "id": "71bbf10f-ca65-4cb5-b360-0eced92f6c63",
      "metadata": {
        "language": "python",
        "name": "py_feature_scaling"
      },
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Identify numerical and categorical columns\nnumerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\ncategorical_features = X_train.select_dtypes(include=['object']).columns\n\nprint(\"Numerical features:\", numerical_features.tolist())\nprint(\"Categorical features:\", categorical_features.tolist())\n\n# Scale numerical features\nscaler = StandardScaler()\nX_train_scaled_num = scaler.fit_transform(X_train[numerical_features])\nX_test_scaled_num = scaler.transform(X_test[numerical_features])\n\n# Convert categorical features using one-hot encoding\nonehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nX_train_scaled_cat = onehot.fit_transform(X_train[categorical_features])\nX_test_scaled_cat = onehot.transform(X_test[categorical_features])\n\n# Get feature names after one-hot encoding\ncat_feature_names = onehot.get_feature_names_out(categorical_features)\n\n# Combine numerical and categorical features\nX_train_scaled = np.hstack([X_train_scaled_num, X_train_scaled_cat])\nX_test_scaled = np.hstack([X_test_scaled_num, X_test_scaled_cat])\n\n# Convert to DataFrame with proper column names\nall_feature_names = list(numerical_features) + list(cat_feature_names)\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=all_feature_names, index=X_train.index)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=all_feature_names, index=X_test.index)\n\nprint(\"\\nâœ… Features scaling completed!\")\nprint('-' * 35) \n\nprint(\"\\nðŸ“Š Scaled Data Dimension:\")\nprint(f\"Scaled training features shape: {X_train_scaled.shape}\")\nprint(f\"Scaled testing features shape: {X_test_scaled.shape}\")\nprint('-' * 35) \n\n# Show scaling effect for numerical features\nif len(numerical_features) > 0:\n    first_num_feature = numerical_features[0]\n    print(\"\\nðŸ“Š Scaling Effect (first numerical feature):\")\n    print(f\"Original {first_num_feature}: mean={X_train[first_num_feature].mean():.3f}, std={X_train[first_num_feature].std():.3f}\")\n    print(f\"Scaled {first_num_feature}: mean={X_train_scaled[first_num_feature].mean():.3f}, std={X_train_scaled[first_num_feature].std():.3f}\")\nprint('-' * 35)\n",
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "md_model_training",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## 3. Machine Learning Model Training\nNow that we have the scaled features, we'll build ML models using `scikit-learn`.",
      "id": "ce110000-1111-2222-3333-ffffff000011"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "md_logreg",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### 3.1. Logistic Regression\n",
      "id": "ce110000-1111-2222-3333-ffffff000012"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "name": "py_logreg",
        "language": "python"
      },
      "outputs": [],
      "source": "# Import logistic model and classification metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, matthews_corrcoef\nimport numpy as np\n\n# Logistic Regression using scikit-learn\nprint(\"ðŸ”§ Training Logistic Regression Model...\")\n\nlog_reg_model = LogisticRegression(random_state=42) # random_state for reproducibility\nlog_reg_model.fit(X_train_scaled, y_train)\n\n# Make predictions (outputs class labels directly)\nlog_reg_train_pred = log_reg_model.predict(X_train_scaled)\nlog_reg_test_pred = log_reg_model.predict(X_test_scaled)\n\n# Calculate classification metrics\nlogreg_train_acc = accuracy_score(y_train, log_reg_train_pred)\nlogreg_test_acc = accuracy_score(y_test, log_reg_test_pred)\nlogreg_train_mcc = matthews_corrcoef(y_train, log_reg_train_pred)\nlogreg_test_mcc = matthews_corrcoef(y_test, log_reg_test_pred)\n\ntest_class_report = classification_report(y_test, log_reg_test_pred)\n\nprint(\"âœ… Logistic Regression model trained!\")\nprint('-' * 35)\n\nprint(f\"ðŸ“Š Logistic Regression Results:\")\nprint(f\"   Training Accuracy: {logreg_train_acc:.4f}\")\nprint(f\"   Testing Accuracy:  {logreg_test_acc:.4f}\")\nprint(f\"   Training MCC:      {logreg_train_mcc:.4f}\")\nprint(f\"   Testing MCC:       {logreg_test_mcc:.4f}\")\nprint('-' * 35)\n\nprint(\"\\nClassification Report (Test Set):\")\nprint(test_class_report)\nprint('-' * 35)",
      "id": "ce110000-1111-2222-3333-ffffff000013"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "md_rf",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### 3.2. Random Forest Classifier\n",
      "id": "ce110000-1111-2222-3333-ffffff000014"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "name": "py_rf",
        "language": "python"
      },
      "outputs": [],
      "source": "# Import ensemble methods and detailed metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, matthews_corrcoef, classification_report\nimport pandas as pd\n\n# Random Forest using scikit-learn\nprint(\"ðŸŒ² Training Random Forest Classifier...\")\nprint('-' * 35)\n\nrf_model = RandomForestClassifier(random_state=42, n_jobs=-1) # Use parallel processing\nrf_model.fit(X_train_scaled, y_train)\n\n# Make predictions\nrf_train_pred = rf_model.predict(X_train_scaled)\nrf_test_pred = rf_model.predict(X_test_scaled)\n\n# Calculate comprehensive metrics\nrf_train_acc = accuracy_score(y_train, rf_train_pred)\nrf_test_acc = accuracy_score(y_test, rf_test_pred)\nrf_train_mcc = matthews_corrcoef(y_train, rf_train_pred)\nrf_test_mcc = matthews_corrcoef(y_test, rf_test_pred)\ntest_class_report = classification_report(y_test, rf_test_pred)\n\nprint(\"âœ… Random Forest model trained!\")\nprint('-' * 35)\n\nprint(f\"ðŸ“Š Random Forest Results:\")\nprint(f\"   Training Accuracy: {rf_train_acc:.4f}\")\nprint(f\"   Testing Accuracy:  {rf_test_acc:.4f}\")\nprint(f\"   Training MCC:      {rf_train_mcc:.4f}\")\nprint(f\"   Testing MCC:       {rf_test_mcc:.4f}\")\nprint(\"\\nClassification Report (Test Set):\")\nprint(test_class_report)\nprint('-' * 35)",
      "id": "ce110000-1111-2222-3333-ffffff000015"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "md_svm",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### 3.3. Support Vector Machine (SVM)\n",
      "id": "ce110000-1111-2222-3333-ffffff000016"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "name": "py_svm",
        "language": "python"
      },
      "outputs": [],
      "source": "# Import SVM and detailed metrics\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, matthews_corrcoef, classification_report\n\n# Support Vector Machine using scikit-learn\nprint(\"ðŸ¤– Training Support Vector Machine...\")\nprint('-' * 35)\n\nsvm_model = SVC(random_state=42)\nsvm_model.fit(X_train_scaled, y_train)\n\n# Make predictions\nsvm_train_pred = svm_model.predict(X_train_scaled)\nsvm_test_pred = svm_model.predict(X_test_scaled)\n\n# Calculate comprehensive metrics\nsvm_train_acc = accuracy_score(y_train, svm_train_pred)\nsvm_test_acc = accuracy_score(y_test, svm_test_pred)\nsvm_train_mcc = matthews_corrcoef(y_train, svm_train_pred)\nsvm_test_mcc = matthews_corrcoef(y_test, svm_test_pred)\ntest_class_report = classification_report(y_test, svm_test_pred)\n\nprint(\"âœ… SVM model trained!\")\nprint('-' * 35)\n\nprint(f\"ðŸ“Š SVM Results:\")\nprint(f\"   Training Accuracy: {svm_train_acc:.4f}\")\nprint(f\"   Testing Accuracy:  {svm_test_acc:.4f}\")\nprint(f\"   Training MCC:      {svm_train_mcc:.4f}\")\nprint(f\"   Testing MCC:       {svm_test_mcc:.4f}\")\nprint(\"\\nClassification Report (Test Set):\")\nprint(test_class_report)\nprint('-' * 35)\n",
      "id": "ce110000-1111-2222-3333-ffffff000017"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "md_benchmark",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## 4. Benchmarking of Machine Learning Algorithms\nBenchmarking essentially means that we're comparing various ML algorithms to see which performs the best and/or are most suitable for our use case.\n\nIn selecting the best ML algorithm to use, we want an algorithm that can generalize well on new, unseen data and one that can provide actionable insights.\n1. Model overfitting: the former point on generalizing well on new, unseen data could be evaluated by the degree at which the algorithm overfits the data\n2. Model interpretability: the latter point on actionable insights can be gained by analyzing important features that contributes to the model's prediction\n",
      "id": "ce110000-1111-2222-3333-ffffff000018"
    },
    {
      "cell_type": "markdown",
      "id": "c6e17b5e-0698-4a5c-aeb8-423b23d8552e",
      "metadata": {
        "name": "md_overfitting",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### 4.1. Assessing Overfitting\n\nOverfitting is a measure of how much better a model performs on the data it was trained on compared to new, unseen data, indicating it has memorized noise instead of learning a general pattern.\n\n> $$ Over fitting = Training Performance - Testing Performance $$\n\nThis formula calculates the performance drop when your model moves from familiar training data to new, unseen testing data.\n\n- A big difference means the model is overfitted: It just memorized the training examples instead of learning the actual patterns, so it fails on new data. ðŸ‘Ž\n- A small difference is good: This means that the model generalizes well. ðŸ‘"
    },
    {
      "cell_type": "code",
      "id": "f2b883db-9db1-4bd1-8db9-214aa07bc83d",
      "metadata": {
        "language": "python",
        "name": "py_overfitting",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "# Import Altair at first use\nimport altair as alt\n\n# Configure Altair for interactive visualizations\nalt.data_transformers.enable('json')\nalt.theme.enable('opaque')\n\n# Compare all models\nmodel_acc = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forest', 'SVM'],\n    'Training_Accuracy': [logreg_train_acc, rf_train_acc, svm_train_acc],\n    'Testing_Accuracy': [logreg_test_acc, rf_test_acc, svm_test_acc]\n})\n\nmodel_acc['Overfitting'] = model_acc['Training_Accuracy'] - model_acc['Testing_Accuracy']\n\nmodel_mcc = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forest', 'SVM'],\n    'Training_MCC': [logreg_train_mcc, rf_train_mcc, svm_train_mcc],\n    'Testing_MCC': [logreg_test_mcc, rf_test_mcc, svm_test_mcc]\n})\n\nmodel_mcc['Overfitting'] = model_mcc['Training_MCC'] - model_mcc['Testing_MCC']\n\n\nprint(\"ðŸ“Š Model Comparison:\")\nprint('-' * 35)\n\nprint(\"Accuracy:\")\nprint(model_acc.round(4))\nprint('-' * 35)\n\nprint(\"MCC:\")\nprint(model_mcc.round(4))\n",
      "execution_count": 31
    },
    {
      "cell_type": "markdown",
      "id": "7783bfdf-489a-4573-8c26-5346fbe1f5bc",
      "metadata": {
        "name": "md_interpretability",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### 4.2. Model interpretability\n\nInterpretable ML models are those that provide the variable coefficients that directly dictates the relative degree at which it influences the target `y` values.\n\nIn linear models this may be summarized in the following equation:\n\n> $$y = m_1x_1 + m_2x_2 + ... + b$$\n\nwhere $$y$$ is the target or dependent variable, $$m_n$$ are the variable coefficients, $$x_n$$ are the features or independent variables and $$b$$ is the baseline value.\n\nIn essence, $$m_n$$ coefficients are direct measure of their influence on the prediction of $$y$$, where larger absolute coefficient value means that it has stronger impact on the prediction of $$y$$.\n\n"
    },
    {
      "cell_type": "markdown",
      "id": "4f6c3893-6d2b-4512-a83b-b9706b090fb5",
      "metadata": {
        "name": "md_interpretability_logreg",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "#### 4.2.1. Interpreting Logistic regression models"
    },
    {
      "cell_type": "code",
      "id": "f7895b1a-4766-439e-acc4-ed91a9b34449",
      "metadata": {
        "language": "python",
        "name": "py_interpretability_logreg"
      },
      "outputs": [],
      "source": "# Generated by Snowflake Copilot\n# Get coefficients from the model\ncoefficients = log_reg_model.coef_[0]\n\n# Create a DataFrame using the transformed feature names\nlogreg_feature_importance = pd.DataFrame({\n    'feature': all_feature_names,  # Using all_feature_names from the py_feature_scaling cell\n    'coefficient': coefficients\n})\n\n# Calculate the absolute value of the coefficients to use as 'importance'\nlogreg_feature_importance['abs_coefficient'] = np.abs(logreg_feature_importance['coefficient'])\n\n# Sort the features by importance in descending order\nlogreg_feature_importance = logreg_feature_importance.sort_values('abs_coefficient', ascending=False)\n\n# Print the results\nprint(\"âœ¨ Top 5 Most Important Features (Logistic Regression):\")\nprint(logreg_feature_importance[['feature', 'coefficient', 'abs_coefficient']].head())\n",
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "id": "88d352f9-c14b-44c5-8ac0-04c5769485b0",
      "metadata": {
        "language": "python",
        "name": "py_importance_plot_logreg",
        "codeCollapsed": false,
        "title": "py_importance_plot_logreg"
      },
      "outputs": [],
      "source": "top_n = 5\nchart_data = logreg_feature_importance.head(top_n).set_index('feature')\n\nchart_data['abs_coefficient'].sort_values(ascending=True).plot.barh(\n    title='Top 5 Feature Importance (Logistic Regression)',\n    xlabel='Importance',\n    ylabel='Features',\n    figsize=(10, 6)\n)",
      "execution_count": 47
    },
    {
      "id": "885a9380-3dd3-4be7-923a-864cee58fd07",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": ""
    },
    {
      "cell_type": "markdown",
      "id": "28c49382-1626-464b-95b8-0d456b59bbf4",
      "metadata": {
        "name": "md_interpretability_rf",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "#### 4.2.2. Interpreting Random Forest models"
    },
    {
      "cell_type": "code",
      "id": "7001b41c-22df-4982-a34f-a215231bc76f",
      "metadata": {
        "language": "python",
        "name": "py_interpretability_rf",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "# Feature importance analysis with Random forest\nrf_feature_importance = pd.DataFrame({\n    'feature': all_feature_names, # Using all_feature_names from the py_feature_scaling cell\n    'importance': rf_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(f\"âœ¨ Top 5 Most Important Features:\")\nprint(rf_feature_importance.head())",
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "id": "789d8b5d-3877-4339-97b5-014041aff18d",
      "metadata": {
        "language": "python",
        "name": "py_importance_plot_rf",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "top_n = 5\nchart_data = rf_feature_importance.head(top_n).set_index('feature')\n\nchart_data['importance'].sort_values(ascending=True).plot.barh(\n    title='Top 5 Feature Importance (Random Forest)',\n    xlabel='Importance',\n    ylabel='Feature',\n    figsize=(10, 6)\n)",
      "execution_count": 35
    },
    {
      "cell_type": "markdown",
      "id": "bd1ec811-6dd9-4dd1-a828-e4ae4b0fda2a",
      "metadata": {
        "name": "md_interpretability_svm",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "#### 4.2.3. Interpreting SVM models\n\nThe only interpretable SVM algorithm are those using linear kernel while those using non-linear kernels like polynomial SVM or radial basis function (RBF) SVM are no longer interpretable and are regarded as black-box models.\n\nThe previously built SVM model is using the RBF kernel and are thus non-linear and not interpretable.\n\nAs already mentioned, if you'd like to have an interpretable SVM model, then you can use linear kernel that you can also try."
    },
    {
      "cell_type": "markdown",
      "id": "4bc5b15b-4bd4-4049-bb9e-980c723e6b57",
      "metadata": {
        "name": "md_resources",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Resources\nIf you'd like to take a deeper dive into the various libraries used in this tutorial, here they are:\n- [pandas on Snowflake](https://docs.snowflake.com/en/developer-guide/snowpark/python/pandas-on-snowflake)\n- [Snowpark pandas API](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/modin/index)\n- [scikit-learn API reference](https://scikit-learn.org/stable/api/index.html)\n- [Altair API reference](https://altair-viz.github.io/user_guide/api.html)\n- [YouTube Playlist on Snowflake Notebooks](https://www.youtube.com/watch?v=YB1B6vcMaGE&list=PLavJpcg8cl1Efw8x_fBKmfA2AMwjUaeBI)"
    }
  ]
}