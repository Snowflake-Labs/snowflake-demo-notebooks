{
  "metadata": {
    "kernelspec": {
      "display_name": "Streamlit Notebook",
      "name": "streamlit"
    },
    "lastEditStatus": {
      "notebookId": "arf7ixegn5jhpeldnvma",
      "authorId": "6841714608330",
      "authorName": "CHANINN",
      "authorEmail": "chanin.nantasenamat@snowflake.com",
      "sessionId": "39cecc10-d792-4cd7-8dae-4096a745bba0",
      "lastEditTime": 1760806281013
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eb7a29c7-92eb-4d12-b8cd-d81075f6e7b4",
      "metadata": {
        "name": "md_title",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# Build Your First Machine Learning Project - Part 1 | `Data Wrangling`\n\nIn this course, we'll build our first machine learning project end-to-end in Python. \n\n### Course Content\nWe'll cover this in five separate notebooks/apps:\n1. **Data Operations** - Ingest data, data wrangling and write to Snowflake using Snowpark (`snowflake-snowpark-python`)\n2. **Exploratory Data Analysis (EDA)** - Explore data, summary statistics, data visualization using `Altair` and `Streamlit`\n3. **Machine learning (ML)** - Prepare data and features for build models using different ML algorithms (Logistic Regression, Random Forest and Support Vector Machine) with `scikit-learn`\n4. **Experiment Tracking** - Initiate experiment tracking when building and trying out different hyperparameters with `ExperimentTracking()` from `snowflake-ml-python`\n5. **Data App** - Build a sharable data app with `Streamlit`\n\n### What We'll Cover (in this Notebook):\n\n1. **Data Loading and Preparation** - Load the bear dataset and prepare it for analysis using Modin (`modin.pandas`) and Snowpark (`snowflake-snowpark-python`)\n2. **Basic Statistics** - Calculate and visualize summary statistics of the dataset\n3. **Feature Distribution Analysis** - Explore the distribution of individual features across different bear species with `Altair` and `Streamlit`\n4. **Correlation Analysis** - Investigate relationships between numeric features using correlation heatmaps with `Altair` and `Streamlit`\n5. **Feature Relationships** - Visualize relationships between pairs of features using interactive scatter plots with `Altair` and `Streamlit`\n6. **Categorical Analysis** - Examine the distribution of categorical features including species classification with `Altair` and `Streamlit`\n\n"
    },
    {
      "cell_type": "markdown",
      "id": "fb1bbba1-0802-4b35-9921-cb899b41bdff",
      "metadata": {
        "name": "md_data_setup",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# Data Setup\n\nIn this step, we'll perform the following:\n1. Load tabular data from a GitHub repo\n2. Create a Snowflake stage for storing image data"
    },
    {
      "cell_type": "markdown",
      "id": "41a7defc-5f38-4387-9f17-abd83c26bb11",
      "metadata": {
        "name": "md_bear_data",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Bear dataset\n\nThe dataset is a classic multi-class classification problem where the goal of the machine learning task is to classify each entry as belonging to one of four species that a bear belongs to based on its features.\n\nThe bear dataset is comprised of 200 bear samples and each entry is described by 6 different features pertaining to the bear's physical characteristics (also known as parameters, independent variables or X variables) and is assigned to one of four bear species (A, B, C and D).\n\n"
    },
    {
      "cell_type": "markdown",
      "id": "623130bf-a2bc-4c09-b387-edec0b100ef7",
      "metadata": {
        "name": "md_load_data",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Load Data\n\nHere, we'll load in the first portion of the data set that comprises of the ID, bear species, and 6 feature columns:\n- id\n- species\n- body_mass_kg\n- shoulder_hump_height_cm\n- claw_length_cm\n- snout_length_cm\n- forearm_circumference_cm\n- ear_length_cm\n\nAs for the second portion, we'll prepare those from features that we'll extract from a collection of bear images that corresponds to each of the row IDs."
    },
    {
      "id": "6d9b2bfe-4c3a-49cd-80c4-cef010998400",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Download CSV data\n\nFor the first portion of the data, please download the [bear_raw_data.csv](https://raw.githubusercontent.com/dataprofessor/bear-dataset/refs/heads/master/bear_raw_data.csv) file onto your computer and then we'll upload it to Snowflake."
    },
    {
      "id": "b71f6df8-0d22-40b9-b10a-eef194566b49",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Upload CSV to Workspaces\n\nFrom within Workspaces, to the same working directory as these notebooks, click **+ Add new** > **Upload Files** and select the `bear_raw_data.csv` file. \n\nOr if the notebook is in a folder, hover on the folder and click on **+** > **Upload Files** and select the `bear_raw_data.csv` file.",
      "execution_count": null
    },
    {
      "id": "5bff5ab7-9e33-4c1b-82fa-743df74a8811",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "import pandas as pd\n\ndf = pd.read_csv(\"bear_raw_data.csv\")\ndf",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "4b2f5a5f-92ed-48f2-ab70-3faa7fe7de79",
      "metadata": {
        "name": "md_load_images",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Load Images\n\nAs previously mentioned, each row from the data set has a unique ID for each bear along with its own corresponding image named using the ID (e.g. `GRZ_01`, `GRZ_02`, `GRZ_03`, etc.)"
    },
    {
      "id": "bdbb735d-8aea-4920-a911-357ac3e69ae0",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "### Upload images to Workspace\n\nIn a similar fashion to how we'd upload the CSV file previously, now we'll upload the 200 images as a folder.\n\nFrom within Workspaces, to the same working directory as these notebooks, click **+ Add new** > **Upload Folder** and select the `images/` folder. \n\nOr if the notebook is in a folder, hover on the folder and click on **+** > **Upload Folder** and select the `images/` folder."
    },
    {
      "id": "3190575e-7db7-44cc-a877-37762bb9f91e",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "### Upload images to Stage\n\nHere, we're going to upload these images to a stage, which is needed when we run the LLM inference on the image in a few moments."
    },
    {
      "id": "149715a5-ba57-4dd2-aa88-2b07c010ce5b",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "import os\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Set database and schema\nDATABASE = \"CHANINN_DEMO_DATA\"\nSCHEMA = \"PUBLIC\"\n\nsession.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE}\").collect()\nsession.sql(f\"USE DATABASE {DATABASE}\").collect()\nsession.sql(f\"USE SCHEMA {SCHEMA}\").collect()\n\n# Create stage with server-side encryption\nsession.sql(\"\"\"\n    CREATE OR REPLACE STAGE img_stage\n    ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')\n\"\"\").collect()\n\n# Get all PNG files from images/ folder\nimage_folder = 'images'\nimage_files = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.png')]\n\n# Upload all images to stage\nfor img in image_files:\n    session.file.put(img, \"@img_stage\", auto_compress=False, overwrite=True)\n\nprint(f\"Stage ready with {len(image_files)} images uploaded: {image_files}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "726d70d6-89b1-4738-9bb3-861734872be4",
      "metadata": {
        "name": "md_display_image",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Display bear images\n\nNow that we have all of the image uploaded, let's have a look at them."
    },
    {
      "cell_type": "code",
      "id": "71460384-3f4d-489e-bea6-eb0353efae1d",
      "metadata": {
        "language": "python",
        "name": "py_display_image",
        "title": "py_display_image"
      },
      "outputs": [],
      "source": "import matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Bear species and their captions\nbears = [\n    (\"ABB\", \"American Black Bear\"),\n    (\"EUR\", \"Eurasian Brown Bear\"), \n    (\"GRZ\", \"Grizzly Bear\"),\n    (\"KDK\", \"Kodiak Bear\")\n]\n\n# Create a figure with 4 subplots\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nfig.suptitle('Bear Species', fontsize=16, fontweight='bold')\n\nfor idx, (species, caption) in enumerate(bears):\n    img_path = f'images/{species}_01.png'\n    \n    try:\n        # Open local image directly\n        img = Image.open(img_path)\n        \n        axes[idx].imshow(img)\n        axes[idx].set_title(f\"{caption}\\n({species}_01.png)\", fontsize=10)\n        axes[idx].axis('off')\n    except Exception as e:\n        axes[idx].text(0.5, 0.5, 'Image\\nNot\\nAvailable', \n                      ha='center', va='center', fontsize=12)\n        axes[idx].set_title(f\"{caption}\\n({species}_01.png)\", fontsize=10)\n        axes[idx].axis('off')\n\nplt.tight_layout()\nplt.show()",
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "id": "33228af2-76b0-4826-9186-127de28368ff",
      "metadata": {
        "name": "md_image_analysis",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# Image Analysis\n\nWhy are we analyzing the images? As mentioned earlier on, we're going to add additional features to the dataset by analyzing the bear images to figure out the following:\n- Fur color\n- Facial profile\n- Paw pad texture\n\nThease 3 features are added to the data set loaded above in the `py_load_data` cell and stored in the `df` variable."
    },
    {
      "cell_type": "markdown",
      "id": "ec088e67-621c-456c-9a3c-216ac6ce7d68",
      "metadata": {
        "name": "md_llm_inference",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Fur color classification with `ai_classify`\n\nTo classify an image into predefined categories, we're performing the following steps:\n1. Use the `ai_classify()` Snowpark function to classify the image\n2. Use `prompt()` to specify the instruction text with `{0}` as a placeholder for the image\n3. Use `to_file()` to reference the image file from the stage\n4. Provide a list of categories (`fur_colors`) for the model to classify into"
    },
    {
      "id": "859520fb-32a2-4d00-a48b-7824103a68e2",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.functions import ai_classify, to_file, prompt\n\nfur_colors = [\"Light Brown\", \"Medium Brown\", \"Blond\", \"Dark Brown\", \"Grizzled Brown\", \n              \"Reddish Brown\", \"Blackish Brown\", \"Black\", \"Brown\", \"Cinnamon\"]\n\nresponse = session.range(1).select(\n    ai_classify(\n        prompt(\"Please help me classify the fur color of the bear {0}\", to_file(\"@img_stage/ABB_01.png\")),\n        fur_colors\n    ).alias(\"classes\")\n)\nresponse.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "9d094f33-256f-4020-9653-61c82cb20984",
      "metadata": {
        "name": "md_iterative_analysis",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Iterative Image Analysis\n\nHere, we'll apply Cortex AISQL to analyze the image and determine the bear's features.\n\n"
    },
    {
      "cell_type": "markdown",
      "id": "4da5078c-59bb-40d0-88b9-e3ad5dfbfb54",
      "metadata": {
        "name": "md_fur_color_iterative",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Fur Color\n\nWe'll start with analyzing the fur color for all 200 images."
    },
    {
      "id": "42fce3d9-a02a-439a-8596-038e0b6d4ef7",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "py_fur_color_iterative",
        "title": "py_fur_color_iterative"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import ai_classify, to_file, prompt, col, concat, lit\n\nsession = get_active_session()\n\nfur_colors = [\n    \"Light Brown\", \"Medium Brown\", \"Blond\", \"Dark Brown\", \"Grizzled Brown\",\n    \"Reddish Brown\", \"Blackish Brown\", \"Black\", \"Brown\", \"Cinnamon\"\n]\n\n# Get files and classify in batch\nresults_df = (\n    session.sql(\"LIST @img_stage\")\n    .select(\n        col('\"name\"').alias(\"file_name\"),\n        concat(lit(\"@\"), col('\"name\"')).alias(\"file_path\")\n    )\n    .select(\n        col(\"file_name\"),\n        ai_classify(\n            prompt(\"Please classify the fur color of the bear {0}\", to_file(col(\"file_path\"))),\n            fur_colors\n        ).alias(\"classification\")\n    )\n)\n\nresults_df.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "31c89ce2-7246-4b3a-a0b9-4a8d2010313d",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "import json\n\nresults_list = [\n    json.loads(row[\"CLASSIFICATION\"])[\"labels\"][0] \n    for row in results_df.select(\"classification\").collect()\n]",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "7702407b-f531-451e-aaec-3861aff5d338",
      "metadata": {
        "language": "python",
        "name": "py_display_results",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "results_list",
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "id": "2b8887e9-1294-493e-a7b2-181736517f2b",
      "metadata": {
        "name": "md_convert_to_dataframe",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Here, we'll convert our collected fur color analysis results into a structured Snowpark DataFrame, which we'll add to the full data set later on.\n"
    },
    {
      "id": "9fa104a6-2fef-48bc-b525-09d9574809f4",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "py_convert_to_dataframe",
        "title": "py_convert_to_dataframe"
      },
      "source": "import json\nimport pandas as pd\n\n# Collect both columns and parse\ndata = [\n    {\n        \"id\": row[\"FILE_NAME\"],\n        \"color\": json.loads(row[\"CLASSIFICATION\"])[\"labels\"][0]\n    }\n    for row in results_df.collect()\n]\n\n# Convert to pandas DataFrame\ndf_fur = pd.DataFrame(data)\ndf_fur[\"id\"] = df_fur[\"id\"].apply(lambda x: x.split(\"/\")[-1].replace(\".png\", \"\"))\ndf_fur",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cea3f1ec-3454-4348-97f0-ce7471d57ec2",
      "metadata": {
        "name": "md_analyze_facial_profile",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Facial Profile\n\nNext, we'll analyze the facial profile of the bears, which is a key distinguishing feature. The facial profile can be either:\n- **Dished**: Concave profile, where the bridge of the nose dips)\n- **Straight**: Flat profile, with no dip from the forehead to the nose)"
    },
    {
      "cell_type": "code",
      "id": "80409a89-8ce4-48d5-8f08-fda603f8473f",
      "metadata": {
        "language": "python",
        "name": "py_analyze_facial_profile"
      },
      "outputs": [],
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import ai_classify, to_file, prompt, col, concat, lit\n\nsession = get_active_session()\n\nfacial_profiles = [\"Dished\", \"Straight\"]\n\n# Get files and classify in batch\nresults_df = (\n    session.sql(\"LIST @img_stage\")\n    .select(\n        col('\"name\"').alias(\"file_name\"),\n        concat(lit(\"@\"), col('\"name\"')).alias(\"file_path\")\n    )\n    .select(\n        col(\"file_name\"),\n        ai_classify(\n            prompt(\"Analyze the facial profile of the bear in this image {0}. Is it Dished (concave, nose dips) or Straight (flat, no dip)?\", to_file(col(\"file_path\"))),\n            facial_profiles\n        ).alias(\"classification\")\n    )\n)\n\nresults_df",
      "execution_count": 28
    },
    {
      "id": "a4ae8288-118b-4a00-8a64-e0c1e98f7d1e",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "import json\nimport pandas as pd\n\n# Collect both columns and parse\ndata = [\n    {\n        \"id\": row[\"FILE_NAME\"].split(\"/\")[-1].replace(\".png\", \"\"),\n        \"facial_profile\": json.loads(row[\"CLASSIFICATION\"])[\"labels\"][0]\n    }\n    for row in results_df.collect()\n]\n\n# Convert to pandas DataFrame\ndf_facial_profile = pd.DataFrame(data)\ndf_facial_profile",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "c5974d98-05de-40b1-9a09-5739ca167a1f",
      "metadata": {
        "name": "md_analyze_paw_pad",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Paw Pad Texture\n\nNext, we'll analyze the texture of the bears' paw pads, which is another distinguishing characteristic. The paw pad texture can be either:\n- **Smooth**: Less textured and relatively flat, for walking\n- **Rough**: More textured and grooved, for gripping and climbing"
    },
    {
      "cell_type": "code",
      "id": "5a1e094b-336b-4cd8-9634-2ee0fc89edcf",
      "metadata": {
        "language": "python",
        "name": "py_analyze_paw_pad"
      },
      "outputs": [],
      "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import ai_classify, to_file, prompt, col, concat, lit\n\nsession = get_active_session()\n\npaw_textures = [\"Smooth\", \"Rough\"]\n\n# Get files and classify in batch\nresults_df = (\n    session.sql(\"LIST @img_stage\")\n    .select(\n        col('\"name\"').alias(\"file_name\"),\n        concat(lit(\"@\"), col('\"name\"')).alias(\"file_path\")\n    )\n    .select(\n        col(\"file_name\"),\n        ai_classify(\n            prompt(\"Analyze the paw pad texture of the bear in this image {0}. Is it Smooth (less textured, flat, for walking) or Rough (more textured, grooved, for gripping)?\", to_file(col(\"file_path\"))),\n            paw_textures\n        ).alias(\"classification\")\n    )\n)\n\nresults_df",
      "execution_count": 30
    },
    {
      "id": "d11b1c49-506f-4d55-ac17-7c0fdd83356c",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "import json\nimport pandas as pd\n\n# Collect both columns and parse\ndata = [\n    {\n        \"id\": row[\"FILE_NAME\"].split(\"/\")[-1].replace(\".png\", \"\"),\n        \"paw_pad_texture\": json.loads(row[\"CLASSIFICATION\"])[\"labels\"][0]\n    }\n    for row in results_df.collect()\n]\n\n# Convert to pandas DataFrame\ndf_paw_pad = pd.DataFrame(data)\ndf_paw_pad",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "d348f459-ed3f-4cbd-a1e2-f4a4170dc695",
      "metadata": {
        "name": "md_data_operations",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# Data Operations\n\nIn this section, we'll perform essential data operations to:\n- Combine the extracted features (fur color, facial profile, and paw pad texture) with the original dataset \n- Write the final dataset to a Snowflake table\n"
    },
    {
      "cell_type": "code",
      "id": "a1b3c87a-6a09-4b78-b561-187370388659",
      "metadata": {
        "language": "python",
        "name": "py_categorical_columns"
      },
      "outputs": [],
      "source": "# Read categorical columns\nimport pandas as pd\n\n# Load the categorical feature data from CSV files\n# df_fur_color = pd.read_csv(\"https://raw.githubusercontent.com/dataprofessor/bear-dataset/refs/heads/master/fur_color.csv\")\n# df_facial_profile = pd.read_csv(\"https://raw.githubusercontent.com/dataprofessor/bear-dataset/refs/heads/master/facial_profile.csv\")\n# df_paw_pad = pd.read_csv(\"https://raw.githubusercontent.com/dataprofessor/bear-dataset/refs/heads/master/paw_pad_texture.csv\")",
      "execution_count": 32
    },
    {
      "cell_type": "markdown",
      "id": "31830327-f269-44c7-931e-ebbbb3964571",
      "metadata": {
        "name": "md_combine_features",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Combining Features\n\nNow that we have extracted all three features (fur color, facial profile, and paw pad texture) from the bear images, let's combine them with our original dataset. \n\nThe final combined dataset will include:\n\n- Original physical measurements (body mass, shoulder hump height, etc.)\n- Fur color analysis\n- Facial profile classification\n- Paw pad texture assessment\n\nThis comprehensive dataset will give us a more complete picture of each bear's characteristics for our analysis.\n"
    },
    {
      "cell_type": "code",
      "id": "36e31897-4d61-4a33-a616-cf07c26243a4",
      "metadata": {
        "language": "python",
        "name": "py_combine_features"
      },
      "outputs": [],
      "source": "# Combining df_fur, df_facial_profile and df_paw_pad to df\n# Standardize column names to match\ndf['id'] = df['id'].str.upper()  # Ensure IDs are in uppercase\ndf_fur_color['id'] = df_fur_color['id'].str.upper()\ndf_facial_profile['id'] = df_facial_profile['id'].str.upper()\ndf_paw_pad['id'] = df_paw_pad['id'].str.upper()\n\n# Perform sequential merges to combine all features using proper indexing\ndf_combined = df.merge(df_fur_color, on='id', how='inner')\ndf_combined = df_combined.merge(df_facial_profile, on='id', how='inner')\ndf_combined = df_combined.merge(df_paw_pad, on='id', how='inner')\n\n# Display the combined DataFrame\ndf_combined",
      "execution_count": 33
    },
    {
      "cell_type": "markdown",
      "id": "ae42bfbd-bbe4-4082-b4c4-1d344d3d97e9",
      "metadata": {
        "name": "md_write_data",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Write data to a database table"
    },
    {
      "cell_type": "markdown",
      "id": "7a4520f2-864f-4ea6-a9e4-2f51617417fb",
      "metadata": {
        "name": "md_db_schema",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Determine current database and schema\n\nBut before we write to a Snowflake database table, let's figure out the current location where this notebook is located, which in turn is where are database table will reside in."
    },
    {
      "cell_type": "code",
      "id": "86228219-bd35-47ab-a47b-6db331080d6d",
      "metadata": {
        "language": "sql",
        "name": "sql_db_schema",
        "resultVariableName": "dataframe_5"
      },
      "outputs": [],
      "source": "SELECT CURRENT_DATABASE(), CURRENT_SCHEMA();",
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "id": "083b6ab0-9919-493a-a778-8a70499511cc",
      "metadata": {
        "language": "sql",
        "name": "sql_set_db_schema",
        "resultVariableName": "dataframe_6"
      },
      "outputs": [],
      "source": "USE DATABASE chaninn_demo_data;\nUSE SCHEMA public;",
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "id": "268c13f1-0614-4acb-affa-b4978262fc85",
      "metadata": {
        "language": "python",
        "name": "py_write_data",
        "codeCollapsed": false,
        "title": "py_write_data"
      },
      "outputs": [],
      "source": "# Convert pandas DataFrame to Snowpark DataFrame and write to table\nsnowpark_df = session.create_dataframe(df_combined)\nsnowpark_df.write.mode(\"overwrite\").save_as_table(\"BEAR\")\n\nprint(\"Data written to BEAR table successfully!\")",
      "execution_count": 36
    },
    {
      "cell_type": "markdown",
      "id": "c7812b6e-8bea-487f-a135-03a4df9a6b1c",
      "metadata": {
        "name": "md_query_data",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Query data from table"
    },
    {
      "cell_type": "code",
      "id": "676033c3-95e6-40d0-b661-f0ab9240402d",
      "metadata": {
        "language": "sql",
        "name": "sqL_query_data",
        "resultVariableName": "dataframe_7"
      },
      "outputs": [],
      "source": "SELECT * FROM CHANINN_DEMO_DATA.PUBLIC.BEAR;",
      "execution_count": 39
    },
    {
      "cell_type": "markdown",
      "id": "a3ea6562-fdad-490e-af12-2da61f65901f",
      "metadata": {
        "name": "md_resources",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# Resources\nIf you'd like to take a deeper dive into Snowpark pandas:\n- [pandas on Snowflake](https://docs.snowflake.com/en/developer-guide/snowpark/python/pandas-on-snowflake)\n- [Snowpark pandas API](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/modin/index)\n- [Snowflake Cortex AISQL](https://docs.snowflake.com/user-guide/snowflake-cortex/aisql)\n- [YouTube Playlist on Snowflake Notebooks](https://www.youtube.com/watch?v=YB1B6vcMaGE&list=PLavJpcg8cl1Efw8x_fBKmfA2AMwjUaeBI)"
    }
  ]
}
