{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "67ozngwmwyycoznp5c3p",
   "authorId": "5057414526494",
   "authorName": "FAWAZG",
   "authorEmail": "fawaz.ghali@snowflake.com",
   "sessionId": "574e10fe-20e8-487f-8fc9-fa65303920df",
   "lastEditTime": 1744224111493
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b05a1208-d14f-41a4-b949-acfa4c53c96b",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "# Introduction\n\nSnowflake's support for Java through Snowpark enables developers to write rich, flexible data processing logic directly within the data platform. This notebook demonstrates how to leverage Snowflake’s Java UDFs and stored procedures to build scalable, reusable, and efficient data workflows. By combining Snowflake's compute engine with Java's maturity and Snowpark's powerful APIs, developers can encapsulate business logic, perform asynchronous processing, and work with structured or unstructured data—all inside Snowflake.\n\nThroughout this notebook, we explore key concepts including the creation and execution of Java-based stored procedures and UDFs, how to read static and dynamic files using Snowflake stages, and how to handle asynchronous operations to optimize performance. Practical examples help illustrate the power of SnowflakeFile, InputStream, and DataFrame integrations for real-time data handling and processing scenarios.\n\n\n![Java UDF Calling Flow](https://docs.snowflake.com/en/_images/UDF_Java_Calling_03a.png)\n"
  },
  {
   "cell_type": "markdown",
   "id": "360f94a5-de0c-4bdd-9db1-c0d5f419b3fe",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "## Step 1: Creating a Stage and Uploading Files in Snowflake\n\n### Create a Stage:\n1. **Sign in** to Snowsight.\n2. Select **Create » Stage » Snowflake Managed**.\n3. Enter **Stage Name** and select the **database/schema**.\n4. Optionally, **deselect Directory table** to avoid warehouse costs.\n5. Choose **Encryption** (cannot be changed later).\n\n### Upload Files:\n1. **Sign in** to Snowsight.\n2. Select **Data » Add Data » Load files into a Stage**.\n3. Choose files to upload.\n4. Select **database/schema** and **stage**.\n5. Optionally, create a **path**.\n6. Click **Upload**."
  },
  {
   "cell_type": "code",
   "id": "10b82e4e-e51a-4221-a158-acd8500a771c",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "--list the staged file(s)\nls @sales_data_stage;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9ee5dd71-ead3-441c-82cc-95408d86b803",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "## Step 2: Stored Procedures in Snowflake for Java Developers\n\nStored procedures in Snowflake allow Java developers to automate and simplify database tasks by writing procedural logic with Java handlers. These procedures can be used to execute dynamic database operations, encapsulate complex logic, and manage privileges securely. Java can be used as the handler language, with code either in-line or staged, and procedures can return single values or tables. Developers can use Snowpark for Java to create, manage, and deploy procedures, while also utilizing features like temporary procedures, logging, and external network access. Security and data protection practices should be followed, especially when deciding between caller's or owner's rights for execution.\n"
  },
  {
   "cell_type": "markdown",
   "id": "b82f15da-fe02-4a95-8844-aa901ee21a97",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "### Step 2.1: Writing Java Handlers for Snowflake Stored Procedures\n\nTo write a Java handler for a Snowflake stored procedure, developers use the Snowpark API to interact with Snowflake tables and data pipelines. The handler code can be deployed in-line with the procedure or as compiled classes stored on a Snowflake stage. The Java method must include a Snowpark Session object as the first argument and return a value (e.g., String or tabular data). Developers need to ensure thread-safety, handle exceptions, and optimize performance to avoid memory limits. It's crucial to consider whether the procedure will run with caller's or owner's rights and manage dependencies by uploading necessary JAR files or resource files to Snowflake. Asynchronous child jobs must be carefully handled, as they can be canceled when the parent procedure completes. Snowflake also supports logging and tracing for monitoring execution, which is vital for debugging and performance tracking."
  },
  {
   "cell_type": "markdown",
   "id": "28f0427e-1a17-4ced-bde3-18e99ef7bbe3",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "39ba866b-46be-43a5-b2de-22eeb732981b",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "### Step 2.2: Reading a Dynamically-Specified File with SnowflakeFile\n\nThe following example demonstrates how to read a dynamically-specified file using the `SnowflakeFile` class. The `execute` handler function takes a `String` as input and returns a `String` containing the file's contents. During execution, Snowflake initializes the handler's `fileName` variable with the incoming file path from the procedure's input variable. The handler code then uses a `SnowflakeFile` instance to read the specified file.\n"
  },
  {
   "cell_type": "code",
   "id": "a357ad7f-df9d-41b8-b83f-0edf01896f8b",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE PROCEDURE file_reader_java_proc_snowflakefile(input VARCHAR)\nRETURNS VARCHAR\nLANGUAGE JAVA\nRUNTIME_VERSION = 11\nHANDLER = 'FileReader.execute'\nPACKAGES=('com.snowflake:snowpark:latest')\nAS $$\nimport java.io.InputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport com.snowflake.snowpark_java.types.SnowflakeFile;\nimport com.snowflake.snowpark_java.Session;\n\nclass FileReader {\n  public String execute(Session session, String fileName) throws IOException {\n    InputStream input = SnowflakeFile.newInstance(fileName).getInputStream();\n    return new String(input.readAllBytes(), StandardCharsets.UTF_8);\n  }\n}\n$$;\nCALL file_reader_java_proc_snowflakefile(BUILD_SCOPED_FILE_URL('@sales_data_stage', '/car_sales.json'));\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "19b929ea-07ec-46b5-8e39-40ceee5aecfa",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "### Step 2.3: Reading a Dynamically-Specified File with InputStream\n\nThe following example demonstrates how to read a dynamically-specified file using `InputStream`. The `execute` handler function takes an `InputStream` as input and returns a `String` containing the file's contents. During execution, Snowflake initializes the handler's `stream` variable with the incoming file path from the procedure's input argument. The handler code then uses the `InputStream` to read the specified file.\n"
  },
  {
   "cell_type": "code",
   "id": "fd83bf06-280a-4dfe-a327-c1494d590f03",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE PROCEDURE file_reader_java_proc_input(input VARCHAR)\nRETURNS VARCHAR\nLANGUAGE JAVA\nRUNTIME_VERSION = 11\nHANDLER = 'FileReader.execute'\nPACKAGES=('com.snowflake:snowpark:latest')\nAS $$\nimport java.io.InputStream;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport com.snowflake.snowpark.Session;\n\nclass FileReader {\n  public String execute(Session session, InputStream stream) throws IOException {\n    String contents = new String(stream.readAllBytes(), StandardCharsets.UTF_8);\n    return contents;\n  }\n}\n$$;\nCALL file_reader_java_proc_input(BUILD_SCOPED_FILE_URL('@sales_data_stage', '/car_sales.json'));\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b7a9b65-fd03-4ebb-928a-3b39e7500c05",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "### Step 2.4: Returning Tabular Data from a Java Stored Procedure\n\nYou can write a stored procedure that returns data in tabular form by following these steps:\n\n1. Specify `TABLE(...)` as the procedure's return type in your `CREATE PROCEDURE` statement.\n  \n2. When defining the procedure, you can specify the returned data's column names and types as `TABLE` parameters if you know them in advance. If the column names are not known at definition time, such as when they are specified at runtime, you can omit the `TABLE` parameters. \n3. Implement the handler to return the tabular result as a Snowpark DataFrame.\n\nFor more information about working with DataFrames, refer to the *Working with DataFrames in Snowpark Java* documentation.\n"
  },
  {
   "cell_type": "code",
   "id": "cae3e555-26ee-432b-82e4-4fd164fb6eef",
   "metadata": {
    "language": "sql",
    "name": "cell13"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE employees(id NUMBER, name VARCHAR, role VARCHAR);\nINSERT INTO employees (id, name, role) VALUES (1, 'Alice', 'op'), (2, 'Bob', 'dev'), (3, 'Cindy', 'dev');\n\nCREATE OR REPLACE PROCEDURE filter_by_role(table_name VARCHAR, role VARCHAR)\nRETURNS TABLE(id NUMBER, name VARCHAR, role VARCHAR)\nLANGUAGE JAVA\nRUNTIME_VERSION = '11'\nPACKAGES = ('com.snowflake:snowpark:latest')\nHANDLER = 'Filter.filterByRole'\nAS\n$$\nimport com.snowflake.snowpark_java.*;\n\npublic class Filter {\n  public DataFrame filterByRole(Session session, String tableName, String role) {\n    DataFrame table = session.table(tableName);\n    DataFrame filteredRows = table.filter(Functions.col(\"role\").equal_to(Functions.lit(role)));\n    return filteredRows;\n  }\n}\n$$;\n\nCALL filter_by_role('employees', 'dev');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b27df6ab-b59c-47db-811a-1377d35bfcb7",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "### Step 2.5: Introduction to Asynchronous Processing in Snowflake Stored Procedures\n\nThis example introduces how to leverage Snowpark APIs for asynchronous processing within a Snowflake stored procedure. The `getResultJDBC()` procedure, written in Java, demonstrates executing an asynchronous query using the `executeAsyncQuery()` method. In this case, it calls `SYSTEM$WAIT(10)` to pause the process for 10 seconds, allowing other operations to continue without blocking the execution. This approach highlights how Snowflake's Snowpark framework enables non-blocking, scalable operations, making it ideal for handling long-running tasks efficiently within Snowflake's data warehouse environment.\n"
  },
  {
   "cell_type": "code",
   "id": "90bb327f-8c0b-457e-ad64-8a6849fabf3f",
   "metadata": {
    "language": "sql",
    "name": "cell17"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE PROCEDURE getResultJDBC()\nRETURNS VARCHAR\nLANGUAGE JAVA\nRUNTIME_VERSION = 11\nPACKAGES = ('com.snowflake:snowpark:latest')\nHANDLER = 'TestJavaSP.asyncBasic'\nAS\n$$\nimport java.sql.*;\nimport net.snowflake.client.jdbc.*;\n\nclass TestJavaSP {\n  public String asyncBasic(com.snowflake.snowpark.Session session) throws Exception {\n    Connection connection = session.jdbcConnection();\n    SnowflakeStatement stmt = (SnowflakeStatement)connection.createStatement();\n    ResultSet resultSet = stmt.executeAsyncQuery(\"CALL SYSTEM$WAIT(10)\");\n    resultSet.next();\n    return resultSet.getString(1);\n  }\n}\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "baecb403-6bf4-4dd1-846a-abbcf5044ecb",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "## Step 3: User-Defined Functions (UDFs)\n\nUser-defined functions (UDFs) allow you to extend Snowflake’s built-in functions by creating custom operations. UDFs are reusable, always return a value, and are ideal for performing calculations. You can write a UDF’s logic in a supported language, then create and execute it using Snowflake’s tools. UDFs can be used to encapsulate standard calculations or extend existing functions, and they are called in the same way as built-in functions. While similar to stored procedures, UDFs differ in key ways. For more details, see *Choosing whether to write a stored procedure or a user-defined function*.\n"
  },
  {
   "cell_type": "markdown",
   "id": "2df61368-bf7e-4e50-b2c6-1244eb73fc4d",
   "metadata": {
    "name": "cell24",
    "collapsed": false
   },
   "source": "### Step 3.1: Passing via an ARRAY\nThis code creates a Snowflake table that stores arrays of strings, inserts three rows with increasingly longer arrays (e.g., `['Hello']`, `['Hello', 'Jay']`, etc.), and defines a Java user-defined function (UDF) that takes an array of strings and concatenates them into a single space-separated string. The final query applies this function to each row, resulting in output like \"Hello\", \"Hello Jay\", and \"Hello Jay Smith\".\n\n"
  },
  {
   "cell_type": "code",
   "id": "e4987896-57ae-40dd-894f-ca54e57ffb15",
   "metadata": {
    "language": "sql",
    "name": "cell22"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE string_array_table(id INTEGER, a ARRAY);\nINSERT INTO string_array_table (id, a) SELECT\n        1, ARRAY_CONSTRUCT('Hello');\nINSERT INTO string_array_table (id, a) SELECT\n        2, ARRAY_CONSTRUCT('Hello', 'Jay');\nINSERT INTO string_array_table (id, a) SELECT\n        3, ARRAY_CONSTRUCT('Hello', 'Jay', 'Smith');\n\nCREATE OR REPLACE FUNCTION concat_varchar_2(a ARRAY)\n  RETURNS VARCHAR\n  LANGUAGE JAVA\n  HANDLER = 'TestFunc_2.concatVarchar2'\n  TARGET_PATH = '@~/TestFunc_2.jar'\n  AS\n  $$\n  class TestFunc_2 {\n      public static String concatVarchar2(String[] strings) {\n          return String.join(\" \", strings);\n      }\n  }\n  $$;\nSELECT concat_varchar_2(a)\n  FROM string_array_table\n  ORDER BY id;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ad545cd-f36e-436f-baa8-b66f1aee4488",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "### Step 3.2: Understanding Java UDF Parallelization\n\nSnowflake improves performance by parallelizing UDF execution both across and within JVMs.\n\n- **Across JVMs**: Snowflake parallelizes work across warehouse workers, with each worker running one or more JVMs. There is no global shared state, and state can only be shared within a single JVM.\n\n- **Within JVMs**: Each JVM can execute multiple threads, allowing parallel calls to the same handler method. Therefore, the handler method must be thread-safe.\n\nIf a UDF is **IMMUTABLE**, it will return the same value for each call with the same arguments on the same row. For example, calling an IMMUTABLE UDF multiple times with the same arguments will return the same result for each row.\n"
  },
  {
   "cell_type": "code",
   "id": "1b161102-8127-4232-b763-21d3f99606c2",
   "metadata": {
    "language": "sql",
    "name": "cell23",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "/*\nCreate a Jar file with the following Class\nclass MyClass {\n\n  private double x;\n\n  // Constructor\n  public MyClass()  {\n    x = Math.random();\n  }\n\n  // Handler\n  public double myHandler() {\n    return x;\n  }\n}\n*/\nCREATE FUNCTION my_java_udf_1()\n  RETURNS DOUBLE\n  LANGUAGE JAVA\n  IMPORTS = ('@sales_data_stage/HelloRandom.jar')\n  HANDLER = 'MyClass.myHandler';\n\nCREATE FUNCTION my_java_udf_2()\n  RETURNS DOUBLE\n  LANGUAGE JAVA\n  IMPORTS = ('@sales_data_stage/HelloRandom.jar')\n  HANDLER = 'MyClass.myHandler';\n\n  SELECT\n    my_java_udf_1(),\n    my_java_udf_2()\n  FROM table1;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "71612834-78b0-4b70-ae3a-bf10b894591e",
   "metadata": {
    "name": "cell26",
    "collapsed": false
   },
   "source": "### Step 3.3: Creating and Calling a Simple In-Line Java UDF\n\nThe following example demonstrates creating and calling a simple in-line Java UDF that returns the `VARCHAR` passed to it. \n\nThis function is declared with the optional `CALLED ON NULL INPUT` clause, which ensures the function is called even if the input value is NULL. While this function would return NULL with or without the clause, you could modify the code to handle NULL differently, such as returning an empty string."
  },
  {
   "cell_type": "code",
   "id": "0ae87165-2713-45c6-b69b-23cd9c8f4870",
   "metadata": {
    "language": "sql",
    "name": "cell27"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION echo_varchar(x VARCHAR)\n  RETURNS VARCHAR\n  LANGUAGE JAVA\n  CALLED ON NULL INPUT\n  HANDLER = 'TestFunc.echoVarchar'\n  TARGET_PATH = '@~/testfunc.jar'\n  AS\n  'class TestFunc {\n    public static String echoVarchar(String x) {\n      return x;\n    }\n  }';\n\n  SELECT echo_varchar('Hello Java');\n\n\n  ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2e2d869-168e-410b-8327-05fed3ec866e",
   "metadata": {
    "name": "cell30",
    "collapsed": false
   },
   "source": "### Step 3.4: Passing an OBJECT to an In-Line Java UDF\n\nThe following example demonstrates using the SQL `OBJECT` data type and the corresponding Java `Map<String, String>` type to extract a value from the object. It also shows how to pass multiple parameters to a Java UDF.\n"
  },
  {
   "cell_type": "code",
   "id": "ebbe228e-c2e5-4583-8c45-c21e4d6e2eb3",
   "metadata": {
    "language": "sql",
    "name": "cell31"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE objectives (o OBJECT);\nINSERT INTO objectives SELECT PARSE_JSON('{\"outer_key\" : {\"inner_key\" : \"inner_value\"} }');\n\nCREATE OR REPLACE FUNCTION extract_from_object(x OBJECT, key VARCHAR)\n  RETURNS VARIANT\n  LANGUAGE JAVA\n  HANDLER = 'VariantLibrary.extract'\n  TARGET_PATH = '@~/VariantLibrary.jar'\n  AS\n  $$\n  import java.util.Map;\n  class VariantLibrary {\n    public static String extract(Map<String, String> m, String key) {\n      return m.get(key);\n    }\n  }\n  $$;\n\n  SELECT extract_from_object(o, 'outer_key'), \n       extract_from_object(o, 'outer_key')['inner_key'] FROM objectives;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cb836511-43d5-4dc8-9ed4-7bef3138dd1f",
   "metadata": {
    "name": "cell33",
    "collapsed": false
   },
   "source": "### Step 3.5: Passing a GEOGRAPHY Value to an In-Line Java UDF\n\nThis example demonstrates how to pass a `GEOGRAPHY` value to an in-line Java UDF, enabling spatial data processing within the function.\n"
  },
  {
   "cell_type": "code",
   "id": "ac779fa0-bb92-4afc-9cd7-07f13c93ab98",
   "metadata": {
    "language": "sql",
    "name": "cell32"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION geography_equals(x GEOGRAPHY, y GEOGRAPHY)\n  RETURNS BOOLEAN\n  LANGUAGE JAVA\n  PACKAGES = ('com.snowflake:snowpark:1.2.0')\n  HANDLER = 'TestGeography.compute'\n  AS\n  $$\n  import com.snowflake.snowpark_java.types.Geography;\n\n  class TestGeography {\n    public static boolean compute(Geography geo1, Geography geo2) {\n      return geo1.equals(geo2);\n    }\n  }\n  $$;\n\nCREATE OR REPLACE TABLE geocache_table (id INTEGER, g1 GEOGRAPHY, g2 GEOGRAPHY);\n\nINSERT INTO geocache_table (id, g1, g2)\n  SELECT 1, TO_GEOGRAPHY('POINT(-122.35 37.55)'), TO_GEOGRAPHY('POINT(-122.35 37.55)');\nINSERT INTO geocache_table (id, g1, g2)\n  SELECT 2, TO_GEOGRAPHY('POINT(-122.35 37.55)'), TO_GEOGRAPHY('POINT(90.0 45.0)');\n\nSELECT id, g1, g2, geography_equals(g1, g2) AS \"EQUAL?\"\n  FROM geocache_table\n  ORDER BY id;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6303d1d9-011a-43f9-8174-9fd0d45b4972",
   "metadata": {
    "name": "cell34",
    "collapsed": false
   },
   "source": "### 3.6: Reading a File with a Java UDF\n\nYou can read a file's contents within a Java UDF handler to process unstructured data. The file must be on a Snowflake stage accessible to your handler. \n\nTo read staged files, your handler can:\n\n- **Statically-specified file**: Access a file by specifying its path in the `IMPORTS` clause, useful for initialization.\n  \n- **Dynamically-specified file**: Use `SnowflakeFile` or `InputStream` methods to read a file specified at runtime by the caller.\n\n`SnowflakeFile` provides additional features compared to `InputStream`."
  },
  {
   "cell_type": "code",
   "id": "c9311a62-051c-4550-b501-980f81a94f6e",
   "metadata": {
    "language": "sql",
    "name": "cell35"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FUNCTION content(file STRING)\n  RETURNS INTEGER\n  LANGUAGE JAVA\n  HANDLER = 'Sales.content'\n  TARGET_PATH = '@sales_data_stage/sales_functions23.jar'\n  AS\n  $$\n  import java.io.InputStream;\n  import java.io.IOException;\n  import java.nio.charset.StandardCharsets;\n  import com.snowflake.snowpark_java.types.SnowflakeFile;\n\n  public class Sales {\n\n    public static String content(String filePath) throws IOException {\n\n      SnowflakeFile file = SnowflakeFile.newInstance(filePath);\n      InputStream stream = file.getInputStream();\n      String contents = new String(stream.readAllBytes(), StandardCharsets.UTF_8);\n      return contents;\n    }\n  }\n  $$;\n\nSELECT content(BUILD_SCOPED_FILE_URL('@sales_data_stage', '/car_sales.json'));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6171f1f9-275d-4300-8c65-990ad6d5bc45",
   "metadata": {
    "name": "cell36",
    "collapsed": false
   },
   "source": "## 🧠 Stored Procedures vs. UDFs: Know the Difference\n\nSnowflake gives you two powerful ways to add custom logic: **Stored Procedures** and **User-Defined Functions**. Here’s a quick comparison:\n\n| Feature           | Stored Procedure                                 | User-Defined Function (UDF)                          |\n|-------------------|--------------------------------------------------|------------------------------------------------------|\n| **Purpose**        | Perform admin or batch operations using SQL.     | Return a computed value, often used in queries.      |\n| **Return Value**   | Optional — may return status or custom values.   | Required — must return a value explicitly.           |\n| **SQL Integration**| Called as stand-alone SQL commands.              | Embedded inline in SQL (e.g., `SELECT MyFunc(col)`). |\n| **Best For**       | DDL/DML, workflows, automation.                  | Transformations, expressions, calculations.          |\n\nAdditionally:\n\n1- UDFs return a value; stored procedures need not\n2- UDF return values are directly usable in SQL; stored procedure return values may not be\n3- UDFs can be called in the context of another statement; stored procedures are called independently\n4- Multiple UDFs may be called with one statement; a single stored procedure is called with one statement\n5- UDFs may access the database with simple queries only; stored procedures can execute DDL and DML statements¶"
  },
  {
   "cell_type": "markdown",
   "id": "30a04740-1d7e-41b3-8c4b-4ade1abd1ab6",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "# Final Thoughts\nThis notebook explored key techniques for building powerful Java-based solutions within Snowflake using Snowpark APIs. We covered creating and calling Java stored procedures and UDFs, performing asynchronous operations, handling unstructured data through file access, and returning tabular results using DataFrames. These tools allow you to extend Snowflake's capabilities with custom logic, parallelism, and integration with external data formats.\n\nAs you continue to develop with Java in Snowflake, consider how these features can help optimize your data workflows and unlock more complex processing scenarios. Whether you're encapsulating business logic, processing files at scale, or improving performance with parallel execution, Snowflake's support for Java gives you the flexibility to build scalable and maintainable solutions.\n\n### Resources\n\n- [Snowflake Java UDFs Documentation](https://docs.snowflake.com/en/developer-guide/udf/java/udf-java-introduction)\n- [Creating Stored Procedures in Java](https://docs.snowflake.com/en/developer-guide/stored-procedure/java/procedure-java-overview)\n- [Quickstarts](https://quickstarts.snowflake.com/)\n"
  }
 ]
}