{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb54abc",
   "metadata": {},
   "source": [
    "- Required snowflake-ml-python version **1.5.5** or higher\n",
    "- Last updated on: 7/22/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae3429",
   "metadata": {},
   "source": [
    "# End to end ML with Feature Store and Model Registry\n",
    "\n",
    "This notebook demonstrates an end-to-end ML experiment cycle including feature creation, training data generation, model training and inference. The workflow touches on key Snowflake ML features including [Snowflake Feature Store](https://docs.snowflake.com/en/developer-guide/snowpark-ml/feature-store/overview), [Dataset](https://docs.snowflake.com/en/developer-guide/snowpark-ml/dataset), ML Lineage, [Snowpark ML Modeling](https://docs.snowflake.com/en/developer-guide/snowpark-ml/modeling) and [Snowflake Model Registry](https://docs.snowflake.com/en/developer-guide/snowpark-ml/model-registry/overview). \n",
    "\n",
    "**Table of contents**\n",
    "- [Set up test environment](#setup-test-env)\n",
    "  - [Connect to Snowflake](#connect-to-snowflake)\n",
    "  - [Select your example](#select-your-example)\n",
    "- [Create features with Feature Store](#create-features-with-feature-store)\n",
    "  - [Initialize Feature Store](#initialize-feature-store)\n",
    "  - [Register entities and feature views](#register-new-entities-and-feature-views)\n",
    "- [Generate Training Data](#gen-training-data)\n",
    "- [Train model with Snowpark ML](#train-with-snowpark-ml)\n",
    "- [Log models in Model Registry](#log-models-in-model-registry)\n",
    "- [Query lineage](#query-lineage)\n",
    "- [Predict with model](#predict-with-model)\n",
    "  - [Predict with local model](#predict-with-local-model)\n",
    "  - [Predict with Model Registry](#predict-with-model-registry)\n",
    "- [Clean up notebook](#cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f16e6a8",
   "metadata": {},
   "source": [
    "<a id='setup-test-env'></a>\n",
    "## Set up test environment\n",
    "\n",
    "<a id='connect-to-snowflake'></a>\n",
    "### Connect to Snowflake\n",
    "\n",
    "Let's start with setting up our test environment. We will create a session and a schema. The schema `FS_DEMO_SCHEMA` will be used as the Feature Store. It will be cleaned up at the end of the demo. You need to fill the `connection_parameters` with your Snowflake connection information. Follow this **[guide](https://docs.snowflake.com/en/developer-guide/snowpark/python/creating-session)** for more details about how to connect to Snowflake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9622928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session, context, exceptions\n",
    "\n",
    "try:\n",
    "    # Retrieve active session if in Snowpark Notebook\n",
    "    session = context.get_active_session()\n",
    "except exceptions.SnowparkSessionException:\n",
    "    # ACTION REQUIRED: Need to manually configure Snowflake connection if using Jupyter\n",
    "    connection_parameters = {\n",
    "        \"account\": \"<your snowflake account>\",\n",
    "        \"user\": \"<your snowflake user>\",\n",
    "        \"password\": \"<your snowflake password>\",\n",
    "        \"role\": \"<your snowflake role>\",\n",
    "        \"warehouse\": \"<your snowflake warehouse>\",\n",
    "        \"database\": \"<your snowflake database>\",\n",
    "        \"schema\": \"<your snowflake schema>\",\n",
    "    }\n",
    "\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "assert session.get_current_database() != None, \"Session must have a database for the demo.\"\n",
    "assert session.get_current_warehouse() != None, \"Session must have a warehouse for the demo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f38d70d-ccc2-40b9-8020-50e3ad3ff165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Schema SNOWFLAKE_FEATURE_STORE_NOTEBOOK_DEMO_MODEL successfully created.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The schema where Feature Store will initialize on and test dataset stores.\n",
    "FS_DEMO_SCHEMA = \"SNOWFLAKE_FEATURE_STORE_NOTEBOOK_DEMO\"\n",
    "# the schema model lives.\n",
    "MODEL_DEMO_SCHEMA = \"SNOWFLAKE_FEATURE_STORE_NOTEBOOK_DEMO_MODEL\"\n",
    "\n",
    "# Make sure your role has CREATE SCHEMA privileges or USAGE privileges on the schema if it already exists.\n",
    "session.sql(f\"CREATE OR REPLACE SCHEMA {FS_DEMO_SCHEMA}\").collect()\n",
    "session.sql(f\"CREATE OR REPLACE SCHEMA {MODEL_DEMO_SCHEMA}\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f89f3-d84d-4226-830d-3a967499fed7",
   "metadata": {},
   "source": [
    "<a id='select-your-example'></a>\n",
    "### Select your example\n",
    "\n",
    "We have prepared some examples that you can find in our [open source repo](https://github.com/snowflakedb/snowflake-ml-python/tree/main/snowflake/ml/feature_store/examples). Each example contains the source dataset, feature view and entity definitions which will be used in this demo. `ExampleHelper` (included in snowflake-ml-python) will setup everything with simple APIs and you don't have to worry about the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbb04de-193c-44e1-b400-802e22eb6941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All examples: ['new_york_taxi_features', 'citibike_trip_features', 'wine_quality_features']\n"
     ]
    }
   ],
   "source": [
    "from snowflake.ml.feature_store.examples.example_helper import ExampleHelper\n",
    "\n",
    "example_helper = ExampleHelper(session, session.get_current_database(), FS_DEMO_SCHEMA)\n",
    "print(f\"All examples: {example_helper.list_examples()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f909c72-e3c0-4834-a935-24a689101979",
   "metadata": {},
   "source": [
    "`load_example()` will load the source data into Snowflake tables. In the example below, we are using the “wine_quality_features” example. You can replace this with any example listed above. Execution of the cell below may take some time depending on the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c12da2-71cc-4c90-89aa-b1bd474ae975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"REGTEST_DB\".SNOWFLAKE_FEATURE_STORE_NOTEBOOK_DEMO.nyc_yellow_trips']\n"
     ]
    }
   ],
   "source": [
    "# replace the value with the example you want to run\n",
    "source_tables = example_helper.load_example('new_york_taxi_features')\n",
    "print(source_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a73b0c-41dd-47f7-b7a1-1da492d23b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VENDORID</th>\n",
       "      <th>PASSENGER_COUNT</th>\n",
       "      <th>TRIP_DISTANCE</th>\n",
       "      <th>RATECODEID</th>\n",
       "      <th>STORE_AND_FWD_FLAG</th>\n",
       "      <th>PULOCATIONID</th>\n",
       "      <th>DOLOCATIONID</th>\n",
       "      <th>PAYMENT_TYPE</th>\n",
       "      <th>FARE_AMOUNT</th>\n",
       "      <th>EXTRA</th>\n",
       "      <th>MTA_TAX</th>\n",
       "      <th>TIP_AMOUNT</th>\n",
       "      <th>TOLLS_AMOUNT</th>\n",
       "      <th>IMPROVEMENT_SURCHARGE</th>\n",
       "      <th>TOTAL_AMOUNT</th>\n",
       "      <th>CONGESTION_SURCHARGE</th>\n",
       "      <th>AIRPORT_FEE</th>\n",
       "      <th>TPEP_PICKUP_DATETIME</th>\n",
       "      <th>TPEP_DROPOFF_DATETIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>48</td>\n",
       "      <td>262</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>18.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-01 00:12:22</td>\n",
       "      <td>2016-01-01 00:29:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>162</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-01 00:41:31</td>\n",
       "      <td>2016-01-01 00:55:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>246</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-01 00:53:37</td>\n",
       "      <td>2016-01-01 00:59:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>170</td>\n",
       "      <td>162</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-01 00:13:28</td>\n",
       "      <td>2016-01-01 00:18:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-01 00:33:04</td>\n",
       "      <td>2016-01-01 00:47:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VENDORID  PASSENGER_COUNT  TRIP_DISTANCE  RATECODEID STORE_AND_FWD_FLAG  \\\n",
       "0         1                1            3.2           1                  N   \n",
       "1         1                2            1.0           1                  N   \n",
       "2         1                1            0.9           1                  N   \n",
       "3         1                1            0.8           1                  N   \n",
       "4         1                1            1.8           1                  N   \n",
       "\n",
       "   PULOCATIONID  DOLOCATIONID  PAYMENT_TYPE  FARE_AMOUNT  EXTRA  MTA_TAX  \\\n",
       "0            48           262             1         14.0    0.5      0.5   \n",
       "1           162            48             2          9.5    0.5      0.5   \n",
       "2           246            90             2          6.0    0.5      0.5   \n",
       "3           170           162             2          5.0    0.5      0.5   \n",
       "4           161           140             2         11.0    0.5      0.5   \n",
       "\n",
       "   TIP_AMOUNT  TOLLS_AMOUNT  IMPROVEMENT_SURCHARGE  TOTAL_AMOUNT  \\\n",
       "0        3.06           0.0                    0.3         18.36   \n",
       "1        0.00           0.0                    0.3         10.80   \n",
       "2        0.00           0.0                    0.3          7.30   \n",
       "3        0.00           0.0                    0.3          6.30   \n",
       "4        0.00           0.0                    0.3         12.30   \n",
       "\n",
       "   CONGESTION_SURCHARGE  AIRPORT_FEE TPEP_PICKUP_DATETIME  \\\n",
       "0                   NaN          NaN  2016-01-01 00:12:22   \n",
       "1                   NaN          NaN  2016-01-01 00:41:31   \n",
       "2                   NaN          NaN  2016-01-01 00:53:37   \n",
       "3                   NaN          NaN  2016-01-01 00:13:28   \n",
       "4                   NaN          NaN  2016-01-01 00:33:04   \n",
       "\n",
       "  TPEP_DROPOFF_DATETIME  \n",
       "0   2016-01-01 00:29:14  \n",
       "1   2016-01-01 00:55:10  \n",
       "2   2016-01-01 00:59:57  \n",
       "3   2016-01-01 00:18:07  \n",
       "4   2016-01-01 00:47:14  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display as Pandas DataFrame\n",
    "session.table(source_tables[0]).limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038124f-123b-4d22-9058-4fe2e57af6fe",
   "metadata": {},
   "source": [
    "<a id='create-features-with-feature-store'></a>\n",
    "## Create features with Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece7a2b",
   "metadata": {},
   "source": [
    "<a id='initialize-feature-store'></a>\n",
    "### Initialize Feature Store\n",
    "\n",
    "Let's first create a feature store client. With `CREATE_IF_NOT_EXIST` mode, it will try to create a new Feature Store schema and all necessary feature store metadata if it doesn't exist already. It is required for the first time to set up a Feature Store. Afterwards, you can use `FAIL_IF_NOT_EXIST` mode to connect to an existing Feature Store. \n",
    "\n",
    "Note that the database being used must already exist. Feature Store will **NOT** try to create the database even in `CREATE_IF_NOT_EXIST` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe850ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode\n",
    ")\n",
    "\n",
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=session.get_current_database(), \n",
    "    name=FS_DEMO_SCHEMA, \n",
    "    default_warehouse=session.get_current_warehouse(),\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b7ad1",
   "metadata": {},
   "source": [
    "<a id='register-new-entities-and-feature-views'></a>\n",
    "### Register entities and feature views\n",
    "\n",
    "Next we register new entities and feature views in Feature Store. Entities will be the join keys used to generate training data. Feature Views contains all the features you need for your model training and inference. We have entities and feature views for this example defined in our [open source repo](https://github.com/snowflakedb/snowflake-ml-python/tree/main/snowflake/ml/feature_store/examples). We will load the definitions with `load_entities()` and `load_draft_feature_views()` for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe57406-6834-428d-8772-8d7e1265b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "|\"NAME\"        |\"JOIN_KEYS\"       |\"DESC\"                |\"OWNER\"     |\n",
      "-----------------------------------------------------------------------\n",
      "|TRIP_DROPOFF  |[\"DOLOCATIONID\"]  |Trip dropoff entity.  |REGTEST_RL  |\n",
      "|TRIP_PICKUP   |[\"PULOCATIONID\"]  |Trip pickup entity.   |REGTEST_RL  |\n",
      "-----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_entities = []\n",
    "for e in example_helper.load_entities():\n",
    "    entity = fs.register_entity(e)\n",
    "    all_entities.append(entity)\n",
    "fs.list_entities().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "415e5e69-d605-4285-bb8b-e4d378cc35e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "|\"NAME\"          |\"VERSION\"  |\"DESC\"                                              |\"REFRESH_FREQ\"  |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "|F_TRIP_DROPOFF  |1.0        |Managed feature view trip dropoff refreshed eve...  |12 hours        |\n",
      "|F_TRIP_PICKUP   |1.0        |Managed feature view trip pickup refreshed ever...  |1 day           |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_feature_views = []\n",
    "for fv in example_helper.load_draft_feature_views():\n",
    "    rf = fs.register_feature_view(\n",
    "        feature_view=fv,\n",
    "        version='1.0'\n",
    "    )\n",
    "    all_feature_views.append(rf)\n",
    "\n",
    "fs.list_feature_views().select('name', 'version', 'desc', 'refresh_freq').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1a7dc",
   "metadata": {},
   "source": [
    "<a id='gen-training-data'></a>\n",
    "## Generate Training Data\n",
    "\n",
    "After our feature pipelines are fully setup, we can use them to generate [Snowflake Dataset](https://docs.snowflake.com/en/developer-guide/snowpark-ml/dataset) and later do model training. Generating training data is easy since materialized FeatureViews already carry most of the metadata like join keys, timestamp for point-in-time lookup, etc. We just need to provide the spine data (it's called spine because it is the list of entity IDs that we are essentially enriching by joining features with it).\n",
    "\n",
    "`generate_dataset()` returns a Snowflake Dataset object, which is best for distributed training with deep learning frameworks like TensorFlow or Pytorch which requires fine-grained file-level access. It creates a new Dataset object (which is versioned and immutable) in Snowflake which materializes the data in Parquet files. If you train models with classic ML libraries like Snowpark ML or scikit-learn, you can use `generate_training_set()` which returns a classic Snowflake table. The Cell below demonstrates `generate_dataset()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec901a65-f3ee-4c12-b8ff-ec0f630e5372",
   "metadata": {},
   "source": [
    "Retrieve some metadata columns that are essential when generating training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6c80b76-99a4-4eb0-b0cb-583afa434ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp col: TPEP_PICKUP_DATETIME\n",
      "excluded cols: []\n",
      "label cols: ['FARE_AMOUNT']\n",
      "join keys: ['PULOCATIONID', 'DOLOCATIONID']\n"
     ]
    }
   ],
   "source": [
    "label_cols = example_helper.get_label_cols()\n",
    "timestamp_col = example_helper.get_training_data_timestamp_col()\n",
    "excluded_cols = example_helper.get_excluded_cols()\n",
    "join_keys = [key for entity in all_entities for key in entity.join_keys]\n",
    "print(f'timestamp col: {timestamp_col}')\n",
    "print(f'excluded cols: {excluded_cols}')\n",
    "print(f'label cols: {label_cols}')\n",
    "print(f'join keys: {join_keys}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66ad1d-5ad1-4ad8-92f9-c65be491e0c3",
   "metadata": {},
   "source": [
    "Create a spine dataframe that's sampled from source table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea00fe3a-ac16-45d7-95c0-7ea7ed344e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FARE_AMOUNT</th>\n",
       "      <th>PULOCATIONID</th>\n",
       "      <th>DOLOCATIONID</th>\n",
       "      <th>TPEP_PICKUP_DATETIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.5</td>\n",
       "      <td>161</td>\n",
       "      <td>68</td>\n",
       "      <td>2016-01-08 10:47:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>234</td>\n",
       "      <td>114</td>\n",
       "      <td>2016-01-09 17:14:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>87</td>\n",
       "      <td>231</td>\n",
       "      <td>2016-01-08 13:26:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.5</td>\n",
       "      <td>170</td>\n",
       "      <td>79</td>\n",
       "      <td>2016-01-09 10:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>97</td>\n",
       "      <td>143</td>\n",
       "      <td>2016-01-07 22:11:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>13.0</td>\n",
       "      <td>231</td>\n",
       "      <td>48</td>\n",
       "      <td>2016-01-04 18:00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>52.0</td>\n",
       "      <td>132</td>\n",
       "      <td>244</td>\n",
       "      <td>2016-01-06 06:16:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>12.5</td>\n",
       "      <td>226</td>\n",
       "      <td>162</td>\n",
       "      <td>2016-01-05 08:29:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>7.0</td>\n",
       "      <td>79</td>\n",
       "      <td>107</td>\n",
       "      <td>2016-01-06 18:02:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>22.0</td>\n",
       "      <td>161</td>\n",
       "      <td>33</td>\n",
       "      <td>2016-01-05 22:55:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FARE_AMOUNT  PULOCATIONID  DOLOCATIONID TPEP_PICKUP_DATETIME\n",
       "0            8.5           161            68  2016-01-08 10:47:49\n",
       "1            6.0           234           114  2016-01-09 17:14:42\n",
       "2            5.0            87           231  2016-01-08 13:26:55\n",
       "3            7.5           170            79  2016-01-09 10:45:00\n",
       "4           28.0            97           143  2016-01-07 22:11:59\n",
       "..           ...           ...           ...                  ...\n",
       "507         13.0           231            48  2016-01-04 18:00:32\n",
       "508         52.0           132           244  2016-01-06 06:16:31\n",
       "509         12.5           226           162  2016-01-05 08:29:48\n",
       "510          7.0            79           107  2016-01-06 18:02:51\n",
       "511         22.0           161            33  2016-01-05 22:55:57\n",
       "\n",
       "[512 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_count = 512\n",
    "source_df = session.sql(f\"\"\"\n",
    "    select {','.join(label_cols)}, \n",
    "            {','.join(join_keys)} \n",
    "            {',' + timestamp_col if timestamp_col is not None else ''} \n",
    "    from {source_tables[0]}\"\"\")\n",
    "spine_df = source_df.sample(n=sample_count)\n",
    "# preview spine dataframe\n",
    "spine_df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8b59a-dcae-41a4-8954-db6719c5cf60",
   "metadata": {},
   "source": [
    "Generate dataset object from spine dataframe and feature views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "574a810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = fs.generate_dataset(\n",
    "    name=\"my_cool_training_dataset\",\n",
    "    spine_df=spine_df, \n",
    "    features=all_feature_views,\n",
    "    version=\"4.0\",\n",
    "    spine_timestamp_col=timestamp_col,\n",
    "    spine_label_cols=label_cols,\n",
    "    exclude_columns=excluded_cols,\n",
    "    desc=\"This is the dataset joined spine dataframe with feature views\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1cf2f4-59b3-40da-8c43-bee27129105d",
   "metadata": {},
   "source": [
    "Convert dataset to a snowpark dataframe and examine all the features in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3c71aa-1c6b-4bf4-83f9-2176dc249f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FARE_AMOUNT</th>\n",
       "      <th>PULOCATIONID</th>\n",
       "      <th>DOLOCATIONID</th>\n",
       "      <th>TPEP_PICKUP_DATETIME</th>\n",
       "      <th>TRIP_COUNT_1H</th>\n",
       "      <th>TRIP_COUNT_5H</th>\n",
       "      <th>MEAN_FARE_2H</th>\n",
       "      <th>MEAN_FARE_5H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>125</td>\n",
       "      <td>52</td>\n",
       "      <td>2016-01-09 01:51:54</td>\n",
       "      <td>42</td>\n",
       "      <td>164</td>\n",
       "      <td>11.920779</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>125</td>\n",
       "      <td>231</td>\n",
       "      <td>2016-01-20 18:24:09</td>\n",
       "      <td>317</td>\n",
       "      <td>1532</td>\n",
       "      <td>11.256917</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>125</td>\n",
       "      <td>88</td>\n",
       "      <td>2016-01-22 00:49:05</td>\n",
       "      <td>65</td>\n",
       "      <td>317</td>\n",
       "      <td>14.458763</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.5</td>\n",
       "      <td>138</td>\n",
       "      <td>48</td>\n",
       "      <td>2016-01-05 21:56:43</td>\n",
       "      <td>591</td>\n",
       "      <td>2974</td>\n",
       "      <td>28.401669</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>138</td>\n",
       "      <td>45</td>\n",
       "      <td>2016-01-06 23:03:19</td>\n",
       "      <td>50</td>\n",
       "      <td>225</td>\n",
       "      <td>28.584057</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>6.5</td>\n",
       "      <td>162</td>\n",
       "      <td>230</td>\n",
       "      <td>2016-01-19 08:05:47</td>\n",
       "      <td>885</td>\n",
       "      <td>1946</td>\n",
       "      <td>9.193698</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>7.5</td>\n",
       "      <td>162</td>\n",
       "      <td>234</td>\n",
       "      <td>2016-01-20 00:37:15</td>\n",
       "      <td>165</td>\n",
       "      <td>1995</td>\n",
       "      <td>11.385578</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>10.0</td>\n",
       "      <td>162</td>\n",
       "      <td>107</td>\n",
       "      <td>2016-01-21 19:45:20</td>\n",
       "      <td>692</td>\n",
       "      <td>2307</td>\n",
       "      <td>11.164576</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>10.0</td>\n",
       "      <td>162</td>\n",
       "      <td>113</td>\n",
       "      <td>2016-01-22 18:56:25</td>\n",
       "      <td>460</td>\n",
       "      <td>1692</td>\n",
       "      <td>10.909946</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>51.0</td>\n",
       "      <td>162</td>\n",
       "      <td>21</td>\n",
       "      <td>2016-01-28 00:52:04</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>11.724587</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FARE_AMOUNT  PULOCATIONID  DOLOCATIONID TPEP_PICKUP_DATETIME  \\\n",
       "0           15.0           125            52  2016-01-09 01:51:54   \n",
       "1            5.0           125           231  2016-01-20 18:24:09   \n",
       "2            7.0           125            88  2016-01-22 00:49:05   \n",
       "3           32.5           138            48  2016-01-05 21:56:43   \n",
       "4           41.0           138            45  2016-01-06 23:03:19   \n",
       "..           ...           ...           ...                  ...   \n",
       "507          6.5           162           230  2016-01-19 08:05:47   \n",
       "508          7.5           162           234  2016-01-20 00:37:15   \n",
       "509         10.0           162           107  2016-01-21 19:45:20   \n",
       "510         10.0           162           113  2016-01-22 18:56:25   \n",
       "511         51.0           162            21  2016-01-28 00:52:04   \n",
       "\n",
       "     TRIP_COUNT_1H  TRIP_COUNT_5H  MEAN_FARE_2H  MEAN_FARE_5H  \n",
       "0               42            164     11.920779           1.0  \n",
       "1              317           1532     11.256917           1.0  \n",
       "2               65            317     14.458763           1.0  \n",
       "3              591           2974     28.401669           1.0  \n",
       "4               50            225     28.584057           1.0  \n",
       "..             ...            ...           ...           ...  \n",
       "507            885           1946      9.193698           1.0  \n",
       "508            165           1995     11.385578           1.0  \n",
       "509            692           2307     11.164576           1.0  \n",
       "510            460           1692     10.909946           1.0  \n",
       "511              7             29     11.724587           1.0  \n",
       "\n",
       "[512 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_df = my_dataset.read.to_snowpark_dataframe()\n",
    "assert training_data_df.count() == sample_count\n",
    "# drop rows that have any nulls in value. \n",
    "training_data_df = training_data_df.dropna(how='any')\n",
    "training_data_df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca7543",
   "metadata": {},
   "source": [
    "<a id='train-with-snowpark-ml'></a>\n",
    "## Train model with Snowpark ML\n",
    "\n",
    "Now let's train a simple random forest model, and evaluate the prediction accuracy. When you call fit() on a DataFrame that is created from a  Dataset, the linkage between the trained model and dataset is automatically wired up. Later, you can easily retrieve the training dataset from this model, or you can query the lineage about the dataset and model. This is work-in-progress and will be available  soon in an upcoming release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "352603a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature cols: ['MEAN_FARE_2H', 'MEAN_FARE_5H', 'TRIP_COUNT_5H', 'TRIP_COUNT_1H']\n",
      "MSE: 101.03885521242017, Accuracy: 99.50167389473202\n"
     ]
    }
   ],
   "source": [
    "from snowflake.ml.modeling.ensemble import RandomForestRegressor\n",
    "from snowflake.ml.modeling import metrics as snowml_metrics\n",
    "from snowflake.snowpark.functions import abs as sp_abs, mean, col\n",
    "\n",
    "def train_model_using_snowpark_ml(training_data_df):\n",
    "    train, test = training_data_df.random_split([0.8, 0.2], seed=42)\n",
    "    feature_columns = list(set(training_data_df.columns) - set(label_cols) - set(join_keys) - set([timestamp_col]))\n",
    "    print(f\"feature cols: {feature_columns}\")\n",
    "    \n",
    "    rf = RandomForestRegressor(\n",
    "        input_cols=feature_columns, label_cols=label_cols, \n",
    "        max_depth=3, n_estimators=20, random_state=42\n",
    "    )\n",
    "\n",
    "    rf.fit(train)\n",
    "    predictions = rf.predict(test)\n",
    "\n",
    "    output_label_names = ['OUTPUT_' + col for col in label_cols]\n",
    "    mse = snowml_metrics.mean_squared_error(\n",
    "        df=predictions, \n",
    "        y_true_col_names=label_cols, \n",
    "        y_pred_col_names=output_label_names\n",
    "    )\n",
    "\n",
    "    accuracy = 100 - snowml_metrics.mean_absolute_percentage_error(\n",
    "        df=predictions,\n",
    "        y_true_col_names=label_cols,\n",
    "        y_pred_col_names=output_label_names\n",
    "    )\n",
    "\n",
    "    print(f\"MSE: {mse}, Accuracy: {accuracy}\")\n",
    "    return rf\n",
    "\n",
    "random_forest_model = train_model_using_snowpark_ml(training_data_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b12493-3339-4369-82d4-46a4389f6bf1",
   "metadata": {},
   "source": [
    "<a id=\"log-models-in-model-registry\"></a>\n",
    "## Log model in Model Registry\n",
    "\n",
    "After the model is trained, we can save the model into Model Registry so we can manage the model, its metadata including metrics, versions, and use it later for inference. Also, ML lineage is built automatically between the model, dataset and feature views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf1209a6-4c8c-4441-9c5d-7688f4ec0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "registry = Registry(\n",
    "    session=session, \n",
    "    database_name=session.get_current_database(), \n",
    "    schema_name=MODEL_DEMO_SCHEMA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bcf485-9c9a-4a74-a8a2-3154c8f5aa74",
   "metadata": {},
   "source": [
    "Log model into Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ded2ec74-fd46-445d-8fe9-1b133e4284eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wezhou/miniconda3/envs/py38/lib/python3.8/contextlib.py:113: UserWarning: `relax_version` is not set and therefore defaulted to True. Dependency version constraints relaxed from ==x.y.z to >=x.y, <(x+1). To use specific dependency versions for compatibility, reproducibility, etc., set `options={'relax_version': False}` when logging the model.\n",
      "  return next(self.gen)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelVersion(\n",
       "  name='MY_RANDOM_FOREST_REGRESSOR_MODEL',\n",
       "  version='V1',\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"MY_RANDOM_FOREST_REGRESSOR_MODEL\"\n",
    "\n",
    "registry.log_model(\n",
    "    model_name=model_name,\n",
    "    version_name=\"v1\",\n",
    "    model=random_forest_model,\n",
    "    comment=\"My model trained with feature views, dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df35d75-072d-484b-9ce4-c5b02fa4eae4",
   "metadata": {},
   "source": [
    "<a id=\"query-lineage\"></a>\n",
    "## Query lineage\n",
    "We can now query the lineage from an object. You can call `lineage()` on any object and it returns a set of objects that it has dependency with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de446b09-c3d0-4c3b-91e8-aefa104229e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.snowpark:LineageNode.lineage() is in private preview since 1.5.3. Do not use it in production. \n",
      "WARNING:snowflake.snowpark:Lineage.trace() is in private preview since 1.16.0. Do not use it in production. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Dataset(\n",
       "   name='REGTEST_DB.SNOWFLAKE_FEATURE_STORE_NOTEBOOK_DEMO.MY_COOL_TRAINING_DATASET',\n",
       "   version='4.0',\n",
       " )]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = registry.get_model(model_name).version(\"v1\")\n",
    "model.lineage(direction=\"upstream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c97f25-d063-45e6-b073-fbe0f61ffc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelVersion(\n",
       "   name='MY_RANDOM_FOREST_REGRESSOR_MODEL',\n",
       "   version='V1',\n",
       " )]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset.lineage(direction=\"downstream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99250b6-9ede-416e-b5d0-14693c8ecc43",
   "metadata": {},
   "source": [
    "There's a bug causing below cell not return Dataset as downstream lineage object of feature view. We are working on fixing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c0828fa-e56f-43d7-89fc-dafa7dcce9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fv in all_feature_views:\n",
    "    fv.lineage(direction='downstream')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8031f",
   "metadata": {},
   "source": [
    "<a id='predict-with-model'></a>\n",
    "## Predict with model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee15a59b-5201-4772-a376-0bb2043da37f",
   "metadata": {},
   "source": [
    "Finally we are almost ready for prediction! For this, we can look up the latest feature values from Feature Store for the specific data records that we are running prediction on. One of the key benefits of using the Feature Store is that it provides a way to automatically serve up the right feature values during prediction with point-in-time correct feature values. `load_feature_views_from_dataset()` gets the same feature views used in training, then `retrieve_feature_values()` lookups the latest feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9452d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------\n",
      "|\"FARE_AMOUNT\"  |\"TPEP_PICKUP_DATETIME\"  |\"TRIP_COUNT_1H\"  |\"TRIP_COUNT_5H\"  |\"MEAN_FARE_2H\"      |\"MEAN_FARE_5H\"  |\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "|33.0           |2016-01-10 20:11:11     |27               |112              |11.381987577639752  |1.000000        |\n",
      "|15.5           |2016-01-28 03:18:50     |35               |907              |12.427487352445194  |1.000000        |\n",
      "|6.5            |2016-01-07 16:49:59     |495              |3035             |10.373705179282869  |1.000000        |\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = source_df.sample(n=3)\n",
    "\n",
    "# load back feature views from dataset\n",
    "fvs = fs.load_feature_views_from_dataset(my_dataset)\n",
    "enriched_df = fs.retrieve_feature_values(\n",
    "    test_df, \n",
    "    features=fvs,\n",
    "    exclude_columns=join_keys,\n",
    "    spine_timestamp_col=timestamp_col\n",
    ")\n",
    "enriched_df = enriched_df.drop(join_keys)\n",
    "enriched_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2f69f-1597-4f43-9b59-554c48112a47",
   "metadata": {},
   "source": [
    "<a id='predict-with-local-model'></a>\n",
    "### [Optional 1] predict with local model\n",
    "Now we can predict with a local model and the feature values retrieved from feature store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46c2546e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FARE_AMOUNT TPEP_PICKUP_DATETIME  TRIP_COUNT_1H  TRIP_COUNT_5H  \\\n",
      "0         10.0  2016-01-28 08:57:11           1559           4636   \n",
      "1          6.5  2016-01-29 07:02:50            137            239   \n",
      "2          7.0  2016-01-09 10:06:25            225            521   \n",
      "\n",
      "   MEAN_FARE_2H  MEAN_FARE_5H  OUTPUT_FARE_AMOUNT  \n",
      "0      9.863181           1.0            8.751825  \n",
      "1     23.330000           1.0           21.714971  \n",
      "2      9.970138           1.0           10.083706  \n"
     ]
    }
   ],
   "source": [
    "pred = random_forest_model.predict(enriched_df.to_pandas())\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b81639",
   "metadata": {},
   "source": [
    "<a id='predict-with-model-registry'></a>\n",
    "### [Option 2] Predict with Model Registry\n",
    "\n",
    "We can also retrieve the model from model registry and run  predictions on the model using latest feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d7fd017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MEAN_FARE_2H  MEAN_FARE_5H  TRIP_COUNT_5H  TRIP_COUNT_1H  \\\n",
      "0     12.713693           1.0           4544            739   \n",
      "1     13.123772           1.0           3463            245   \n",
      "2      9.186856           1.0           4495            878   \n",
      "\n",
      "   OUTPUT_FARE_AMOUNT  \n",
      "0           10.083857  \n",
      "1           10.548088  \n",
      "2            8.751825  \n"
     ]
    }
   ],
   "source": [
    "# model is retrieved from Model Registry in earlier step.\n",
    "restored_prediction = model.run(\n",
    "    enriched_df.to_pandas(), function_name=\"predict\")\n",
    "\n",
    "print(restored_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173da73",
   "metadata": {},
   "source": [
    "<a id='cleanup'></a>\n",
    "## Clean up notebook\n",
    "\n",
    "This cell will drop the schemas have been created at beginning of this notebook, and also drop all objects live in the schemas including source data tables, feature views, datasets, and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea4e1ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='SNOWFLAKE_FEATURE_STORE_NOTEBOOK_DEMO_MODEL successfully dropped.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(f\"DROP SCHEMA IF EXISTS {FS_DEMO_SCHEMA}\").collect()\n",
    "session.sql(f\"DROP SCHEMA IF EXISTS {MODEL_DEMO_SCHEMA}\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
