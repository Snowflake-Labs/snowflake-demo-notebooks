{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "qztxe7bewci5d3lymaxp",
   "authorId": "25646740481",
   "authorName": "CNANTASENAMAT",
   "authorEmail": "chanin.nantasenamat@snowflake.com",
   "sessionId": "aceaea76-f878-49c3-a2f9-d25c87e9668f",
   "lastEditTime": 1762837193609
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3cc4f4-d14b-443d-a1e7-8165049a315c",
   "metadata": {
    "name": "md_title",
    "collapsed": false
   },
   "source": "# Snowflake Semantic View and Agentic Analytics VHOL\n\nIn this hands-on lab, you'll build business-friendly semantic views on top of enterprise data warehouses and enhance them with AI-powered natural language querying through Cortex Analyst. You'll create intelligent agents using Snowflake Intelligence that can answer cross-functional business questions by automatically routing queries across Sales, Marketing, Finance, and HR semantic views.\n\n\n## What You'll Build:\n- **Data Foundation**: 13 dimension tables, 4 fact tables, Salesforce CRM integration\n- **Semantic Views**: Business-friendly layers for Sales, Marketing, Finance & HR\n- **AI Enhancement**: Auto-generate semantic views using Semantic View Autopilot\n- **Natural Language Querying**: Query data with plain English via Cortex Analyst\n- **Interactive Apps**: Streamlit visualizations and chat interfaces\n- **Intelligent Agents**: Cross-functional AI agents for business analytics\n\n## Key Technologies:\n- Semantic Views & Semantic SQL\n- Cortex Analyst (text-to-SQL)\n- Snowflake Intelligence (AI agents)\n- Streamlit"
  },
  {
   "cell_type": "markdown",
   "id": "0a0dc32f-3769-413f-9bd9-690ea79824c5",
   "metadata": {
    "name": "md_load_base_data",
    "collapsed": false
   },
   "source": "### Load the Base Data\n\nThis creates a comprehensive data warehouse supporting cross-functional analytics across Sales, Marketing, Finance, and HR domains.\n\n#### Dimension Tables (13)\n    product_category_dim, product_dim, vendor_dim, customer_dim\n    account_dim, department_dim, region_dim, sales_rep_dim\n    campaign_dim, channel_dim, employee_dim, job_dim, location_dim\n#### Fact Tables (4)\n    sales_fact - Sales transactions with amounts and units (12,000 records)\n    finance_transactions - Financial transactions across departments\n    marketing_campaign_fact - Campaign performance metrics with product targeting\n    hr_employee_fact - Employee data with salary and attrition (5,640 records)\n\n#### Salesforce CRM Tables (3)\n    sf_accounts - Customer accounts linked to customer_dim (1,000 records)\n    sf_opportunities - Sales pipeline and revenue data (25,000 records)\n    sf_contacts - Contact records with campaign attribution (37,563 records)"
  },
  {
   "cell_type": "code",
   "id": "75c22d22-d6fd-41dc-9c8b-a2b98d902dbf",
   "metadata": {
    "language": "sql",
    "name": "sql_load_base_data"
   },
   "outputs": [],
   "source": "--- This script borrows heavily from the Snowflake Intelligence end to end demo here: https://github.com/NickAkincilar/Snowflake_AI_DEMO\n\n--- should take around 2 minutes to run completely\n\n\n -- Switch to accountadmin role to create warehouse\n    USE ROLE accountadmin;\n\n    -- Enable Snowflake Intelligence by creating the Config DB & Schema\n    CREATE DATABASE IF NOT EXISTS agentic_analytics_vhol;\n    CREATE SCHEMA IF NOT EXISTS agentic_analytics_vhol.agents;\n    \n    -- Allow anyone to see the agents in this schema\n    GRANT USAGE ON DATABASE agentic_analytics_vhol TO ROLE PUBLIC;\n    GRANT USAGE ON SCHEMA agentic_analytics_vhol.agents TO ROLE PUBLIC;\n\n\n    create or replace role agentic_analytics_vhol_role;\n\n\n    SET current_user_name = CURRENT_USER();\n    \n    -- Step 2: Use the variable to grant the role\n    GRANT ROLE agentic_analytics_vhol_role TO USER IDENTIFIER($current_user_name);\n    GRANT CREATE DATABASE ON ACCOUNT TO ROLE agentic_analytics_vhol_role;\n    \n    -- Create a dedicated warehouse for the demo with auto-suspend/resume\n    CREATE OR REPLACE WAREHOUSE agentic_analytics_vhol_wh \n        WITH WAREHOUSE_SIZE = 'XSMALL'\n        AUTO_SUSPEND = 300\n        AUTO_RESUME = TRUE;\n\n\n    -- Grant usage on warehouse to admin role\n    GRANT USAGE ON WAREHOUSE agentic_analytics_vhol_wh TO ROLE agentic_analytics_vhol_role;\n\n\n  -- Alter current user's default role and warehouse to the ones used here\n    ALTER USER IDENTIFIER($current_user_name) SET DEFAULT_ROLE = agentic_analytics_vhol_role;\n    ALTER USER IDENTIFIER($current_user_name) SET DEFAULT_WAREHOUSE = agentic_analytics_vhol_wh;\n    \n\n    -- Switch to SF_Intelligence_Demo role to create demo objects\n    use role agentic_analytics_vhol_role;\n  \n    -- Create database and schema\n    CREATE OR REPLACE DATABASE SV_VHOL_DB;\n    USE DATABASE SV_VHOL_DB;\n\n    CREATE SCHEMA IF NOT EXISTS VHOL_SCHEMA;\n    USE SCHEMA VHOL_SCHEMA;\n\n    -- Create file format for CSV files\n    CREATE OR REPLACE FILE FORMAT CSV_FORMAT\n        TYPE = 'CSV'\n        FIELD_DELIMITER = ','\n        RECORD_DELIMITER = '\\n'\n        SKIP_HEADER = 1\n        FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n        TRIM_SPACE = TRUE\n        ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE\n        ESCAPE = 'NONE'\n        ESCAPE_UNENCLOSED_FIELD = '\\134'\n        DATE_FORMAT = 'YYYY-MM-DD'\n        TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS'\n        NULL_IF = ('NULL', 'null', '', 'N/A', 'n/a');\n\n\nuse role accountadmin;\n    -- Create API Integration for GitHub (public repository access)\n    CREATE OR REPLACE API INTEGRATION git_api_integration\n        API_PROVIDER = git_https_api\n        API_ALLOWED_PREFIXES = ('https://github.com/NickAkincilar/')\n        ENABLED = TRUE;\n\n\nGRANT USAGE ON INTEGRATION GIT_API_INTEGRATION TO ROLE agentic_analytics_vhol_role;\n\n\nuse role agentic_analytics_vhol_role;\n    -- Create Git repository integration for the public demo repository\n    CREATE OR REPLACE GIT REPOSITORY AA_VHOL_REPO\n        API_INTEGRATION = git_api_integration\n        ORIGIN = 'https://github.com/NickAkincilar/Snowflake_AI_DEMO.git';\n\n    -- Create internal stage for copied data files\n    CREATE OR REPLACE STAGE INTERNAL_DATA_STAGE\n        FILE_FORMAT = CSV_FORMAT\n        COMMENT = 'Internal stage for copied demo data files'\n        DIRECTORY = ( ENABLE = TRUE)\n        ENCRYPTION = (   TYPE = 'SNOWFLAKE_SSE');\n\n    ALTER GIT REPOSITORY AA_VHOL_REPO FETCH;\n\n    -- ========================================================================\n    -- COPY DATA FROM GIT TO INTERNAL STAGE\n    -- ========================================================================\n\n    -- Copy all CSV files from Git repository demo_data folder to internal stage\n    COPY FILES\n    INTO @INTERNAL_DATA_STAGE/demo_data/\n    FROM @AA_VHOL_REPO/branches/main/demo_data/;\n\n\n    COPY FILES\n    INTO @INTERNAL_DATA_STAGE/unstructured_docs/\n    FROM @AA_VHOL_REPO/branches/main/unstructured_docs/;\n\n    -- Verify files were copied\n    LS @INTERNAL_DATA_STAGE;\n\n    ALTER STAGE INTERNAL_DATA_STAGE refresh;\n\n  \n\n    -- ========================================================================\n    -- DIMENSION TABLES\n    -- ========================================================================\n\n    -- Product Category Dimension\n    CREATE OR REPLACE TABLE product_category_dim (\n        category_key INT PRIMARY KEY,\n        category_name VARCHAR(100) NOT NULL,\n        vertical VARCHAR(50) NOT NULL\n    );\n\n    -- Product Dimension\n    CREATE OR REPLACE TABLE product_dim (\n        product_key INT PRIMARY KEY,\n        product_name VARCHAR(200) NOT NULL,\n        category_key INT NOT NULL,\n        category_name VARCHAR(100),\n        vertical VARCHAR(50)\n    );\n\n    -- Vendor Dimension\n    CREATE OR REPLACE TABLE vendor_dim (\n        vendor_key INT PRIMARY KEY,\n        vendor_name VARCHAR(200) NOT NULL,\n        vertical VARCHAR(50) NOT NULL,\n        address VARCHAR(200),\n        city VARCHAR(100),\n        state VARCHAR(10),\n        zip VARCHAR(20)\n    );\n\n    -- Customer Dimension\n    CREATE OR REPLACE TABLE customer_dim (\n        customer_key INT PRIMARY KEY,\n        customer_name VARCHAR(200) NOT NULL,\n        industry VARCHAR(100),\n        vertical VARCHAR(50),\n        address VARCHAR(200),\n        city VARCHAR(100),\n        state VARCHAR(10),\n        zip VARCHAR(20)\n    );\n\n    -- Account Dimension (Finance)\n    CREATE OR REPLACE TABLE account_dim (\n        account_key INT PRIMARY KEY,\n        account_name VARCHAR(100) NOT NULL,\n        account_type VARCHAR(50)\n    );\n\n    -- Department Dimension\n    CREATE OR REPLACE TABLE department_dim (\n        department_key INT PRIMARY KEY,\n        department_name VARCHAR(100) NOT NULL\n    );\n\n    -- Region Dimension\n    CREATE OR REPLACE TABLE region_dim (\n        region_key INT PRIMARY KEY,\n        region_name VARCHAR(100) NOT NULL\n    );\n\n    -- Sales Rep Dimension\n    CREATE OR REPLACE TABLE sales_rep_dim (\n        sales_rep_key INT PRIMARY KEY,\n        rep_name VARCHAR(200) NOT NULL,\n        hire_date DATE\n    );\n\n    -- Campaign Dimension (Marketing)\n    CREATE OR REPLACE TABLE campaign_dim (\n        campaign_key INT PRIMARY KEY,\n        campaign_name VARCHAR(300) NOT NULL,\n        objective VARCHAR(100)\n    );\n\n    -- Channel Dimension (Marketing)\n    CREATE OR REPLACE TABLE channel_dim (\n        channel_key INT PRIMARY KEY,\n        channel_name VARCHAR(100) NOT NULL\n    );\n\n    -- Employee Dimension (HR)\n    CREATE OR REPLACE TABLE employee_dim (\n        employee_key INT PRIMARY KEY,\n        employee_name VARCHAR(200) NOT NULL,\n        gender VARCHAR(1),\n        hire_date DATE\n    );\n\n    -- Job Dimension (HR)\n    CREATE OR REPLACE TABLE job_dim (\n        job_key INT PRIMARY KEY,\n        job_title VARCHAR(100) NOT NULL,\n        job_level INT\n    );\n\n    -- Location Dimension (HR)\n    CREATE OR REPLACE TABLE location_dim (\n        location_key INT PRIMARY KEY,\n        location_name VARCHAR(200) NOT NULL\n    );\n\n    -- ========================================================================\n    -- FACT TABLES\n    -- ========================================================================\n\n    -- Sales Fact Table\n    CREATE OR REPLACE TABLE sales_fact (\n        sale_id INT PRIMARY KEY,\n        date DATE NOT NULL,\n        customer_key INT NOT NULL,\n        product_key INT NOT NULL,\n        sales_rep_key INT NOT NULL,\n        region_key INT NOT NULL,\n        vendor_key INT NOT NULL,\n        amount DECIMAL(10,2) NOT NULL,\n        units INT NOT NULL\n    );\n\n    -- Finance Transactions Fact Table\n    CREATE OR REPLACE TABLE finance_transactions (\n        transaction_id INT PRIMARY KEY,\n        date DATE NOT NULL,\n        account_key INT NOT NULL,\n        department_key INT NOT NULL,\n        vendor_key INT NOT NULL,\n        product_key INT NOT NULL,\n        customer_key INT NOT NULL,\n        amount DECIMAL(12,2) NOT NULL,\n        approval_status VARCHAR(20) DEFAULT 'Pending',\n        procurement_method VARCHAR(50),\n        approver_id INT,\n        approval_date DATE,\n        purchase_order_number VARCHAR(50),\n        contract_reference VARCHAR(100),\n        CONSTRAINT fk_approver FOREIGN KEY (approver_id) REFERENCES employee_dim(employee_key)\n    ) COMMENT = 'Financial transactions with compliance tracking. approval_status should be Approved/Pending/Rejected. procurement_method should be RFP/Quotes/Emergency/Contract';\n\n    -- Marketing Campaign Fact Table\n    CREATE OR REPLACE TABLE marketing_campaign_fact (\n        campaign_fact_id INT PRIMARY KEY,\n        date DATE NOT NULL,\n        campaign_key INT NOT NULL,\n        product_key INT NOT NULL,\n        channel_key INT NOT NULL,\n        region_key INT NOT NULL,\n        spend DECIMAL(10,2) NOT NULL,\n        leads_generated INT NOT NULL,\n        impressions INT NOT NULL\n    );\n\n    -- HR Employee Fact Table\n    CREATE OR REPLACE TABLE hr_employee_fact (\n        hr_fact_id INT PRIMARY KEY,\n        date DATE NOT NULL,\n        employee_key INT NOT NULL,\n        department_key INT NOT NULL,\n        job_key INT NOT NULL,\n        location_key INT NOT NULL,\n        salary DECIMAL(10,2) NOT NULL,\n        attrition_flag INT NOT NULL\n    );\n\n    -- ========================================================================\n    -- SALESFORCE CRM TABLES\n    -- ========================================================================\n\n    -- Salesforce Accounts Table\n    CREATE OR REPLACE TABLE sf_accounts (\n        account_id VARCHAR(20) PRIMARY KEY,\n        account_name VARCHAR(200) NOT NULL,\n        customer_key INT NOT NULL,\n        industry VARCHAR(100),\n        vertical VARCHAR(50),\n        billing_street VARCHAR(200),\n        billing_city VARCHAR(100),\n        billing_state VARCHAR(10),\n        billing_postal_code VARCHAR(20),\n        account_type VARCHAR(50),\n        annual_revenue DECIMAL(15,2),\n        employees INT,\n        created_date DATE\n    );\n\n    -- Salesforce Opportunities Table\n    CREATE OR REPLACE TABLE sf_opportunities (\n        opportunity_id VARCHAR(20) PRIMARY KEY,\n        sale_id INT,\n        account_id VARCHAR(20) NOT NULL,\n        opportunity_name VARCHAR(200) NOT NULL,\n        stage_name VARCHAR(100) NOT NULL,\n        amount DECIMAL(15,2) NOT NULL,\n        probability DECIMAL(5,2),\n        close_date DATE,\n        created_date DATE,\n        lead_source VARCHAR(100),\n        type VARCHAR(100),\n        campaign_id INT\n    );\n\n    -- Salesforce Contacts Table\n    CREATE OR REPLACE TABLE sf_contacts (\n        contact_id VARCHAR(20) PRIMARY KEY,\n        opportunity_id VARCHAR(20) NOT NULL,\n        account_id VARCHAR(20) NOT NULL,\n        first_name VARCHAR(100),\n        last_name VARCHAR(100),\n        email VARCHAR(200),\n        phone VARCHAR(50),\n        title VARCHAR(100),\n        department VARCHAR(100),\n        lead_source VARCHAR(100),\n        campaign_no INT,\n        created_date DATE\n    );\n\n    -- ========================================================================\n    -- LOAD DIMENSION DATA FROM INTERNAL STAGE\n    -- ========================================================================\n\n    -- Load Product Category Dimension\n    COPY INTO product_category_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/product_category_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Product Dimension\n    COPY INTO product_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/product_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Vendor Dimension\n    COPY INTO vendor_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/vendor_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Customer Dimension\n    COPY INTO customer_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/customer_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Account Dimension\n    COPY INTO account_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/account_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Department Dimension\n    COPY INTO department_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/department_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Region Dimension\n    COPY INTO region_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/region_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Sales Rep Dimension\n    COPY INTO sales_rep_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/sales_rep_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Campaign Dimension\n    COPY INTO campaign_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/campaign_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Channel Dimension\n    COPY INTO channel_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/channel_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Employee Dimension\n    COPY INTO employee_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/employee_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Job Dimension\n    COPY INTO job_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/job_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Location Dimension\n    COPY INTO location_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/location_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- ========================================================================\n    -- LOAD FACT DATA FROM INTERNAL STAGE\n    -- ========================================================================\n\n    -- Load Sales Fact\n    COPY INTO sales_fact\n    FROM @INTERNAL_DATA_STAGE/demo_data/sales_fact.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Finance Transactions\n    COPY INTO finance_transactions\n    FROM @INTERNAL_DATA_STAGE/demo_data/finance_transactions.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Marketing Campaign Fact\n    COPY INTO marketing_campaign_fact\n    FROM @INTERNAL_DATA_STAGE/demo_data/marketing_campaign_fact.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load HR Employee Fact\n    COPY INTO hr_employee_fact\n    FROM @INTERNAL_DATA_STAGE/demo_data/hr_employee_fact.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- ========================================================================\n    -- LOAD SALESFORCE DATA FROM INTERNAL STAGE\n    -- ========================================================================\n\n    -- Load Salesforce Accounts\n    COPY INTO sf_accounts\n    FROM @INTERNAL_DATA_STAGE/demo_data/sf_accounts.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Salesforce Opportunities\n    COPY INTO sf_opportunities\n    FROM @INTERNAL_DATA_STAGE/demo_data/sf_opportunities.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Salesforce Contacts\n    COPY INTO sf_contacts\n    FROM @INTERNAL_DATA_STAGE/demo_data/sf_contacts.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- ========================================================================\n    -- VERIFICATION\n    -- ========================================================================\n\n    -- Verify Git integration and file copy\n    SHOW GIT REPOSITORIES;\n  -- SELECT 'Internal Stage Files' as stage_type, COUNT(*) as file_count FROM (LS @INTERNAL_DATA_STAGE);\n\n    -- Verify data loads\n    SELECT 'DIMENSION TABLES' as category, '' as table_name, NULL as row_count\n    UNION ALL\n    SELECT '', 'product_category_dim', COUNT(*) FROM product_category_dim\n    UNION ALL\n    SELECT '', 'product_dim', COUNT(*) FROM product_dim\n    UNION ALL\n    SELECT '', 'vendor_dim', COUNT(*) FROM vendor_dim\n    UNION ALL\n    SELECT '', 'customer_dim', COUNT(*) FROM customer_dim\n    UNION ALL\n    SELECT '', 'account_dim', COUNT(*) FROM account_dim\n    UNION ALL\n    SELECT '', 'department_dim', COUNT(*) FROM department_dim\n    UNION ALL\n    SELECT '', 'region_dim', COUNT(*) FROM region_dim\n    UNION ALL\n    SELECT '', 'sales_rep_dim', COUNT(*) FROM sales_rep_dim\n    UNION ALL\n    SELECT '', 'campaign_dim', COUNT(*) FROM campaign_dim\n    UNION ALL\n    SELECT '', 'channel_dim', COUNT(*) FROM channel_dim\n    UNION ALL\n    SELECT '', 'employee_dim', COUNT(*) FROM employee_dim\n    UNION ALL\n    SELECT '', 'job_dim', COUNT(*) FROM job_dim\n    UNION ALL\n    SELECT '', 'location_dim', COUNT(*) FROM location_dim\n    UNION ALL\n    SELECT '', '', NULL\n    UNION ALL\n    SELECT 'FACT TABLES', '', NULL\n    UNION ALL\n    SELECT '', 'sales_fact', COUNT(*) FROM sales_fact\n    UNION ALL\n    SELECT '', 'finance_transactions', COUNT(*) FROM finance_transactions\n    UNION ALL\n    SELECT '', 'marketing_campaign_fact', COUNT(*) FROM marketing_campaign_fact\n    UNION ALL\n    SELECT '', 'hr_employee_fact', COUNT(*) FROM hr_employee_fact\n    UNION ALL\n    SELECT '', '', NULL\n    UNION ALL\n    SELECT 'SALESFORCE TABLES', '', NULL\n    UNION ALL\n    SELECT '', 'sf_accounts', COUNT(*) FROM sf_accounts\n    UNION ALL\n    SELECT '', 'sf_opportunities', COUNT(*) FROM sf_opportunities\n    UNION ALL\n    SELECT '', 'sf_contacts', COUNT(*) FROM sf_contacts;\n\n    -- Show all tables\n    SHOW TABLES IN SCHEMA VHOL_SCHEMA; ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37b16f61-2415-41cb-910e-250b9559285f",
   "metadata": {
    "name": "md_create_semantic_views",
    "collapsed": false
   },
   "source": "### Create Semantic Views\n\nWe will create 3 semantic views, one each for:\n- Sales\n- Marketing\n- Finance \n\nWe will use the Semantic View Autopilot feature to create the 4$^{th}$ on an HR semantic view.\n\nMore info here https://docs.snowflake.com/en/user-guide/views-semantic/sql"
  },
  {
   "cell_type": "code",
   "id": "cfcc5338-2bf5-4fe1-a0df-24b6ad23589d",
   "metadata": {
    "language": "sql",
    "name": "sql_create_semantic_views"
   },
   "outputs": [],
   "source": "-- Creates business unit-specific semantic views for natural language queries\n\n-- Set role, database and schema\nUSE ROLE agentic_analytics_vhol_role;\nUSE DATABASE SV_VHOL_DB;\nUSE SCHEMA VHOL_SCHEMA;\n\n-- FINANCE SEMANTIC VIEW\ncreate or replace semantic view FINANCE_SEMANTIC_VIEW\n    tables (\n        TRANSACTIONS as FINANCE_TRANSACTIONS primary key (TRANSACTION_ID) with synonyms=('finance transactions','financial data') comment='All financial transactions across departments',\n        ACCOUNTS as ACCOUNT_DIM primary key (ACCOUNT_KEY) with synonyms=('chart of accounts','account types') comment='Account dimension for financial categorization',\n        DEPARTMENTS as DEPARTMENT_DIM primary key (DEPARTMENT_KEY) with synonyms=('business units','departments') comment='Department dimension for cost center analysis',\n        VENDORS as VENDOR_DIM primary key (VENDOR_KEY) with synonyms=('suppliers','vendors') comment='Vendor information for spend analysis',\n        PRODUCTS as PRODUCT_DIM primary key (PRODUCT_KEY) with synonyms=('products','items') comment='Product dimension for transaction analysis',\n        CUSTOMERS as CUSTOMER_DIM primary key (CUSTOMER_KEY) with synonyms=('clients','customers') comment='Customer dimension for revenue analysis'\n    )\n    relationships (\n        TRANSACTIONS_TO_ACCOUNTS as TRANSACTIONS(ACCOUNT_KEY) references ACCOUNTS(ACCOUNT_KEY),\n        TRANSACTIONS_TO_DEPARTMENTS as TRANSACTIONS(DEPARTMENT_KEY) references DEPARTMENTS(DEPARTMENT_KEY),\n        TRANSACTIONS_TO_VENDORS as TRANSACTIONS(VENDOR_KEY) references VENDORS(VENDOR_KEY),\n        TRANSACTIONS_TO_PRODUCTS as TRANSACTIONS(PRODUCT_KEY) references PRODUCTS(PRODUCT_KEY),\n        TRANSACTIONS_TO_CUSTOMERS as TRANSACTIONS(CUSTOMER_KEY) references CUSTOMERS(CUSTOMER_KEY)\n    )\n    facts (\n        TRANSACTIONS.TRANSACTION_AMOUNT as amount comment='Transaction amount in dollars',\n        TRANSACTIONS.TRANSACTION_RECORD as 1 comment='Count of transactions'\n    )\n    dimensions (\n        TRANSACTIONS.TRANSACTION_DATE as date with synonyms=('date','transaction date') comment='Date of the financial transaction',\n        TRANSACTIONS.TRANSACTION_MONTH as MONTH(date) comment='Month of the transaction',\n        TRANSACTIONS.TRANSACTION_YEAR as YEAR(date) comment='Year of the transaction',\n        ACCOUNTS.ACCOUNT_NAME as account_name with synonyms=('account','account type') comment='Name of the account',\n        ACCOUNTS.ACCOUNT_TYPE as account_type with synonyms=('type','category') comment='Type of account (Income/Expense)',\n        DEPARTMENTS.DEPARTMENT_NAME as department_name with synonyms=('department','business unit') comment='Name of the department',\n        VENDORS.VENDOR_NAME as vendor_name with synonyms=('vendor','supplier') comment='Name of the vendor',\n        PRODUCTS.PRODUCT_NAME as product_name with synonyms=('product','item') comment='Name of the product',\n        CUSTOMERS.CUSTOMER_NAME as customer_name with synonyms=('customer','client') comment='Name of the customer',\n        TRANSACTIONS.APPROVAL_STATUS as approval_status with synonyms=('approval','status','approval state') comment='Transaction approval status (Approved/Pending/Rejected)',\n        TRANSACTIONS.PROCUREMENT_METHOD as procurement_method with synonyms=('procurement','method','purchase method') comment='Method of procurement (RFP/Quotes/Emergency/Contract)',\n        TRANSACTIONS.APPROVER_ID as approver_id with synonyms=('approver','approver employee id') comment='Employee ID of the approver from HR',\n        TRANSACTIONS.APPROVAL_DATE as approval_date with synonyms=('approved date','date approved') comment='Date when transaction was approved',\n        TRANSACTIONS.PURCHASE_ORDER_NUMBER as purchase_order_number with synonyms=('PO number','PO','purchase order') comment='Purchase order number for tracking',\n        TRANSACTIONS.CONTRACT_REFERENCE as contract_reference with synonyms=('contract','contract number','contract ref') comment='Reference to related contract'\n    )\n    metrics (\n        TRANSACTIONS.AVERAGE_AMOUNT as AVG(transactions.amount) comment='Average transaction amount',\n        TRANSACTIONS.TOTAL_AMOUNT as SUM(transactions.amount) comment='Total transaction amount',\n        TRANSACTIONS.TOTAL_TRANSACTIONS as COUNT(transactions.transaction_record) comment='Total number of transactions'\n    )\n    comment='Semantic view for financial analysis and reporting';\n\n\n-- SALES SEMANTIC VIEW\ncreate or replace semantic view SALES_SEMANTIC_VIEW\n  tables (\n    CUSTOMERS as CUSTOMER_DIM primary key (CUSTOMER_KEY) with synonyms=('clients','customers','accounts') comment='Customer information for sales analysis',\n    PRODUCTS as PRODUCT_DIM primary key (PRODUCT_KEY) with synonyms=('products','items','SKUs') comment='Product catalog for sales analysis',\n    PRODUCT_CATEGORY_DIM primary key (CATEGORY_KEY),\n    REGIONS as REGION_DIM primary key (REGION_KEY) with synonyms=('territories','regions','areas') comment='Regional information for territory analysis',\n    SALES as SALES_FACT primary key (SALE_ID) with synonyms=('sales transactions','sales data') comment='All sales transactions and deals',\n    SALES_REPS as SALES_REP_DIM primary key (SALES_REP_KEY) with synonyms=('sales representatives','reps','salespeople') comment='Sales representative information',\n    VENDORS as VENDOR_DIM primary key (VENDOR_KEY) with synonyms=('suppliers','vendors') comment='Vendor information for supply chain analysis'\n  )\n  relationships (\n    PRODUCT_TO_CATEGORY as PRODUCTS(CATEGORY_KEY) references PRODUCT_CATEGORY_DIM(CATEGORY_KEY),\n    SALES_TO_CUSTOMERS as SALES(CUSTOMER_KEY) references CUSTOMERS(CUSTOMER_KEY),\n    SALES_TO_PRODUCTS as SALES(PRODUCT_KEY) references PRODUCTS(PRODUCT_KEY),\n    SALES_TO_REGIONS as SALES(REGION_KEY) references REGIONS(REGION_KEY),\n    SALES_TO_REPS as SALES(SALES_REP_KEY) references SALES_REPS(SALES_REP_KEY),\n    SALES_TO_VENDORS as SALES(VENDOR_KEY) references VENDORS(VENDOR_KEY)\n  )\n  facts (\n    SALES.SALE_AMOUNT as amount comment='Sale amount in dollars',\n    SALES.SALE_RECORD as 1 comment='Count of sales transactions',\n    SALES.UNITS_SOLD as units comment='Number of units sold'\n  )\n  dimensions (\n    CUSTOMERS.CUSTOMER_INDUSTRY as INDUSTRY with synonyms=('industry','customer type') comment='Customer industry',\n    CUSTOMERS.CUSTOMER_KEY as CUSTOMER_KEY,\n    CUSTOMERS.CUSTOMER_NAME as customer_name with synonyms=('customer','client','account') comment='Name of the customer',\n    PRODUCTS.CATEGORY_KEY as CATEGORY_KEY with synonyms=('category_id','product_category','category_code','classification_key','group_key','product_group_id') comment='Unique identifier for the product category.',\n    PRODUCTS.PRODUCT_KEY as PRODUCT_KEY,\n    PRODUCTS.PRODUCT_NAME as product_name with synonyms=('product','item') comment='Name of the product',\n    PRODUCT_CATEGORY_DIM.CATEGORY_KEY as CATEGORY_KEY with synonyms=('category_id','category_code','product_category_number','category_identifier','classification_key') comment='Unique identifier for a product category.',\n    PRODUCT_CATEGORY_DIM.CATEGORY_NAME as CATEGORY_NAME with synonyms=('category_title','product_group','classification_name','category_label','product_category_description') comment='The category to which a product belongs, such as electronics, clothing, or software as a service.',\n    PRODUCT_CATEGORY_DIM.VERTICAL as VERTICAL with synonyms=('industry','sector','market','category_group','business_area','domain') comment='The industry or sector in which a product is categorized, such as retail, technology, or manufacturing.',\n    REGIONS.REGION_KEY as REGION_KEY,\n    REGIONS.REGION_NAME as region_name with synonyms=('region','territory','area') comment='Name of the region',\n    SALES.CUSTOMER_KEY as CUSTOMER_KEY,\n    SALES.PRODUCT_KEY as PRODUCT_KEY,\n    SALES.REGION_KEY as REGION_KEY,\n    SALES.SALES_REP_KEY as SALES_REP_KEY,\n    SALES.SALE_DATE as date with synonyms=('date','sale date','transaction date') comment='Date of the sale',\n    SALES.SALE_ID as SALE_ID,\n    SALES.SALE_MONTH as MONTH(date) comment='Month of the sale',\n    SALES.SALE_YEAR as YEAR(date) comment='Year of the sale',\n    SALES.VENDOR_KEY as VENDOR_KEY,\n    SALES_REPS.SALES_REP_KEY as SALES_REP_KEY,\n    SALES_REPS.SALES_REP_NAME as REP_NAME with synonyms=('sales rep','representative','salesperson') comment='Name of the sales representative',\n    VENDORS.VENDOR_KEY as VENDOR_KEY,\n    VENDORS.VENDOR_NAME as vendor_name with synonyms=('vendor','supplier','provider') comment='Name of the vendor'\n  )\n  metrics (\n    SALES.AVERAGE_DEAL_SIZE as AVG(sales.amount) comment='Average deal size',\n    SALES.AVERAGE_UNITS_PER_SALE as AVG(sales.units) comment='Average units per sale',\n    SALES.TOTAL_DEALS as COUNT(sales.sale_record) comment='Total number of deals',\n    SALES.TOTAL_REVENUE as SUM(sales.amount) comment='Total sales revenue',\n    SALES.TOTAL_UNITS as SUM(sales.units) comment='Total units sold'\n  )\n  comment='Semantic view for sales analysis and performance tracking'\n;\n\n\n-- MARKETING SEMANTIC VIEW\ncreate or replace semantic view MARKETING_SEMANTIC_VIEW\n  tables (\n    ACCOUNTS as SF_ACCOUNTS primary key (ACCOUNT_ID) with synonyms=('customers','accounts','clients') comment='Customer account information for revenue analysis',\n    CAMPAIGNS as MARKETING_CAMPAIGN_FACT primary key (CAMPAIGN_FACT_ID) with synonyms=('marketing campaigns','campaign data') comment='Marketing campaign performance data',\n    CAMPAIGN_DETAILS as CAMPAIGN_DIM primary key (CAMPAIGN_KEY) with synonyms=('campaign info','campaign details') comment='Campaign dimension with objectives and names',\n    CHANNELS as CHANNEL_DIM primary key (CHANNEL_KEY) with synonyms=('marketing channels','channels') comment='Marketing channel information',\n    CONTACTS as SF_CONTACTS primary key (CONTACT_ID) with synonyms=('leads','contacts','prospects') comment='Contact records generated from marketing campaigns',\n    CONTACTS_FOR_OPPORTUNITIES as SF_CONTACTS primary key (CONTACT_ID) with synonyms=('opportunity contacts') comment='Contact records generated from marketing campaigns, specifically for opportunities, not leads',\n    OPPORTUNITIES as SF_OPPORTUNITIES primary key (OPPORTUNITY_ID) with synonyms=('deals','opportunities','sales pipeline') comment='Sales opportunities and revenue data',\n    PRODUCTS as PRODUCT_DIM primary key (PRODUCT_KEY) with synonyms=('products','items') comment='Product dimension for campaign-specific analysis',\n    REGIONS as REGION_DIM primary key (REGION_KEY) with synonyms=('territories','regions','markets') comment='Regional information for campaign analysis'\n  )\n  relationships (\n    CAMPAIGNS_TO_CHANNELS as CAMPAIGNS(CHANNEL_KEY) references CHANNELS(CHANNEL_KEY),\n    CAMPAIGNS_TO_DETAILS as CAMPAIGNS(CAMPAIGN_KEY) references CAMPAIGN_DETAILS(CAMPAIGN_KEY),\n    CAMPAIGNS_TO_PRODUCTS as CAMPAIGNS(PRODUCT_KEY) references PRODUCTS(PRODUCT_KEY),\n    CAMPAIGNS_TO_REGIONS as CAMPAIGNS(REGION_KEY) references REGIONS(REGION_KEY),\n    CONTACTS_TO_ACCOUNTS as CONTACTS(ACCOUNT_ID) references ACCOUNTS(ACCOUNT_ID),\n    CONTACTS_TO_CAMPAIGNS as CONTACTS(CAMPAIGN_NO) references CAMPAIGNS(CAMPAIGN_FACT_ID),\n    CONTACTS_TO_OPPORTUNITIES as CONTACTS_FOR_OPPORTUNITIES(OPPORTUNITY_ID) references OPPORTUNITIES(OPPORTUNITY_ID),\n    OPPORTUNITIES_TO_ACCOUNTS as OPPORTUNITIES(ACCOUNT_ID) references ACCOUNTS(ACCOUNT_ID),\n    OPPORTUNITIES_TO_CAMPAIGNS as OPPORTUNITIES(CAMPAIGN_ID) references CAMPAIGNS(CAMPAIGN_FACT_ID)\n  )\n  facts (\n    PUBLIC CAMPAIGNS.CAMPAIGN_RECORD as 1 comment='Count of campaign activities',\n    PUBLIC CAMPAIGNS.CAMPAIGN_SPEND as spend comment='Marketing spend in dollars',\n    PUBLIC CAMPAIGNS.IMPRESSIONS as IMPRESSIONS comment='Number of impressions',\n    PUBLIC CAMPAIGNS.LEADS_GENERATED as LEADS_GENERATED comment='Number of leads generated',\n    PUBLIC CONTACTS.CONTACT_RECORD as 1 comment='Count of contacts generated',\n    PUBLIC OPPORTUNITIES.OPPORTUNITY_RECORD as 1 comment='Count of opportunities created',\n    PUBLIC OPPORTUNITIES.REVENUE as AMOUNT comment='Opportunity revenue in dollars'\n  )\n  dimensions (\n    PUBLIC ACCOUNTS.ACCOUNT_ID as ACCOUNT_ID,\n    PUBLIC ACCOUNTS.ACCOUNT_NAME as ACCOUNT_NAME with synonyms=('customer name','client name','company') comment='Name of the customer account',\n    PUBLIC ACCOUNTS.ACCOUNT_TYPE as ACCOUNT_TYPE with synonyms=('customer type','account category') comment='Type of customer account',\n    PUBLIC ACCOUNTS.ANNUAL_REVENUE as ANNUAL_REVENUE with synonyms=('customer revenue','company revenue') comment='Customer annual revenue',\n    PUBLIC ACCOUNTS.EMPLOYEES as EMPLOYEES with synonyms=('company size','employee count') comment='Number of employees at customer',\n    PUBLIC ACCOUNTS.INDUSTRY as INDUSTRY with synonyms=('industry','sector') comment='Customer industry',\n    PUBLIC ACCOUNTS.SALES_CUSTOMER_KEY as CUSTOMER_KEY with synonyms=('Customer No','Customer ID') comment='This is the customer key thank links the Salesforce account to customers table.',\n    PUBLIC CAMPAIGNS.CAMPAIGN_DATE as date with synonyms=('date','campaign date') comment='Date of the campaign activity',\n    PUBLIC CAMPAIGNS.CAMPAIGN_FACT_ID as CAMPAIGN_FACT_ID,\n    PUBLIC CAMPAIGNS.CAMPAIGN_KEY as CAMPAIGN_KEY,\n    PUBLIC CAMPAIGNS.CAMPAIGN_MONTH as MONTH(date) comment='Month of the campaign',\n    PUBLIC CAMPAIGNS.CAMPAIGN_YEAR as YEAR(date) comment='Year of the campaign',\n    PUBLIC CAMPAIGNS.CHANNEL_KEY as CHANNEL_KEY,\n    PUBLIC CAMPAIGNS.PRODUCT_KEY as PRODUCT_KEY with synonyms=('product_id','product identifier') comment='Product identifier for campaign targeting',\n    PUBLIC CAMPAIGNS.REGION_KEY as REGION_KEY,\n    PUBLIC CAMPAIGN_DETAILS.CAMPAIGN_KEY as CAMPAIGN_KEY,\n    PUBLIC CAMPAIGN_DETAILS.CAMPAIGN_NAME as CAMPAIGN_NAME with synonyms=('campaign','campaign title') comment='Name of the marketing campaign',\n    PUBLIC CAMPAIGN_DETAILS.CAMPAIGN_OBJECTIVE as OBJECTIVE with synonyms=('objective','goal','purpose') comment='Campaign objective',\n    PUBLIC CHANNELS.CHANNEL_KEY as CHANNEL_KEY,\n    PUBLIC CHANNELS.CHANNEL_NAME as CHANNEL_NAME with synonyms=('channel','marketing channel') comment='Name of the marketing channel',\n    PUBLIC CONTACTS.ACCOUNT_ID as ACCOUNT_ID,\n    PUBLIC CONTACTS.CAMPAIGN_NO as CAMPAIGN_NO,\n    PUBLIC CONTACTS.CONTACT_ID as CONTACT_ID,\n    PUBLIC CONTACTS.DEPARTMENT as DEPARTMENT with synonyms=('department','business unit') comment='Contact department',\n    PUBLIC CONTACTS.EMAIL as EMAIL with synonyms=('email','email address') comment='Contact email address',\n    PUBLIC CONTACTS.FIRST_NAME as FIRST_NAME with synonyms=('first name','contact name') comment='Contact first name',\n    PUBLIC CONTACTS.LAST_NAME as LAST_NAME with synonyms=('last name','surname') comment='Contact last name',\n    PUBLIC CONTACTS.LEAD_SOURCE as LEAD_SOURCE with synonyms=('lead source','source') comment='How the contact was generated',\n    PUBLIC CONTACTS.OPPORTUNITY_ID as OPPORTUNITY_ID,\n    PUBLIC CONTACTS.TITLE as TITLE with synonyms=('job title','position') comment='Contact job title',\n    PUBLIC OPPORTUNITIES.ACCOUNT_ID as ACCOUNT_ID,\n    PUBLIC OPPORTUNITIES.CAMPAIGN_ID as CAMPAIGN_ID with synonyms=('campaign fact id','marketing campaign id') comment='Campaign fact ID that links opportunity to marketing campaign',\n    PUBLIC OPPORTUNITIES.CLOSE_DATE as CLOSE_DATE with synonyms=('close date','expected close') comment='Expected or actual close date',\n    PUBLIC OPPORTUNITIES.OPPORTUNITY_ID as OPPORTUNITY_ID,\n    PUBLIC OPPORTUNITIES.OPPORTUNITY_LEAD_SOURCE as lead_source with synonyms=('opportunity source','deal source') comment='Source of the opportunity',\n    PUBLIC OPPORTUNITIES.OPPORTUNITY_NAME as OPPORTUNITY_NAME with synonyms=('deal name','opportunity title') comment='Name of the sales opportunity',\n    PUBLIC OPPORTUNITIES.OPPORTUNITY_STAGE as STAGE_NAME comment='Stage name of the opportinity. Closed Won indicates an actual sale with revenue',\n    PUBLIC OPPORTUNITIES.OPPORTUNITY_TYPE as TYPE with synonyms=('deal type','opportunity type') comment='Type of opportunity',\n    PUBLIC OPPORTUNITIES.SALES_SALE_ID as SALE_ID with synonyms=('sales id','invoice no') comment='Sales_ID for sales_fact table that links this opp to a sales record.',\n    PUBLIC PRODUCTS.PRODUCT_CATEGORY as CATEGORY_NAME with synonyms=('category','product category') comment='Category of the product',\n    PUBLIC PRODUCTS.PRODUCT_KEY as PRODUCT_KEY,\n    PUBLIC PRODUCTS.PRODUCT_NAME as PRODUCT_NAME with synonyms=('product','item','product title') comment='Name of the product being promoted',\n    PUBLIC PRODUCTS.PRODUCT_VERTICAL as VERTICAL with synonyms=('vertical','industry') comment='Business vertical of the product',\n    PUBLIC REGIONS.REGION_KEY as REGION_KEY,\n    PUBLIC REGIONS.REGION_NAME as REGION_NAME with synonyms=('region','market','territory') comment='Name of the region'\n  )\n  metrics (\n    PUBLIC CAMPAIGNS.AVERAGE_SPEND as AVG(CAMPAIGNS.spend) comment='Average campaign spend',\n    PUBLIC CAMPAIGNS.TOTAL_CAMPAIGNS as COUNT(CAMPAIGNS.campaign_record) comment='Total number of campaign activities',\n    PUBLIC CAMPAIGNS.TOTAL_IMPRESSIONS as SUM(CAMPAIGNS.impressions) comment='Total impressions across campaigns',\n    PUBLIC CAMPAIGNS.TOTAL_LEADS as SUM(CAMPAIGNS.leads_generated) comment='Total leads generated from campaigns',\n    PUBLIC CAMPAIGNS.TOTAL_SPEND as SUM(CAMPAIGNS.spend) comment='Total marketing spend',\n    PUBLIC CONTACTS.TOTAL_CONTACTS as COUNT(CONTACTS.contact_record) comment='Total contacts generated from campaigns',\n    PUBLIC OPPORTUNITIES.AVERAGE_DEAL_SIZE as AVG(OPPORTUNITIES.revenue) comment='Average opportunity size from marketing',\n    PUBLIC OPPORTUNITIES.CLOSED_WON_REVENUE as SUM(CASE WHEN OPPORTUNITIES.opportunity_stage = 'Closed Won' THEN OPPORTUNITIES.revenue ELSE 0 END) comment='Revenue from closed won opportunities',\n    PUBLIC OPPORTUNITIES.TOTAL_OPPORTUNITIES as COUNT(OPPORTUNITIES.opportunity_record) comment='Total opportunities from marketing',\n    PUBLIC OPPORTUNITIES.TOTAL_REVENUE as SUM(OPPORTUNITIES.revenue) comment='Total revenue from marketing-driven opportunities'\n  )\n  comment='Enhanced semantic view for marketing campaign analysis with complete revenue attribution and ROI tracking'\n;\n\n-- Display the semantic views\nSHOW SEMANTIC VIEWS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3433c602-5fdb-4b8c-ab9c-b77d67f1fce6",
   "metadata": {
    "name": "md_query_semantic_views",
    "collapsed": false
   },
   "source": "### Query Semantic Views\n\nLet's try a natural language query that uses our semantic view first.\n\nLet's open the [Marketing semantic view in Cortex Analyst](https://app.snowflake.com/_deeplink/#/cortex/analyst/databases/SV_VHOL_DB/schemas/VHOL_SCHEMA/semanticView/MARKETING_SEMANTIC_VIEW/edit) and ask a question like:\n\n> *\"Which marketing campaign names generated the most revenue in 2025? Show me marketing ROI and cost per lead by channel.\"*\n\n\nAnd *now* let's try running that same query, but use Snowflake's declarative \"Semantic SQL\" interface to the semantic view."
  },
  {
   "cell_type": "code",
   "id": "7ef199a9-b877-4e24-960e-b3d00336f8df",
   "metadata": {
    "language": "sql",
    "name": "sql_query_semantic_views"
   },
   "outputs": [],
   "source": "USE ROLE agentic_analytics_vhol_role;\nUSE DATABASE SV_VHOL_DB;\nUSE SCHEMA VHOL_SCHEMA;\n\n\nSELECT * FROM SEMANTIC_VIEW(\n  SV_VHOL_DB.VHOL_SCHEMA.MARKETING_SEMANTIC_VIEW \n  DIMENSIONS \n    campaign_details.campaign_name,\n    channels.channel_name \n  METRICS \n    opportunities.total_revenue, \n    campaigns.total_spend,\n    campaigns.total_leads\n  WHERE\n    campaigns.campaign_year = 2025\n                        )\nWHERE total_revenue > 0\nORDER BY total_revenue DESC\n;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9b5a53fe-7827-4ff8-91db-f80bc4524300",
   "metadata": {
    "name": "md_sva",
    "collapsed": false
   },
   "source": "### AI-Assisted Semantic View Creation with Semantic View Autopilot (SVA)\n\nFor the previous semantic views, you were provided a pre-created script. In this step, we will use the semantic view wizard and new auto pilot feature. To get started, you will want to [go to the Cortex Analyst home page](https://app.snowflake.com/_deeplink/#/cortex/analyst) and select \"Create New Semantic View\".\n\n- Select `SV_VHOL_DB.VHOL_SCHEMA` for the \"Location to store\" field\n- Name your semantic view `HR_SEMANTIC_VIEW`\n- Select Employee, Department, Job and Location dimensions\n- In the \"Semantic View\" tab, scroll down and for \"Verified Queries\" click on the \"+\" button then pass the questions and SQL from the 5 examples below into the wizard\n\n(Note - run the optional cell below to delete the `HR_SEMANTIC_VIEW` if this is not your first time running through the VHOL)\n"
  },
  {
   "cell_type": "code",
   "id": "ac5361e4-3963-417b-9f07-199fef64a1d1",
   "metadata": {
    "language": "sql",
    "name": "optional_to_delete_hr_semantic_view",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--- optional - if this is not your first time running through this lab, you may want to run this command before creating your HR_SEMANTIC_VIEW\n\nUSE ROLE agentic_analytics_vhol_role;\nUSE DATABASE SV_VHOL_DB;\nUSE SCHEMA VHOL_SCHEMA;\n\nDROP SEMANTIC VIEW HR_SEMANTIC_VIEW;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ee315b4-1aca-46e1-815a-e9c215974cdf",
   "metadata": {
    "name": "md_seed_sva_queries",
    "collapsed": false
   },
   "source": "### Query 1\n```\nShow a complete workforce breakdown report across employee, department, and location\n```\n```\nSELECT \n    -- Employee dimensions\n    e.EMPLOYEE_KEY,\n    e.EMPLOYEE_NAME,\n    e.GENDER,\n    e.HIRE_DATE,\n    -- Department dimensions  \n    d.DEPARTMENT_KEY,\n    d.DEPARTMENT_NAME,\n    -- Job dimensions\n    j.JOB_KEY,\n    j.JOB_TITLE,\n    j.JOB_LEVEL,\n    -- Location dimensions\n    l.LOCATION_KEY,\n    l.LOCATION_NAME,\n    -- Fact metrics\n    f.HR_FACT_ID,\n    f.DATE as RECORD_DATE,\n    EXTRACT(YEAR FROM f.DATE) as RECORD_YEAR,\n    EXTRACT(MONTH FROM f.DATE) as RECORD_MONTH,\n    f.SALARY as EMPLOYEE_SALARY,\n    f.ATTRITION_FLAG,\n    -- Aggregated metrics\n    COUNT(*) as EMPLOYEE_RECORD,\n    SUM(f.SALARY) as TOTAL_SALARY_COST,\n    AVG(f.SALARY) as AVG_SALARY,\n    SUM(f.ATTRITION_FLAG) as ATTRITION_COUNT,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as TOTAL_EMPLOYEES\nFROM SV_VHOL_DB.VHOL_SCHEMA.HR_EMPLOYEE_FACT f\nJOIN SV_VHOL_DB.VHOL_SCHEMA.EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nJOIN SV_VHOL_DB.VHOL_SCHEMA.DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nJOIN SV_VHOL_DB.VHOL_SCHEMA.JOB_DIM j \n    ON f.JOB_KEY = j.JOB_KEY\nJOIN SV_VHOL_DB.VHOL_SCHEMA.LOCATION_DIM l \n    ON f.LOCATION_KEY = l.LOCATION_KEY\nGROUP BY \n    e.EMPLOYEE_KEY, e.EMPLOYEE_NAME, e.GENDER, e.HIRE_DATE,\n    d.DEPARTMENT_KEY, d.DEPARTMENT_NAME,\n    j.JOB_KEY, j.JOB_TITLE, j.JOB_LEVEL,\n    l.LOCATION_KEY, l.LOCATION_NAME,\n    f.HR_FACT_ID, f.DATE, f.SALARY, f.ATTRITION_FLAG\nORDER BY f.DATE DESC, f.SALARY DESC;\n```\n\n### Query 2\n```\nProvide department-Level Analytics over time with  salary metrics and attrition metrics\n```\n```\nSELECT \n    d.DEPARTMENT_KEY,\n    d.DEPARTMENT_NAME,\n    EXTRACT(YEAR FROM f.DATE) as RECORD_YEAR,\n    EXTRACT(MONTH FROM f.DATE) as RECORD_MONTH,\n    -- Employee metrics\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as TOTAL_EMPLOYEES,\n    COUNT(DISTINCT CASE WHEN e.GENDER = 'F' THEN f.EMPLOYEE_KEY END) as FEMALE_EMPLOYEES,\n    COUNT(DISTINCT CASE WHEN e.GENDER = 'M' THEN f.EMPLOYEE_KEY END) as MALE_EMPLOYEES,\n    -- Salary metrics\n    SUM(f.SALARY) as TOTAL_SALARY_COST,\n    AVG(f.SALARY) as AVG_SALARY,\n    MIN(f.SALARY) as MIN_SALARY,\n    MAX(f.SALARY) as MAX_SALARY,\n    -- Attrition metrics\n    SUM(f.ATTRITION_FLAG) as ATTRITION_COUNT,\n    ROUND(SUM(f.ATTRITION_FLAG) * 100.0 / COUNT(*), 2) as ATTRITION_RATE_PCT,\n    -- Tenure metrics\n    AVG(DATEDIFF('month', e.HIRE_DATE, f.DATE)) as AVG_TENURE_MONTHS,\n    AVG(DATEDIFF('day', e.HIRE_DATE, f.DATE)) as AVG_TENURE_DAYS\nFROM SV_VHOL_DB.VHOL_SCHEMA.HR_EMPLOYEE_FACT f\nJOIN SV_VHOL_DB.VHOL_SCHEMA.DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nJOIN SV_VHOL_DB.VHOL_SCHEMA.EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nGROUP BY d.DEPARTMENT_KEY, d.DEPARTMENT_NAME, EXTRACT(YEAR FROM f.DATE), EXTRACT(MONTH FROM f.DATE)\nORDER BY d.DEPARTMENT_NAME, RECORD_YEAR, RECORD_MONTH;\n```\n\n### Query 3\n```\nProvide Job and Location Analytics over time, with salary metrics\n```\n```\nSELECT \n    j.JOB_KEY,\n    j.JOB_TITLE,\n    j.JOB_LEVEL,\n    l.LOCATION_KEY,\n    l.LOCATION_NAME,\n    EXTRACT(YEAR FROM f.DATE) as RECORD_YEAR,\n    -- Employee counts by job and location\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as TOTAL_EMPLOYEES,\n    COUNT(DISTINCT CASE WHEN e.GENDER = 'F' THEN f.EMPLOYEE_KEY END) as FEMALE_EMPLOYEES,\n    COUNT(DISTINCT CASE WHEN e.GENDER = 'M' THEN f.EMPLOYEE_KEY END) as MALE_EMPLOYEES,\n    -- Salary analysis\n    SUM(f.SALARY) as TOTAL_SALARY_COST,\n    AVG(f.SALARY) as AVG_SALARY,\n    MIN(f.SALARY) as MIN_SALARY,\n    MAX(f.SALARY) as MAX_SALARY,\n    STDDEV(f.SALARY) as SALARY_STDDEV,\n    -- Attrition analysis\n    SUM(f.ATTRITION_FLAG) as ATTRITION_COUNT,\n    ROUND(SUM(f.ATTRITION_FLAG) * 100.0 / COUNT(*), 2) as ATTRITION_RATE_PCT,\n    -- Tenure analysis\n    AVG(DATEDIFF('month', e.HIRE_DATE, f.DATE)) as AVG_TENURE_MONTHS\nFROM SV_VHOL_DB.VHOL_SCHEMA.HR_EMPLOYEE_FACT f\nJOIN SV_VHOL_DB.VHOL_SCHEMA.JOB_DIM j \n    ON f.JOB_KEY = j.JOB_KEY\nJOIN SV_VHOL_DB.VHOL_SCHEMA.LOCATION_DIM l \n    ON f.LOCATION_KEY = l.LOCATION_KEY\nJOIN SV_VHOL_DB.VHOL_SCHEMA.EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nGROUP BY j.JOB_KEY, j.JOB_TITLE, j.JOB_LEVEL, l.LOCATION_KEY, l.LOCATION_NAME, EXTRACT(YEAR FROM f.DATE)\nORDER BY j.JOB_TITLE, l.LOCATION_NAME, RECORD_YEAR;\n```\n\n### Query 4\n```\nShow a trend of all key HR metrics over time\n```\n```\nSELECT \n    EXTRACT(YEAR FROM f.DATE) as RECORD_YEAR,\n    EXTRACT(MONTH FROM f.DATE) as RECORD_MONTH,\n    f.DATE as RECORD_DATE,\n    -- Employee metrics over time\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as TOTAL_EMPLOYEES,\n    COUNT(DISTINCT f.DEPARTMENT_KEY) as TOTAL_DEPARTMENTS,\n    COUNT(DISTINCT f.JOB_KEY) as TOTAL_JOBS,\n    COUNT(DISTINCT f.LOCATION_KEY) as TOTAL_LOCATIONS,\n    -- Salary trends\n    SUM(f.SALARY) as TOTAL_SALARY_COST,\n    AVG(f.SALARY) as AVG_SALARY,\n    MIN(f.SALARY) as MIN_SALARY,\n    MAX(f.SALARY) as MAX_SALARY,\n    -- Attrition trends\n    SUM(f.ATTRITION_FLAG) as ATTRITION_COUNT,\n    ROUND(SUM(f.ATTRITION_FLAG) * 100.0 / COUNT(*), 2) as ATTRITION_RATE_PCT,\n    -- Gender distribution over time\n    COUNT(DISTINCT CASE WHEN e.GENDER = 'F' THEN f.EMPLOYEE_KEY END) as FEMALE_EMPLOYEES,\n    COUNT(DISTINCT CASE WHEN e.GENDER = 'M' THEN f.EMPLOYEE_KEY END) as MALE_EMPLOYEES,\n    -- Tenure analysis over time\n    AVG(DATEDIFF('month', e.HIRE_DATE, f.DATE)) as AVG_TENURE_MONTHS\nFROM SV_VHOL_DB.VHOL_SCHEMA.HR_EMPLOYEE_FACT f\nJOIN SV_VHOL_DB.VHOL_SCHEMA.EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nGROUP BY EXTRACT(YEAR FROM f.DATE), EXTRACT(MONTH FROM f.DATE), f.DATE\nORDER BY RECORD_YEAR, RECORD_MONTH, RECORD_DATE;\n```\n\n### Query 5\n```\nProvide an Executive Summary with All Key Metrics\n```\n```\nSELECT \n    'HR_ANALYTICS_SUMMARY' as REPORT_TYPE,\n    -- Employee metrics\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as TOTAL_EMPLOYEES,\n    COUNT(DISTINCT f.DEPARTMENT_KEY) as TOTAL_DEPARTMENTS,\n    COUNT(DISTINCT f.JOB_KEY) as TOTAL_JOBS,\n    COUNT(DISTINCT f.LOCATION_KEY) as TOTAL_LOCATIONS,\n    -- Salary metrics\n    SUM(f.SALARY) as TOTAL_SALARY_COST,\n    AVG(f.SALARY) as AVG_SALARY,\n    MIN(f.SALARY) as MIN_SALARY,\n    MAX(f.SALARY) as MAX_SALARY,\n    -- Attrition metrics\n    SUM(f.ATTRITION_FLAG) as TOTAL_ATTRITION_COUNT,\n    ROUND(SUM(f.ATTRITION_FLAG) * 100.0 / COUNT(*), 2) as OVERALL_ATTRITION_RATE,\n    -- Gender metrics\n    COUNT(DISTINCT CASE WHEN e.GENDER = 'F' THEN f.EMPLOYEE_KEY END) as FEMALE_EMPLOYEES,\n    COUNT(DISTINCT CASE WHEN e.GENDER = 'M' THEN f.EMPLOYEE_KEY END) as MALE_EMPLOYEES,\n    ROUND(COUNT(DISTINCT CASE WHEN e.GENDER = 'F' THEN f.EMPLOYEE_KEY END) * 100.0 / \n          COUNT(DISTINCT f.EMPLOYEE_KEY), 2) as FEMALE_PERCENTAGE,\n    -- Tenure metrics\n    AVG(DATEDIFF('month', e.HIRE_DATE, f.DATE)) as AVG_TENURE_MONTHS,\n    AVG(DATEDIFF('day', e.HIRE_DATE, f.DATE)) as AVG_TENURE_DAYS,\n    -- Time range\n    MIN(f.DATE) as EARLIEST_RECORD_DATE,\n    MAX(f.DATE) as LATEST_RECORD_DATE\nFROM SV_VHOL_DB.VHOL_SCHEMA.HR_EMPLOYEE_FACT f\nJOIN SV_VHOL_DB.VHOL_SCHEMA.EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY;\n```"
  },
  {
   "cell_type": "markdown",
   "id": "dfa9c504-c314-4caf-b3c2-d8f2fbe4f42f",
   "metadata": {
    "name": "md_ai_sv_enrich",
    "collapsed": false
   },
   "source": "## AI-Powered Semantic View Enrichment\n\nIn this section, we will run some SQL queries to generate a synthetic set of query history entries in Snowflake.  We will then use AI (leveraging Snowlake AISQL) to mine query history and suggest enhancements to the HR semantic view.\n\nLet's first generate those query histroy entries by running the SQL in the cell below.\n\nCheck out the [Query History](https://app.snowflake.com/_deeplink/#/compute/history/) to see if they are running."
  },
  {
   "cell_type": "markdown",
   "id": "fdacbb87-ff67-42f0-a745-a377d0785878",
   "metadata": {
    "name": "md_generate_query_history",
    "collapsed": false
   },
   "source": "### Generate Synthetic Query History for AI Enhancement\n\nThis cell executes 25+ HR analytics queries to create realistic query history entries. These queries cover employee demographics, salary analysis, attrition rates, and performance metrics - simulating typical business questions analysts ask about HR data.\n\nThe generated query history (marked with `-- VHOL Seed Query` comments) will be analyzed by AI to automatically suggest enhancements to the HR semantic view, including new metrics, dimensions, and business logic based on actual usage patterns.\n\nSubsequent Python cells will mine this query history and use Cortex AI to generate an improved semantic view DDL."
  },
  {
   "cell_type": "code",
   "id": "731a0c6f-6d34-4816-9fa7-761199de5ccf",
   "metadata": {
    "language": "sql",
    "name": "sql_generate_query_history",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Set role, database and schema\nUSE ROLE agentic_analytics_vhol_role;\nUSE DATABASE SV_VHOL_DB;\nUSE SCHEMA VHOL_SCHEMA;\n\n\n-- VHOL Seed Query\nSELECT \n    d.DEPARTMENT_NAME,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count\nFROM HR_EMPLOYEE_FACT f\nJOIN DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nGROUP BY d.DEPARTMENT_NAME\nORDER BY employee_count DESC;\n\n-- 2. Average Salary by Department and Gender\n-- Business Question: \"What is the average salary by department and gender?\"\nSELECT \n    d.DEPARTMENT_NAME,\n    e.GENDER,\n    AVG(f.SALARY) as avg_salary,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count\nFROM HR_EMPLOYEE_FACT f\nJOIN DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nJOIN EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nGROUP BY d.DEPARTMENT_NAME, e.GENDER\nORDER BY d.DEPARTMENT_NAME, e.GENDER;\n\n-- VHOL Seed Query\nSELECT \n    d.DEPARTMENT_NAME,\n    COUNT(*) as total_records,\n    SUM(f.ATTRITION_FLAG) as attrition_count,\n    ROUND(SUM(f.ATTRITION_FLAG) * 100.0 / COUNT(*), 2) as attrition_rate_pct\nFROM HR_EMPLOYEE_FACT f\nJOIN DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nGROUP BY d.DEPARTMENT_NAME\nORDER BY attrition_rate_pct DESC;\n\n-- VHOL Seed Query\nSELECT \n    EXTRACT(YEAR FROM f.DATE) as year,\n    EXTRACT(MONTH FROM f.DATE) as month,\n    AVG(f.SALARY) as avg_salary,\n    MIN(f.SALARY) as min_salary,\n    MAX(f.SALARY) as max_salary,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count\nFROM HR_EMPLOYEE_FACT f\nGROUP BY EXTRACT(YEAR FROM f.DATE), EXTRACT(MONTH FROM f.DATE)\nORDER BY year, month;\n\n-- VHOL Seed Query\nSELECT \n    d.DEPARTMENT_NAME,\n    AVG(DATEDIFF('day', e.HIRE_DATE, f.DATE)) as avg_tenure_days,\n    AVG(DATEDIFF('month', e.HIRE_DATE, f.DATE)) as avg_tenure_months,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count\nFROM HR_EMPLOYEE_FACT f\nJOIN DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nJOIN EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nGROUP BY d.DEPARTMENT_NAME\nORDER BY avg_tenure_days DESC;\n\n-- VHOL Seed Query\nSELECT \n    e.EMPLOYEE_NAME,\n    d.DEPARTMENT_NAME,\n    j.JOB_TITLE,\n    f.SALARY,\n    f.DATE as salary_date\nFROM HR_EMPLOYEE_FACT f\nJOIN EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nJOIN DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nJOIN JOB_DIM j \n    ON f.JOB_KEY = j.JOB_KEY\nORDER BY f.SALARY DESC\nLIMIT 10;\n\n-- VHOL Seed Query\nSELECT \n    j.JOB_TITLE,\n    j.JOB_LEVEL,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count,\n    AVG(f.SALARY) as avg_salary\nFROM HR_EMPLOYEE_FACT f\nJOIN JOB_DIM j \n    ON f.JOB_KEY = j.JOB_KEY\nGROUP BY j.JOB_TITLE, j.JOB_LEVEL\nORDER BY j.JOB_LEVEL, employee_count DESC;\n\n-- VHOL Seed Query\nSELECT \n    l.LOCATION_NAME,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count,\n    AVG(f.SALARY) as avg_salary,\n    SUM(f.ATTRITION_FLAG) as attrition_count\nFROM HR_EMPLOYEE_FACT f\nJOIN LOCATION_DIM l \n    ON f.LOCATION_KEY = l.LOCATION_KEY\nGROUP BY l.LOCATION_NAME\nORDER BY employee_count DESC;\n\n-- VHOL Seed Query\nSELECT \n    d.DEPARTMENT_NAME,\n    e.GENDER,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count,\n    ROUND(COUNT(DISTINCT f.EMPLOYEE_KEY) * 100.0 / \n          SUM(COUNT(DISTINCT f.EMPLOYEE_KEY)) OVER (PARTITION BY d.DEPARTMENT_NAME), 2) as gender_pct\nFROM HR_EMPLOYEE_FACT f\nJOIN DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nJOIN EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nGROUP BY d.DEPARTMENT_NAME, e.GENDER\nORDER BY d.DEPARTMENT_NAME, e.GENDER;\n\n-- VHOL Seed Query\nSELECT \n    d.DEPARTMENT_NAME,\n    EXTRACT(YEAR FROM f.DATE) as year,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count,\n    AVG(f.SALARY) as avg_salary\nFROM HR_EMPLOYEE_FACT f\nJOIN DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nGROUP BY d.DEPARTMENT_NAME, EXTRACT(YEAR FROM f.DATE)\nORDER BY d.DEPARTMENT_NAME, year;\n\n-- VHOL Seed Query\nSELECT \n    j.JOB_TITLE,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count,\n    MIN(f.SALARY) as min_salary,\n    MAX(f.SALARY) as max_salary,\n    AVG(f.SALARY) as avg_salary,\n    STDDEV(f.SALARY) as salary_stddev\nFROM HR_EMPLOYEE_FACT f\nJOIN JOB_DIM j \n    ON f.JOB_KEY = j.JOB_KEY\nGROUP BY j.JOB_TITLE\nORDER BY avg_salary DESC;\n\n-- VHOL Seed Query\nSELECT \n    d.DEPARTMENT_NAME,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count,\n    SUM(f.SALARY) as total_salary_cost,\n    AVG(f.SALARY) as avg_salary,\n    MAX(f.SALARY) as max_salary,\n    MIN(f.SALARY) as min_salary\nFROM HR_EMPLOYEE_FACT f\nJOIN DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nGROUP BY d.DEPARTMENT_NAME\nORDER BY total_salary_cost DESC;\n\n-- VHOL Seed Query\nSELECT \n    d.DEPARTMENT_NAME,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count,\n    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY f.SALARY) as p25_salary,\n    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY f.SALARY) as p50_salary,\n    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY f.SALARY) as p75_salary,\n    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY f.SALARY) as p90_salary\nFROM HR_EMPLOYEE_FACT f\nJOIN DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nGROUP BY d.DEPARTMENT_NAME\nORDER BY p50_salary DESC;\n\n-- VHOL Seed Query\nSELECT \n    j.JOB_LEVEL,\n    j.JOB_TITLE,\n    COUNT(*) as total_records,\n    SUM(f.ATTRITION_FLAG) as attrition_count,\n    ROUND(SUM(f.ATTRITION_FLAG) * 100.0 / COUNT(*), 2) as attrition_rate_pct,\n    AVG(f.SALARY) as avg_salary\nFROM HR_EMPLOYEE_FACT f\nJOIN JOB_DIM j \n    ON f.JOB_KEY = j.JOB_KEY\nWHERE j.JOB_LEVEL IS NOT NULL\nGROUP BY j.JOB_LEVEL, j.JOB_TITLE\nORDER BY attrition_rate_pct DESC;\n\n-- VHOL Seed Query\nSELECT \n    CASE \n        WHEN DATEDIFF('month', e.HIRE_DATE, f.DATE) <= 12 THEN 'Recent Hire (12 months)'\n        WHEN DATEDIFF('month', e.HIRE_DATE, f.DATE) <= 24 THEN 'Mid-tenure (13-24 months)'\n        ELSE 'Long-tenure (>24 months)'\n    END as tenure_category,\n    COUNT(*) as total_records,\n    SUM(f.ATTRITION_FLAG) as attrition_count,\n    ROUND(SUM(f.ATTRITION_FLAG) * 100.0 / COUNT(*), 2) as attrition_rate_pct\nFROM HR_EMPLOYEE_FACT f\nJOIN EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nGROUP BY tenure_category\nORDER BY attrition_rate_pct DESC;\n\n-- VHOL Seed Query\nSELECT \n    CASE \n        WHEN f.SALARY < 40000 THEN 'Low Salary (<40k)'\n        WHEN f.SALARY < 60000 THEN 'Mid Salary (40k-60k)'\n        ELSE 'High Salary (>60k)'\n    END as salary_bracket,\n    COUNT(*) as total_records,\n    SUM(f.ATTRITION_FLAG) as attrition_count,\n    ROUND(SUM(f.ATTRITION_FLAG) * 100.0 / COUNT(*), 2) as attrition_rate_pct,\n    AVG(f.SALARY) as avg_salary\nFROM HR_EMPLOYEE_FACT f\nGROUP BY salary_bracket\nORDER BY avg_salary;\n\n-- VHOL Seed Query\nSELECT \n    EXTRACT(YEAR FROM f.DATE) as year,\n    EXTRACT(MONTH FROM f.DATE) as month,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as active_employees,\n    SUM(f.ATTRITION_FLAG) as attrition_count,\n    AVG(f.SALARY) as avg_salary\nFROM HR_EMPLOYEE_FACT f\nGROUP BY EXTRACT(YEAR FROM f.DATE), EXTRACT(MONTH FROM f.DATE)\nORDER BY year, month;\n\n-- VHOL Seed Query\nSELECT \n    EXTRACT(YEAR FROM f.DATE) as year,\n    CASE \n        WHEN EXTRACT(MONTH FROM f.DATE) IN (1,2,3) THEN 'Q1'\n        WHEN EXTRACT(MONTH FROM f.DATE) IN (4,5,6) THEN 'Q2'\n        WHEN EXTRACT(MONTH FROM f.DATE) IN (7,8,9) THEN 'Q3'\n        ELSE 'Q4'\n    END as quarter,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count,\n    AVG(f.SALARY) as avg_salary,\n    SUM(f.ATTRITION_FLAG) as attrition_count\nFROM HR_EMPLOYEE_FACT f\nGROUP BY EXTRACT(YEAR FROM f.DATE), quarter\nORDER BY year, quarter;\n\n-- VHOL Seed Query\nSELECT \n    f.EMPLOYEE_KEY,\n    e.EMPLOYEE_NAME,\n    COUNT(DISTINCT f.DEPARTMENT_KEY) as departments_worked,\n    MIN(f.DATE) as first_date,\n    MAX(f.DATE) as last_date,\n    AVG(f.SALARY) as avg_salary\nFROM HR_EMPLOYEE_FACT f\nJOIN EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nGROUP BY f.EMPLOYEE_KEY, e.EMPLOYEE_NAME\nHAVING COUNT(DISTINCT f.DEPARTMENT_KEY) > 1\nORDER BY departments_worked DESC, avg_salary DESC;\n\n-- VHOL Seed Query\nSELECT \n    e.EMPLOYEE_NAME,\n    d.DEPARTMENT_NAME,\n    MIN(f.SALARY) as starting_salary,\n    MAX(f.SALARY) as current_salary,\n    MAX(f.SALARY) - MIN(f.SALARY) as salary_growth,\n    ROUND((MAX(f.SALARY) - MIN(f.SALARY)) / MIN(f.SALARY) * 100, 2) as growth_pct\nFROM HR_EMPLOYEE_FACT f\nJOIN EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nJOIN DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nGROUP BY e.EMPLOYEE_NAME, d.DEPARTMENT_NAME\nHAVING COUNT(*) > 1\nORDER BY growth_pct DESC;\n\n-- VHOL Seed Query\nSELECT \n    d.DEPARTMENT_NAME,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as total_employees,\n    COUNT(DISTINCT CASE WHEN e.GENDER = 'F' THEN f.EMPLOYEE_KEY END) as female_employees,\n    COUNT(DISTINCT CASE WHEN e.GENDER = 'M' THEN f.EMPLOYEE_KEY END) as male_employees,\n    ROUND(COUNT(DISTINCT CASE WHEN e.GENDER = 'F' THEN f.EMPLOYEE_KEY END) * 100.0 / \n          COUNT(DISTINCT f.EMPLOYEE_KEY), 2) as female_pct,\n    AVG(f.SALARY) as avg_salary\nFROM HR_EMPLOYEE_FACT f\nJOIN DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nJOIN EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nGROUP BY d.DEPARTMENT_NAME\nORDER BY total_employees DESC;\n\n-- VHOL Seed Query\nSELECT \n    'Total Employees' as metric,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as value\nFROM HR_EMPLOYEE_FACT f\nWHERE f.DATE = (SELECT MAX(DATE) FROM HR_EMPLOYEE_FACT)\n\nUNION ALL\n\nSELECT \n    'Total Departments' as metric,\n    COUNT(DISTINCT f.DEPARTMENT_KEY) as value\nFROM HR_EMPLOYEE_FACT f\nWHERE f.DATE = (SELECT MAX(DATE) FROM HR_EMPLOYEE_FACT)\n\nUNION ALL\n\nSELECT \n    'Average Salary' as metric,\n    ROUND(AVG(f.SALARY), 2) as value\nFROM HR_EMPLOYEE_FACT f\nWHERE f.DATE = (SELECT MAX(DATE) FROM HR_EMPLOYEE_FACT)\n\nUNION ALL\n\nSELECT \n    'Total Attrition Count' as metric,\n    SUM(f.ATTRITION_FLAG) as value\nFROM HR_EMPLOYEE_FACT f\nWHERE f.DATE = (SELECT MAX(DATE) FROM HR_EMPLOYEE_FACT);\n\n-- VHOL Seed Query\nSELECT \n    d.DEPARTMENT_NAME,\n    COUNT(DISTINCT f.EMPLOYEE_KEY) as employee_count,\n    AVG(f.SALARY) as avg_salary,\n    ROUND(SUM(f.ATTRITION_FLAG) * 100.0 / COUNT(*), 2) as attrition_rate_pct,\n    AVG(DATEDIFF('month', e.HIRE_DATE, f.DATE)) as avg_tenure_months,\n    -- Health score: lower attrition + higher tenure + reasonable salary = healthier\n    ROUND(\n        (100 - ROUND(SUM(f.ATTRITION_FLAG) * 100.0 / COUNT(*), 2)) * 0.4 +\n        LEAST(AVG(DATEDIFF('month', e.HIRE_DATE, f.DATE)) / 12, 10) * 0.3 +\n        LEAST(AVG(f.SALARY) / 1000, 10) * 0.3, 2\n    ) as health_score\nFROM HR_EMPLOYEE_FACT f\nJOIN DEPARTMENT_DIM d \n    ON f.DEPARTMENT_KEY = d.DEPARTMENT_KEY\nJOIN EMPLOYEE_DIM e \n    ON f.EMPLOYEE_KEY = e.EMPLOYEE_KEY\nGROUP BY d.DEPARTMENT_NAME\nORDER BY health_score DESC;\n\nSELECT 1;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a0bc82d-7eff-4ce4-b9b7-c5a49367d97e",
   "metadata": {
    "name": "md_run_before_agentic_optimize",
    "collapsed": false
   },
   "source": "L## Setup Semantic View Enhancement with AI Workflow\n\nInitialize libraries, session, and configuration for AI-powered semantic view enhancement workflow."
  },
  {
   "cell_type": "code",
   "id": "fb560af8-a454-4bda-b269-1097932ea3f2",
   "metadata": {
    "language": "python",
    "name": "py_run_before_agentic_optimize",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# setup for AI-powered enrichment\n# Import required libraries (available in Snowflake notebooks)\nimport json\nimport re\nimport pandas as pd\nfrom typing import List, Dict, Any\nfrom snowflake.snowpark import Session\n\n# Get the built-in Snowpark session\nsession = get_active_session()\n\n# Configuration\nHOURS_BACK = 12  # How many hours back to look in query history\nSEMANTIC_VIEW_NAME = 'HR_SEMANTIC_VIEW'\nCORTEX_MODEL = 'claude-3-5-sonnet'  # Claude model with high token limit\n\n# Set context for the analysis\nsession.sql(\"USE ROLE agentic_analytics_vhol_role\").collect()\nsession.sql(\"USE DATABASE SV_VHOL_DB\").collect()\nsession.sql(\"USE SCHEMA VHOL_SCHEMA\").collect()\n\n# Verify connection\ncurrent_context = session.sql(\"\"\"\n    SELECT \n        CURRENT_DATABASE() as database,\n        CURRENT_SCHEMA() as schema,\n        CURRENT_WAREHOUSE() as warehouse,\n        CURRENT_ROLE() as role,\n        CURRENT_USER() as user\n\"\"\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aab14a09-b5e3-44b6-8b73-5ccb04c6f69b",
   "metadata": {
    "name": "md_query_history_mining",
    "collapsed": false
   },
   "source": "### Mine Query History\n\nRetrieve and analyze recent VHOL Seed Queries from Snowflake query history for enhancement patterns."
  },
  {
   "cell_type": "code",
   "id": "25787390-19c5-4556-9239-2a746283cc43",
   "metadata": {
    "language": "python",
    "name": "py_query_history_mining"
   },
   "outputs": [],
   "source": "# Query to retrieve VHOL Seed Queries from history\nquery_history_sql = f\"\"\"\nSELECT \n    QUERY_TEXT,\n    START_TIME,\n    EXECUTION_STATUS,\n    USER_NAME\nFROM \n    SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\nWHERE \n    START_TIME >= DATEADD('hour', -{HOURS_BACK}, CURRENT_TIMESTAMP())\n    AND QUERY_TEXT ILIKE '%VHOL Seed Query%'\n    AND QUERY_TEXT NOT ILIKE '%QUERY_TEXT%'\n    AND EXECUTION_STATUS = 'SUCCESS'\nORDER BY \n    START_TIME DESC\nLIMIT 50\n\"\"\"\n\nprint(f\" Retrieving VHOL Seed Queries from last {HOURS_BACK} hours...\")\n\n# Execute query and convert to pandas DataFrame\nquery_history_result = session.sql(query_history_sql).collect()\nquery_history_df = pd.DataFrame([dict(row.asDict()) for row in query_history_result])\n\nprint(f\" Found {len(query_history_df)} VHOL Seed Queries in the last {HOURS_BACK} hours\")\n\nif len(query_history_df) > 0:\n    print(\"\\nSample queries found:\")\n    for i, row in query_history_df.head(3).iterrows():\n        print(f\"\\n{i+1}. Query at {row['START_TIME']}:\")\n        # Show first 1000 characters of query\n        query_preview = row['QUERY_TEXT'][:1000] + \"...\" if len(row['QUERY_TEXT']) > 1000 else row['QUERY_TEXT']\n        print(f\"   {query_preview}\")\nelse:\n    print(\"  No VHOL Seed Queries found. You may need to:\")\n    print(\"   1. Run some queries with 'VHOL Seed Query' comments\")\n    print(\"   2. Increase the HOURS_BACK parameter\")\n    print(\"   3. Check that the queries executed successfully\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b5b99f33-2b81-48cf-8842-1590cb673f46",
   "metadata": {
    "name": "md_extract_metrics_dimensions",
    "collapsed": false
   },
   "source": "### Extract Metrics & Dimensions  \nParse SQL queries to identify aggregation functions (metrics) and column references (dimensions) with alias resolution."
  },
  {
   "cell_type": "code",
   "id": "a38272a9-fc14-4c05-89e6-0a74202949df",
   "metadata": {
    "language": "python",
    "name": "py_extract_metrics_dimensions"
   },
   "outputs": [],
   "source": "def extract_metrics_and_dimensions(query_text: str) -> Dict[str, List[str]]:\n    \"\"\"\n    Extract metrics (aggregation functions) and dimensions from SQL query\n    \"\"\"\n    metrics = []\n    dimensions = []\n    \n    # Clean query text\n    query_clean = re.sub(r'--.*?\\n', '\\n', query_text)  # Remove line comments\n    query_clean = re.sub(r'/\\*.*?\\*/', '', query_clean, flags=re.DOTALL)  # Remove block comments\n    query_upper = query_clean.upper()\n    \n    # Extract aggregation functions (metrics)\n    metric_patterns = [\n        r'COUNT\\s*\\([^)]+\\)',\n        r'SUM\\s*\\([^)]+\\)',\n        r'AVG\\s*\\([^)]+\\)',\n        r'MIN\\s*\\([^)]+\\)',\n        r'MAX\\s*\\([^)]+\\)',\n        r'STDDEV\\s*\\([^)]+\\)',\n        r'PERCENTILE_CONT\\s*\\([^)]+\\)',\n        r'ROUND\\s*\\([^)]+\\)',\n    ]\n    \n    for pattern in metric_patterns:\n        matches = re.findall(pattern, query_upper)\n        metrics.extend(matches)\n    \n    # Extract column references from SELECT, WHERE, GROUP BY\n    column_patterns = [\n        r'SELECT\\s+.*?([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)',  # table.column in SELECT\n        r'WHERE\\s+.*?([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)',   # table.column in WHERE\n        r'GROUP BY\\s+.*?([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)', # table.column in GROUP BY\n        r'EXTRACT\\s*\\(\\s*[A-Z]+\\s+FROM\\s+([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)\\)',  # EXTRACT functions\n        r'DATEDIFF\\s*\\([^,]+,\\s*([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)',  # DATEDIFF functions\n    ]\n    \n    for pattern in column_patterns:\n        matches = re.findall(pattern, query_upper)\n        for match in matches:\n            # Skip if it's part of an aggregation function\n            if not any(agg in match for agg in ['COUNT', 'SUM', 'AVG', 'MIN', 'MAX']):\n                dimensions.append(match)\n    \n    # Clean and deduplicate\n    metrics = list(set([m.strip() for m in metrics if m.strip()]))\n    dimensions = list(set([d.strip() for d in dimensions if d.strip()]))\n    \n    return {\n        'metrics': metrics,\n        'dimensions': dimensions\n    }\n\n# Analyze all queries\nall_metrics = []\nall_dimensions = []\n\nprint(\" Analyzing queries for metrics and dimensions...\")\n\nfor i, row in query_history_df.iterrows():\n    analysis = extract_metrics_and_dimensions(row['QUERY_TEXT'])\n    all_metrics.extend(analysis['metrics'])\n    all_dimensions.extend(analysis['dimensions'])\n\n# Deduplicate and summarize\nunique_metrics = list(set(all_metrics))\nunique_dimensions = list(set(all_dimensions))\n\nprint(f\"\\n Analysis Results (with aliases):\")\nprint(f\"   Total unique metrics found: {len(unique_metrics)}\")\nprint(f\"   Total unique dimensions found: {len(unique_dimensions)}\")\n\nif unique_metrics:\n    print(f\"\\n Sample Metrics (with aliases):\")\n    for metric in unique_metrics[:5]:  # Show first 5\n        print(f\"   - {metric}\")\n\nif unique_dimensions:\n    print(f\"\\n Sample Dimensions (with aliases):\")\n    for dim in unique_dimensions[:5]:  # Show first 5\n        print(f\"   - {dim}\")\n\n# VHOL table alias mappings\nalias_to_table = {\n    'F': 'HR_EMPLOYEE_FACT',\n    'E': 'EMPLOYEE_DIM', \n    'D': 'DEPARTMENT_DIM',\n    'J': 'JOB_DIM',\n    'L': 'LOCATION_DIM'\n}\n\nprint(f\"\\n Resolving VHOL table aliases to actual table names...\")\nprint(f\" Alias mappings: {alias_to_table}\")\n\n# Resolve aliases in metrics\nresolved_metrics = []\nfor metric in unique_metrics:\n    resolved_metric = metric\n    for alias, table in alias_to_table.items():\n        resolved_metric = resolved_metric.replace(f'{alias}.', f'{table}.')\n    resolved_metrics.append(resolved_metric)\n\n# Resolve aliases in dimensions\nresolved_dimensions = []\nfor dim in unique_dimensions:\n    if '.' in dim:\n        table_alias = dim.split('.')[0]\n        column_name = dim.split('.')[1]\n        \n        if table_alias in alias_to_table:\n            resolved_dim = f\"{alias_to_table[table_alias]}.{column_name}\"\n            resolved_dimensions.append(resolved_dim)\n        else:\n            resolved_dimensions.append(dim)\n    else:\n        resolved_dimensions.append(dim)\n\n# Update with resolved names\nunique_metrics = list(set(resolved_metrics))\nunique_dimensions = list(set(resolved_dimensions))\n\nprint(f\"\\n Final Analysis Results (aliases resolved):\")\nprint(f\"    Resolved metrics: {len(unique_metrics)}\")\nprint(f\"    Resolved dimensions: {len(unique_dimensions)}\")\n\nif unique_metrics:\n    print(f\"\\n Final Resolved Metrics:\")\n    for metric in unique_metrics[:5]:\n        print(f\"   - {metric}\")\n\nif unique_dimensions:\n    print(f\"\\n Final Resolved Dimensions:\")\n    for dim in unique_dimensions[:5]:\n        print(f\"   - {dim}\")\n\nprint(f\"\\n Ready for semantic view enhancement!\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae589069-e886-44a1-aa77-b516b9e5efd1",
   "metadata": {
    "name": "md_pre_ai_sv_ddl",
    "collapsed": false
   },
   "source": "### Retrieve Current Semantic View DDL\nFetch existing `HR_SEMANTIC_VIEW` definition for AI enhancement analysis."
  },
  {
   "cell_type": "code",
   "id": "5f5a4507-057b-4b80-8689-a92cb1ea65c0",
   "metadata": {
    "language": "python",
    "name": "py_pre_ai_sv_ddl"
   },
   "outputs": [],
   "source": "# Retrieve current semantic view DDL\nprint(f\" Retrieving DDL for {SEMANTIC_VIEW_NAME}...\")\n\ntry:\n    ddl_result = session.sql(f\"SELECT GET_DDL('semantic_view','{SEMANTIC_VIEW_NAME}') as DDL\").collect()\n    \n    if ddl_result and len(ddl_result) > 0:\n        current_ddl = ddl_result[0]['DDL']\n        print(f\" Retrieved DDL for {SEMANTIC_VIEW_NAME}\")\n        print(f\" DDL Length: {len(current_ddl)} characters\")\n        \n        # Show first few lines\n        ddl_lines = current_ddl.split('\\n')\n        print(f\"\\n Preview (first 20 lines):\")\n        for i, line in enumerate(ddl_lines[:20]):\n            print(f\"   {i+1:2d}: {line}\")\n        \n        if len(ddl_lines) > 20:\n            print(f\"   ... ({len(ddl_lines)-20} more lines)\")\n    else:\n        print(f\" No DDL found for {SEMANTIC_VIEW_NAME}\")\n        current_ddl = \"\"\n        \nexcept Exception as e:\n    print(f\" Error retrieving DDL: {e}\")\n    current_ddl = \"\"\n\nif current_ddl:\n    print(f\"\\n DDL retrieval successful! Ready for AI enhancement.\")\nelse:\n    print(f\"\\n  No DDL available - you may need to create the semantic view first.\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6634fed0-a716-4e08-b5c2-355601514f54",
   "metadata": {
    "name": "md_ai_enriched_sv_ddl",
    "collapsed": false
   },
   "source": "### AI-Enhanced DDL Generation\nUse Cortex AI to enhance semantic view with discovered metrics and dimensions from query patterns."
  },
  {
   "cell_type": "code",
   "id": "ad65117c-d2a3-4005-9e66-0f43f7f39118",
   "metadata": {
    "language": "python",
    "name": "py_ai_enriched_sv_ddl"
   },
   "outputs": [],
   "source": "if current_ddl and (unique_metrics or unique_dimensions):\n    # Create AI prompt for enhancement (optimized for token efficiency)\n    top_metrics = unique_metrics[:10]  # Top 10 most important\n    top_dimensions = unique_dimensions[:10]  # Top 10 most important\n    \n    prompt = f\"\"\"\nEnhance this CREATE SEMANTIC VIEW DDL by adding new METRICS/DIMENSION clauses for discovered query patterns.\n\nCURRENT DDL:\n{current_ddl}\n\nADD THESE NEW METRICS: {', '.join(top_metrics)}\nADD THESE NEW DIMENSIONS: {', '.join(top_dimensions)}\n\nRULES:\n- Keep all existing content unchanged\n- IMPORTANT: Maintain correct DDL section order: FACTS(), DIMENSIONS(), METRICS()\n- Add ALL aggregate expressions (SUM, COUNT, AVG, etc.) to the METRICS() section ONLY\n- METRICS() format: table_name.metric_name AS AGG(expression) --- added with AI enhancement\n- FACTS() section is for table references, NOT aggregate expressions\n- Non-aggregate column references go in DIMENSION sections\n- DO NOT include any \"WITH EXTENSION\" section in the output\n- Mark new additions with comment: --- added with AI enhancement\n- Return only the complete enhanced DDL, no explanation\n\nCORRECT DDL STRUCTURE:\nFACTS (table_references)\nDIMENSIONS (column_references)  \nMETRICS (\n  HR_EMPLOYEE_FACT.total_salary AS SUM(salary) --- added with AI enhancement\n)\n\nOUTPUT:\n\"\"\"\n    \n    # Escape single quotes for SQL\n    prompt_escaped = prompt.replace(\"'\", \"''\")\n    \n    # Use CORTEX_COMPLETE to generate enhanced DDL\n    cortex_sql = f\"\"\"\n    SELECT SNOWFLAKE.CORTEX.COMPLETE(\n        '{CORTEX_MODEL}',\n        '{prompt_escaped}'\n    ) as enhanced_ddl\n    \"\"\"\n    \n    print(f\" Using CORTEX_COMPLETE with {CORTEX_MODEL} to enhance semantic view...\")\n    print(\"   This may take 30-60 seconds...\")\n    \n    try:\n        # Execute CORTEX_COMPLETE\n        cortex_result = session.sql(cortex_sql).collect()\n        \n        if cortex_result and len(cortex_result) > 0:\n            enhanced_ddl = cortex_result[0]['ENHANCED_DDL']\n            print(\"\\n Successfully generated enhanced semantic view DDL!\")\n            \n            # Show statistics\n            original_lines = len(current_ddl.split('\\n'))\n            enhanced_lines = len(enhanced_ddl.split('\\n'))\n            \n            print(f\" Enhancement Statistics:\")\n            print(f\"   Original DDL: {original_lines} lines, {len(current_ddl)} characters\")\n            print(f\"   Enhanced DDL: {enhanced_lines} lines, {len(enhanced_ddl)} characters\")\n            print(f\"   Lines added: {enhanced_lines - original_lines}\")\n            \n            # Count new metrics and dimensions by looking for AI enhancement comments\n            ai_additions_count = enhanced_ddl.count('--- added with AI enhancement')\n            \n            print(f\"   New metrics/dimensions added: {ai_additions_count}\")\n            \n        else:\n            print(\" CORTEX_COMPLETE returned no result\")\n            enhanced_ddl = current_ddl\n            \n    except Exception as e:\n        print(f\" Error with CORTEX_COMPLETE: {e}\")\n        enhanced_ddl = current_ddl\n        \nelse:\n    print(\"  Skipping enhancement - no DDL or no new metrics/dimensions found\")\n    enhanced_ddl = current_ddl if 'current_ddl' in locals() else \"\"\n\n# Display enhanced DDL results\nif 'enhanced_ddl' in locals() and enhanced_ddl:\n    print(\"\\n\" + \"=\"*80)\n    print(\"COMPLETE ENHANCED SEMANTIC VIEW DDL\")\n    print(\"=\"*80)\n    print(\" COMPLETE DDL OUTPUT (no truncation):\")\n    print()\n    print(enhanced_ddl)\n    print()\n    print(\"=\"*80)\n    \n    # Highlight the new AI-enhanced additions\n    enhanced_lines = enhanced_ddl.split('\\n')\n    new_additions = [line for line in enhanced_lines if '--- added with AI enhancement' in line]\n    \n    if new_additions:\n        print(\"\\n AI-ENHANCED ADDITIONS:\")\n        print(\"-\" * 50)\n        for addition in new_additions:\n            print(addition.strip())\n    else:\n        print(\"\\n  No new additions detected in the enhanced DDL\")\n    \n    print(f\"\\n Next Steps:\")\n    print(f\"   1. Review the enhanced DDL above\")\n    print(f\"   2. Test the DDL in a development environment\")\n    print(f\"   3. Deploy to production when ready\")\n    print(f\"   4. Update documentation with new metrics/dimensions\")\n    \nelse:\n    print(\" No enhanced DDL available\")\n\nprint(\"\\n Analysis complete!\")\nif 'query_history_df' in locals():\n    print(f\"    Analyzed {len(query_history_df)} queries from the last {HOURS_BACK} hours\")\nif 'unique_metrics' in locals():\n    print(f\"    Found {len(unique_metrics)} unique metrics\")\nif 'unique_dimensions' in locals():\n    print(f\"    Found {len(unique_dimensions)} unique dimensions\")\nprint(f\"    Enhanced {SEMANTIC_VIEW_NAME} using {CORTEX_MODEL}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0488fbc3-593f-41e7-a3fb-5aa01e358bba",
   "metadata": {
    "name": "md_deploy_new_sv",
    "collapsed": false
   },
   "source": "### Deploy Enhanced Semantic View\nDrop existing view and create enhanced version with AI-generated improvements."
  },
  {
   "cell_type": "code",
   "id": "fcc8884b-5f23-4fcc-b25a-a85dc98cedb7",
   "metadata": {
    "language": "python",
    "name": "py_deploy_new_sv"
   },
   "outputs": [],
   "source": "# Deploy the enhanced semantic view DDL\nif 'enhanced_ddl' in locals() and enhanced_ddl and enhanced_ddl.strip():\n    print(\" Deploying Enhanced Semantic View...\")\n    print(\"=\"*60)\n    \n    try:\n        # First, drop the existing semantic view if it exists\n        drop_sql = f\"DROP SEMANTIC VIEW IF EXISTS {SEMANTIC_VIEW_NAME}\"\n        print(f\" Dropping existing semantic view: {SEMANTIC_VIEW_NAME}\")\n        session.sql(drop_sql).collect()\n        print(\"    Existing semantic view dropped successfully\")\n        \n        # Execute the enhanced DDL\n        print(f\" Creating enhanced semantic view...\")\n        session.sql(enhanced_ddl).collect()\n        print(\"    Enhanced semantic view created successfully!\")\n        \n        # Verify the deployment\n        verification_sql = f\"SHOW SEMANTIC VIEWS LIKE '{SEMANTIC_VIEW_NAME}'\"\n        result = session.sql(verification_sql).collect()\n        \n        if result:\n            print(f\"\\n SUCCESS! Enhanced {SEMANTIC_VIEW_NAME} deployed successfully!\")\n            print(f\" Semantic view details:\")\n            for row in result:\n                print(f\"   Name: {row['name']}\")\n                print(f\"   Database: {row['database_name']}\")\n                print(f\"   Schema: {row['schema_name']}\")\n                print(f\"   Created: {row['created_on']}\")\n        else:\n            print(f\"  Deployment completed but verification failed - please check manually\")\n            \n        # Show what was added\n        if '--- added with AI enhancement' in enhanced_ddl:\n            additions_count = enhanced_ddl.count('--- added with AI enhancement')\n            print(f\"\\n AI Enhancement Summary:\")\n            print(f\"    {additions_count} new metrics/dimensions added\")\n            print(f\"    All additions marked with '--- added with AI enhancement'\")\n            print(f\"    Ready for immediate use in analytics!\")\n        \n    except Exception as e:\n        print(f\" Error deploying semantic view: {e}\")\n        print(f\"\\n Troubleshooting:\")\n        print(f\"   1. Check if you have CREATE SEMANTIC VIEW privileges\")\n        print(f\"   2. Verify the DDL syntax above is correct\")\n        print(f\"   3. Ensure all referenced tables exist\")\n        print(f\"   4. Try running the DDL manually if needed\")\n        \nelse:\n    print(\"  No enhanced DDL available for deployment\")\n    print(\"   Please run Step 5 first to generate the enhanced DDL\")\n\nprint(f\"\\n\" + \"=\"*60)\nprint(\" SEMANTIC VIEW ENHANCEMENT WORKFLOW COMPLETE!\")\nprint(\"=\"*60)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ef7256a9-b8c8-4b86-a993-d7f98349cf87",
   "metadata": {
    "name": "md_simple_viz",
    "collapsed": false
   },
   "source": "### Interactive Semantic View Visualization\nStreamlit app for exploring semantic views with dynamic metric/dimension discovery and chart generation."
  },
  {
   "cell_type": "code",
   "id": "ad085a22-5652-449a-a2b6-07883d7754bc",
   "metadata": {
    "language": "python",
    "name": "py_simple_viz",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Interactive Semantic View Visualization - Streamlit App for Snowflake Notebooks\n# Uses SHOW METRICS and SHOW DIMENSIONS to dynamically discover available metrics and dimensions\n# \n# Usage in Snowflake Notebook:\n# 1. Make sure you have created the HR_SEMANTIC_VIEW\n# 2. Paste this code into a Streamlit cell\n# 3. The app will automatically discover metrics and dimensions\n\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\n\n# Semantic view configuration - adjust if needed\nSEMANTIC_VIEW_NAME = \"HR_SEMANTIC_VIEW\"\nSEMANTIC_VIEW_SCHEMA = \"SV_VHOL_DB.VHOL_SCHEMA\"  # Full schema path\nSEMANTIC_VIEW_FULL_NAME = f\"{SEMANTIC_VIEW_SCHEMA}.{SEMANTIC_VIEW_NAME}\"\n\ndef main():\n    st.title(\" Semantic View Interactive Visualization\")\n    st.markdown(f\"**Semantic View:** `{SEMANTIC_VIEW_FULL_NAME}`\")\n    \n    # Check if session is available (Snowflake notebook context)\n    if 'session' not in globals():\n        st.error(\" Snowflake session not available. Please run this in a Snowflake notebook.\")\n        st.info(\" Make sure you're running this in a Snowflake notebook with `session` available\")\n        return\n    \n    # Extract available metrics and dimensions using SHOW commands\n    @st.cache_data\n    def get_options():\n        \"\"\"Get metrics and dimensions from semantic view using SHOW SEMANTIC METRICS/DIMENSIONS commands\n        Returns: (metrics_list, dimensions_list, metrics_map, dimensions_map)\n        where maps contain full_name -> short_name mappings\n        \"\"\"\n        metrics = []\n        dimensions = []\n        metrics_map = {}  # full_name -> short_name\n        dimensions_map = {}  # full_name -> short_name\n        \n        try:\n            # Get metrics from semantic view\n            show_metrics_sql = f\"SHOW SEMANTIC METRICS IN {SEMANTIC_VIEW_FULL_NAME}\"\n            \n            with st.spinner(\" Fetching metrics from semantic view...\"):\n                metrics_result = session.sql(show_metrics_sql).collect()\n            \n            if metrics_result and len(metrics_result) > 0:\n                # Convert to DataFrame to inspect structure\n                metrics_df = pd.DataFrame([dict(row.asDict()) for row in metrics_result])\n                \n                # Debug: Show available columns (first time only)\n                if 'metrics_debug' not in st.session_state:\n                    with st.expander(\" Metrics Result Structure (Debug)\", expanded=False):\n                        st.dataframe(metrics_df.head())\n                        st.write(f\"Columns: {list(metrics_df.columns)}\")\n                    st.session_state.metrics_debug = True\n                \n                # Extract metric names - try common column names\n                metric_name_col = None\n                table_name_col = None\n                \n                for col in ['name', 'metric_name', 'metric', 'METRIC_NAME', 'NAME']:\n                    if col in metrics_df.columns:\n                        metric_name_col = col\n                        break\n                \n                # Try to find table name column\n                for col in ['table_name', 'table', 'TABLE_NAME', 'TABLE', 'source_table', 'entity_name']:\n                    if col in metrics_df.columns:\n                        table_name_col = col\n                        break\n                \n                if metric_name_col:\n                    for _, row in metrics_df.iterrows():\n                        metric_name = str(row[metric_name_col]).strip()\n                        if pd.isna(metric_name) or not metric_name:\n                            continue\n                        \n                        # Try to get table name\n                        table_name = None\n                        if table_name_col and table_name_col in row:\n                            table_name = str(row[table_name_col]).strip()\n                            if pd.isna(table_name) or not table_name:\n                                table_name = None\n                        \n                        # Check if metric_name already contains table prefix (table.metric format)\n                        if '.' in metric_name:\n                            # Already has table prefix\n                            full_name = metric_name\n                            short_name = metric_name.split('.')[-1]\n                            metrics.append(full_name)\n                            metrics_map[full_name] = short_name\n                        elif table_name:\n                            # Create full name with table prefix\n                            full_name = f\"{table_name}.{metric_name}\"\n                            metrics.append(full_name)\n                            metrics_map[full_name] = metric_name\n                        else:\n                            # If no table name, use just the metric name\n                            metrics.append(metric_name)\n                            metrics_map[metric_name] = metric_name\n                else:\n                    # Fallback: use first column\n                    metrics_raw = metrics_df.iloc[:, 0].dropna().unique().tolist()\n                    for metric in metrics_raw:\n                        metrics.append(str(metric))\n                        metrics_map[str(metric)] = str(metric)\n            else:\n                st.warning(\" No metrics found in semantic view\")\n            \n            # Get dimensions from semantic view\n            show_dimensions_sql = f\"SHOW SEMANTIC DIMENSIONS IN {SEMANTIC_VIEW_FULL_NAME}\"\n            \n            with st.spinner(\" Fetching dimensions from semantic view...\"):\n                dimensions_result = session.sql(show_dimensions_sql).collect()\n            \n            if dimensions_result and len(dimensions_result) > 0:\n                # Convert to DataFrame to inspect structure\n                dimensions_df = pd.DataFrame([dict(row.asDict()) for row in dimensions_result])\n                \n                # Debug: Show available columns (first time only)\n                if 'dimensions_debug' not in st.session_state:\n                    with st.expander(\" Dimensions Result Structure (Debug)\", expanded=False):\n                        st.dataframe(dimensions_df.head())\n                        st.write(f\"Columns: {list(dimensions_df.columns)}\")\n                    st.session_state.dimensions_debug = True\n                \n                # Extract dimension names - try common column names\n                dimension_name_col = None\n                table_name_col = None\n                \n                for col in ['name', 'dimension_name', 'dimension', 'DIMENSION_NAME', 'NAME']:\n                    if col in dimensions_df.columns:\n                        dimension_name_col = col\n                        break\n                \n                # Try to find table name column\n                for col in ['table_name', 'table', 'TABLE_NAME', 'TABLE', 'source_table', 'entity_name']:\n                    if col in dimensions_df.columns:\n                        table_name_col = col\n                        break\n                \n                if dimension_name_col:\n                    for _, row in dimensions_df.iterrows():\n                        dimension_name = str(row[dimension_name_col]).strip()\n                        if pd.isna(dimension_name) or not dimension_name:\n                            continue\n                        \n                        # Try to get table name\n                        table_name = None\n                        if table_name_col and table_name_col in row:\n                            table_name = str(row[table_name_col]).strip()\n                            if pd.isna(table_name) or not table_name:\n                                table_name = None\n                        \n                        # Check if dimension_name already contains table prefix (table.dimension format)\n                        if '.' in dimension_name:\n                            # Already has table prefix\n                            full_name = dimension_name\n                            short_name = dimension_name.split('.')[-1]\n                            dimensions.append(full_name)\n                            dimensions_map[full_name] = short_name\n                        elif table_name:\n                            # Create full name with table prefix\n                            full_name = f\"{table_name}.{dimension_name}\"\n                            dimensions.append(full_name)\n                            dimensions_map[full_name] = dimension_name\n                        else:\n                            # If no table name, use just the dimension name\n                            dimensions.append(dimension_name)\n                            dimensions_map[dimension_name] = dimension_name\n                else:\n                    # Fallback: use first column\n                    dimensions_raw = dimensions_df.iloc[:, 0].dropna().unique().tolist()\n                    for dim in dimensions_raw:\n                        dimensions.append(str(dim))\n                        dimensions_map[str(dim)] = str(dim)\n            else:\n                st.warning(\" No dimensions found in semantic view\")\n            \n            # Fallback values if nothing found\n            if not metrics and not dimensions:\n                st.error(\" Could not retrieve metrics or dimensions. Using fallback values.\")\n                st.info(\" Make sure the semantic view exists and is accessible\")\n                metrics = [\"HR_EMPLOYEE_FACT.TOTAL_EMPLOYEES\", \"HR_EMPLOYEE_FACT.AVG_SALARY\", \n                          \"HR_EMPLOYEE_FACT.TOTAL_SALARY_COST\", \"HR_EMPLOYEE_FACT.ATTRITION_COUNT\"]\n                dimensions = [\"DEPARTMENT_DIM.DEPARTMENT_NAME\", \"JOB_DIM.JOB_TITLE\", \n                            \"LOCATION_DIM.LOCATION_NAME\", \"EMPLOYEE_DIM.EMPLOYEE_NAME\"]\n                # Create mappings for fallback\n                for m in metrics:\n                    metrics_map[m] = m.split('.')[-1] if '.' in m else m\n                for d in dimensions:\n                    dimensions_map[d] = d.split('.')[-1] if '.' in d else d\n            elif not metrics:\n                st.warning(\" No metrics found, using fallback\")\n                metrics = [\"HR_EMPLOYEE_FACT.TOTAL_EMPLOYEES\", \"HR_EMPLOYEE_FACT.AVG_SALARY\", \n                          \"HR_EMPLOYEE_FACT.TOTAL_SALARY_COST\"]\n                for m in metrics:\n                    metrics_map[m] = m.split('.')[-1] if '.' in m else m\n            elif not dimensions:\n                st.warning(\" No dimensions found, using fallback\")\n                dimensions = [\"DEPARTMENT_DIM.DEPARTMENT_NAME\", \"JOB_DIM.JOB_TITLE\", \n                            \"LOCATION_DIM.LOCATION_NAME\"]\n                for d in dimensions:\n                    dimensions_map[d] = d.split('.')[-1] if '.' in d else d\n            \n        except Exception as e:\n            st.error(f\" Error fetching metrics/dimensions: {str(e)}\")\n            st.info(\" Using fallback values. Make sure the semantic view exists and is accessible.\")\n            # Fallback values\n            metrics = [\"HR_EMPLOYEE_FACT.TOTAL_EMPLOYEES\", \"HR_EMPLOYEE_FACT.AVG_SALARY\", \n                      \"HR_EMPLOYEE_FACT.TOTAL_SALARY_COST\", \"HR_EMPLOYEE_FACT.ATTRITION_COUNT\"]\n            dimensions = [\"DEPARTMENT_DIM.DEPARTMENT_NAME\", \"JOB_DIM.JOB_TITLE\", \n                        \"LOCATION_DIM.LOCATION_NAME\", \"EMPLOYEE_DIM.EMPLOYEE_NAME\"]\n            # Create mappings for fallback\n            for m in metrics:\n                metrics_map[m] = m.split('.')[-1] if '.' in m else m\n            for d in dimensions:\n                dimensions_map[d] = d.split('.')[-1] if '.' in d else d\n            import traceback\n            with st.expander(\" Error Details\"):\n                st.code(traceback.format_exc(), language='python')\n        \n        # Remove duplicates while preserving order\n        metrics = list(dict.fromkeys(metrics))\n        dimensions = list(dict.fromkeys(dimensions))\n        \n        return metrics, dimensions, metrics_map, dimensions_map\n\n    try:\n        metrics, dimensions, metrics_map, dimensions_map = get_options()\n        \n        if not metrics or not dimensions:\n            st.error(\" Could not load metrics or dimensions. Please check the semantic view.\")\n            return\n        \n        # Create two columns for the dropdowns\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            selected_metric_full = st.selectbox(\n                \" Select Metric:\",\n                metrics,\n                help=\"Choose a metric to visualize\",\n                index=0 if metrics else None\n            )\n        \n        with col2:\n            selected_dimension_full = st.selectbox(\n                \" Select Dimension:\",\n                dimensions,\n                help=\"Choose a dimension to group by\",\n                index=0 if dimensions else None\n            )\n        \n        if selected_metric_full and selected_dimension_full:\n            # Get short names for ORDER BY (without table prefix)\n            selected_metric_short = metrics_map.get(selected_metric_full, selected_metric_full.split('.')[-1] if '.' in selected_metric_full else selected_metric_full)\n            selected_dimension_short = dimensions_map.get(selected_dimension_full, selected_dimension_full.split('.')[-1] if '.' in selected_dimension_full else selected_dimension_full)\n            \n            # Configuration section\n            st.markdown(\"---\")\n            st.subheader(\" Visualization Configuration\")\n            \n            col_config1, col_config2, col_config3, col_config4 = st.columns(4)\n            \n            with col_config1:\n                limit_rows = st.number_input(\n                    \" Number of Rows:\",\n                    min_value=1,\n                    max_value=1000,\n                    value=10,\n                    step=1,\n                    help=\"Limit the number of rows returned\"\n                )\n            \n            with col_config2:\n                viz_type = st.selectbox(\n                    \" Visualization Type:\",\n                    [\"Table\", \"Vertical Bar\", \"Horizontal Bar\", \"Line\", \"Pie\"],\n                    index=1,  # Default to Vertical Bar\n                    help=\"Choose the chart type\"\n                )\n            \n            with col_config3:\n                sort_by = st.selectbox(\n                    \" Sort By:\",\n                    [\"Metric\", \"Dimension\"],\n                    index=0,  # Default to Metric\n                    help=\"Choose which column to sort by\"\n                )\n            \n            with col_config4:\n                sort_direction = st.selectbox(\n                    \" Sort Direction:\",\n                    [\"DESC\", \"ASC\"],\n                    index=0,  # Default to DESC\n                    help=\"Choose sort direction\"\n                )\n            \n            # Determine sort column\n            if sort_by == \"Metric\":\n                sort_column = selected_metric_short\n            else:\n                sort_column = selected_dimension_short\n            \n            # Generate semantic SQL using SEMANTIC_VIEW() function\n            # Use full names (with table prefix) inside SEMANTIC_VIEW()\n            # Use short names (without prefix) in ORDER BY outside SEMANTIC_VIEW()\n            query_sql = f\"\"\"SELECT * FROM SEMANTIC_VIEW(\n    {SEMANTIC_VIEW_FULL_NAME}\n    DIMENSIONS {selected_dimension_full}\n    METRICS {selected_metric_full}\n) ORDER BY {sort_column} {sort_direction} LIMIT {limit_rows}\"\"\"\n            \n            # Show the generated SQL in an expander\n            with st.expander(\" View Generated Semantic SQL\"):\n                st.code(query_sql, language='sql')\n            \n            # Execute the query and create visualization\n            try:\n                with st.spinner(\" Executing query and creating visualization...\"):\n                    try:\n                        result = session.sql(query_sql).collect()\n                    except Exception as sql_error:\n                        # If full name doesn't work, try with just the view name\n                        if \"SEMANTIC_VIEW\" in str(sql_error).upper() or \"syntax\" in str(sql_error).lower():\n                            st.info(\" Trying with view name only (without schema qualification)...\")\n                            fallback_query = f\"\"\"SELECT * FROM SEMANTIC_VIEW(\n    {SEMANTIC_VIEW_NAME}\n    DIMENSIONS {selected_dimension_full}\n    METRICS {selected_metric_full}\n) ORDER BY {sort_column} {sort_direction} LIMIT {limit_rows}\"\"\"\n                            result = session.sql(fallback_query).collect()\n                            query_sql = fallback_query  # Update the query shown\n                        else:\n                            raise sql_error\n                \n                if result and len(result) > 0:\n                    # Convert to DataFrame\n                    df = pd.DataFrame([dict(row.asDict()) for row in result])\n                    \n                    # Clean column names\n                    df.columns = [col.strip() for col in df.columns]\n                    \n                    # Ensure we have numeric data for the metric\n                    if len(df.columns) >= 2:\n                        # Try to convert metric column to numeric\n                        metric_col = df.columns[1]\n                        df[metric_col] = pd.to_numeric(df[metric_col], errors='coerce')\n                    \n                    # Determine which columns to use\n                    x_col = df.columns[0]\n                    y_col = df.columns[1] if len(df.columns) > 1 else selected_metric_short\n                    \n                    # Explicitly sort the dataframe to maintain SQL sort order\n                    # This ensures Plotly respects the sort order\n                    sort_col_in_df = None\n                    if sort_by == \"Metric\":\n                        sort_col_in_df = y_col\n                    else:\n                        sort_col_in_df = x_col\n                    \n                    # Sort dataframe to match SQL ORDER BY\n                    ascending = (sort_direction == \"ASC\")\n                    df = df.sort_values(by=sort_col_in_df, ascending=ascending).reset_index(drop=True)\n                    \n                    metric_name = selected_metric_short.replace('_', ' ').title()\n                    dimension_name = selected_dimension_short.replace('_', ' ').title()\n                    \n                    # Create visualization based on selected type\n                    if viz_type == \"Table\":\n                        # Show table directly\n                        st.dataframe(df, use_container_width=True)\n                    else:\n                        # Create chart based on type\n                        if viz_type == \"Vertical Bar\":\n                            # Create category order to preserve dataframe sort order\n                            category_order = df[x_col].tolist()\n                            fig = px.bar(\n                                df, \n                                x=x_col, \n                                y=y_col,\n                                title=f'{metric_name} by {dimension_name}',\n                                labels={\n                                    x_col: dimension_name,\n                                    y_col: metric_name\n                                },\n                                color=y_col,\n                                color_continuous_scale='Blues',\n                                category_orders={x_col: category_order}\n                            )\n                            fig.update_layout(\n                                showlegend=False,\n                                height=500,\n                                xaxis_tickangle=-45,\n                                hovermode='x unified',\n                                xaxis={'categoryorder': 'array', 'categoryarray': category_order}\n                            )\n                        \n                        elif viz_type == \"Horizontal Bar\":\n                            # For horizontal bars, preserve y-axis (category) order\n                            category_order = df[x_col].tolist()\n                            fig = px.bar(\n                                df, \n                                x=y_col,\n                                y=x_col,\n                                orientation='h',\n                                title=f'{metric_name} by {dimension_name}',\n                                labels={\n                                    x_col: dimension_name,\n                                    y_col: metric_name\n                                },\n                                color=y_col,\n                                color_continuous_scale='Blues',\n                                category_orders={x_col: category_order}\n                            )\n                            fig.update_layout(\n                                showlegend=False,\n                                height=max(400, len(df) * 30),  # Dynamic height based on rows\n                                hovermode='y unified',\n                                yaxis={'categoryorder': 'array', 'categoryarray': category_order}\n                            )\n                        \n                        elif viz_type == \"Line\":\n                            # Preserve x-axis order for line charts\n                            category_order = df[x_col].tolist()\n                            fig = px.line(\n                                df, \n                                x=x_col, \n                                y=y_col,\n                                title=f'{metric_name} by {dimension_name}',\n                                labels={\n                                    x_col: dimension_name,\n                                    y_col: metric_name\n                                },\n                                markers=True,\n                                category_orders={x_col: category_order}\n                            )\n                            fig.update_layout(\n                                height=500,\n                                xaxis_tickangle=-45,\n                                hovermode='x unified',\n                                xaxis={'categoryorder': 'array', 'categoryarray': category_order}\n                            )\n                        \n                        elif viz_type == \"Pie\":\n                            fig = px.pie(\n                                df,\n                                values=y_col,\n                                names=x_col,\n                                title=f'{metric_name} by {dimension_name}'\n                            )\n                            fig.update_layout(\n                                height=500,\n                                showlegend=True\n                            )\n                            fig.update_traces(textposition='inside', textinfo='percent+label')\n                        \n                        st.plotly_chart(fig, use_container_width=True)\n                    \n                    # Show data table in expander (always available)\n                    with st.expander(\" View Data Table\"):\n                        st.dataframe(df, use_container_width=True)\n                    \n                    # Show query execution info\n                    with st.expander(\" Query Execution Details\"):\n                        st.code(query_sql, language='sql')\n                        st.write(f\"**Rows returned:** {len(df)}\")\n                        st.write(f\"**Columns:** {', '.join(df.columns)}\")\n                        if len(df.columns) >= 2:\n                            st.write(f\"**Metric range:** {df[y_col].min():,.2f} to {df[y_col].max():,.2f}\")\n                    \n                    st.success(f\" Successfully visualized {len(df)} data points!\")\n                    \n                else:\n                    st.warning(\" No data returned from the semantic view query\")\n                    st.info(\" Try selecting different metrics or dimensions\")\n                    \n            except Exception as e:\n                st.error(f\" Error executing query: {str(e)}\")\n                st.info(\" Troubleshooting tips:\")\n                st.info(\"1. Make sure the semantic view exists and is accessible\")\n                st.info(\"2. Verify you have proper permissions to query the semantic view\")\n                st.info(\"3. Check that the metric and dimension names are correct\")\n                st.info(\"4. Try the SQL query manually in a SQL cell to debug\")\n                import traceback\n                with st.expander(\" Error Details\"):\n                    st.code(traceback.format_exc(), language='python')\n    \n    except Exception as e:\n        st.error(f\" Error loading options: {str(e)}\")\n        st.info(\" Make sure the semantic view was created successfully\")\n        import traceback\n        with st.expander(\" Error Details\"):\n            st.code(traceback.format_exc(), language='python')\n\n# Run the Streamlit app\nif __name__ == \"__main__\":\n    main()\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "634a8723-10c9-4954-aabb-f0ca3a561139",
   "metadata": {
    "name": "md_ask_cortex_analyst",
    "collapsed": false
   },
   "source": "### Natural Language Query Interface\nLet's now build a Streamlit app usin the Cortex Analyst API to convert plain English questions into SQL queries."
  },
  {
   "cell_type": "code",
   "id": "166fb675-4573-4480-93b3-58e7a78af4bb",
   "metadata": {
    "language": "python",
    "name": "py_ask_cortex_analyst",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Natural Language Query Interface for Semantic Views\n# Streamlit App for Snowflake Notebooks\n# Uses Cortex Analyst REST API\n# \n# Usage in Snowflake Notebook:\n# 1. Make sure you're in a Snowflake notebook (not local Streamlit)\n# 2. The 'session' variable should be automatically available\n# 3. Paste this code into a Streamlit cell\n# 4. Select a semantic view from the dropdown\n# 5. Type your natural language question\n# 6. Click \"Answer!\" to execute\n#\n# Note: If session is not available, ensure you're running in a Snowflake notebook environment.\n# The session variable is created automatically when you run a SQL cell in a Snowflake notebook.\n\nimport streamlit as st\nimport pandas as pd\nimport json\nimport time\n\n# Try to import _snowflake (available in Snowflake notebooks)\ntry:\n    import _snowflake  # For interacting with Snowflake-specific APIs\n    SNOWFLAKE_API_AVAILABLE = True\nexcept ImportError:\n    SNOWFLAKE_API_AVAILABLE = False\n    _snowflake = None\n\n# Schema configuration - adjust if needed\nDEFAULT_SCHEMA = \"SV_VHOL_DB.VHOL_SCHEMA\"\n\ndef make_authenticated_request_via_session(session, url, method=\"POST\", json_data=None, headers=None):\n    \"\"\"\n    Attempt to make an HTTP request using the session's connection\n    This bypasses the need for explicit OAuth token extraction\n    \"\"\"\n    try:\n        # Try to get the connection object\n        conn = None\n        if hasattr(session, '_conn'):\n            conn = session._conn\n        elif hasattr(session, 'connection'):\n            conn = session.connection\n        \n        if not conn:\n            return None\n        \n        # Try different methods to make HTTP requests through the connection\n        # Method 1: Check if connection has an HTTP client or request method\n        if hasattr(conn, '_request') or hasattr(conn, 'request'):\n            request_method = getattr(conn, '_request', None) or getattr(conn, 'request', None)\n            if request_method:\n                try:\n                    # Try to make the request\n                    response = request_method(url, method=method, json=json_data, headers=headers)\n                    return response\n                except:\n                    pass\n        \n        # Method 2: Check if there's an HTTP client or session object\n        if hasattr(conn, '_http') or hasattr(conn, 'http') or hasattr(conn, '_session') or hasattr(conn, 'session'):\n            http_client = (getattr(conn, '_http', None) or \n                          getattr(conn, 'http', None) or\n                          getattr(conn, '_session', None) or\n                          getattr(conn, 'session', None))\n            if http_client:\n                try:\n                    if method == \"POST\":\n                        response = http_client.post(url, json=json_data, headers=headers)\n                    else:\n                        response = http_client.request(method, url, json=json_data, headers=headers)\n                    return response\n                except:\n                    pass\n        \n    except Exception:\n        pass\n    \n    return None\n\ndef generate_oauth_token_from_session(session, account, region):\n    \"\"\"\n    Attempt to generate an OAuth token using the current session\n    This uses Snowflake's OAuth API to create a token for REST API calls\n    \"\"\"\n    try:\n        # Try to use Snowflake's OAuth token generation\n        # Note: SYSTEM$GENERATE_OAUTH_TOKEN might not be available\n        try:\n            token_result = session.sql(\"SELECT SYSTEM$GENERATE_OAUTH_TOKEN() as token\").collect()\n            if token_result and len(token_result) > 0:\n                token = token_result[0].get('TOKEN')\n                if token:\n                    return token\n        except:\n            # SYSTEM$GENERATE_OAUTH_TOKEN might not be available\n            pass\n        \n    except Exception as e:\n        # Silently fail\n        pass\n    \n    return None\n\ndef get_auth_token(session):\n    \"\"\"Try to extract authentication token from Snowflake session\"\"\"\n    auth_token = None\n    \n    def _check_object_for_token(obj, depth=0, max_depth=3):\n        \"\"\"Recursively search an object for token-like values\"\"\"\n        if depth > max_depth or obj is None:\n            return None\n        \n        # Check direct token attributes\n        token_attrs = ['_token', 'token', '_master_token', 'master_token', '_session_token', \n                      'session_token', 'access_token', '_access_token', 'bearer_token', '_bearer_token']\n        for attr in token_attrs:\n            if hasattr(obj, attr):\n                try:\n                    value = getattr(obj, attr)\n                    if value and isinstance(value, str) and len(value) > 20:  # Tokens are usually long strings\n                        return value\n                except:\n                    pass\n        \n        # Check if it's a dict-like object\n        if hasattr(obj, '__dict__'):\n            for key, value in obj.__dict__.items():\n                if 'token' in key.lower() and isinstance(value, str) and len(value) > 20:\n                    return value\n                # Recursively check nested objects (but limit depth)\n                if depth < max_depth and isinstance(value, object) and not isinstance(value, (str, int, float, bool)):\n                    result = _check_object_for_token(value, depth + 1, max_depth)\n                    if result:\n                        return result\n        \n        return None\n    \n    try:\n        # Try to get from session's connection\n        conn = None\n        \n        # Method 1: Try session._conn (Snowpark)\n        if hasattr(session, '_conn'):\n            conn = session._conn\n        # Method 2: Try session.connection (alternative attribute name)\n        elif hasattr(session, 'connection'):\n            conn = session.connection\n        # Method 3: Try session._connection (another variant)\n        elif hasattr(session, '_connection'):\n            conn = session._connection\n        \n        if conn:\n            # Method A: Try REST client token (for Python connector connections)\n            if hasattr(conn, '_rest'):\n                rest_client = conn._rest\n                # Try direct attributes first\n                for token_attr in ['_token', 'token', '_master_token', 'master_token', '_session_token']:\n                    if hasattr(rest_client, token_attr):\n                        try:\n                            token_value = getattr(rest_client, token_attr)\n                            if token_value and isinstance(token_value, str) and len(token_value) > 20:\n                                auth_token = token_value\n                                break\n                        except:\n                            pass\n                \n                # Try recursive search if direct access failed\n                if not auth_token:\n                    auth_token = _check_object_for_token(rest_client, max_depth=2)\n                \n                # Try token manager if available\n                if not auth_token and hasattr(rest_client, '_token_manager'):\n                    token_manager = rest_client._token_manager\n                    auth_token = _check_object_for_token(token_manager, max_depth=2)\n            \n            # Method A2: For ServerConnection (Snowflake notebooks), try different attributes\n            # ServerConnection might have token stored differently\n            if not auth_token:\n                # Try connection-level token attributes\n                auth_token = _check_object_for_token(conn, max_depth=3)\n            \n            # Method A3: Try to get from connection's internal state\n            if not auth_token:\n                # Check for session token or authentication state\n                internal_attrs = ['_session_token', '_auth_token', '_token', 'token', \n                                 '_session', '_authenticator', '_login_manager']\n                for attr in internal_attrs:\n                    if hasattr(conn, attr):\n                        try:\n                            value = getattr(conn, attr)\n                            if isinstance(value, str) and len(value) > 20:\n                                auth_token = value\n                                break\n                            elif hasattr(value, '__dict__'):\n                                # If it's an object, search it recursively\n                                token = _check_object_for_token(value, max_depth=2)\n                                if token:\n                                    auth_token = token\n                                    break\n                        except:\n                            pass\n            \n            # Method B: Try connection-level token attributes (recursive)\n            if not auth_token:\n                auth_token = _check_object_for_token(conn, max_depth=3)\n            \n            # Method C: Try from connection's authentication handler\n            if not auth_token:\n                auth_attrs = ['_authenticate', '_auth', 'authenticate', '_auth_handler', 'auth_handler']\n                for auth_attr in auth_attrs:\n                    if hasattr(conn, auth_attr):\n                        try:\n                            auth_handler = getattr(conn, auth_attr)\n                            auth_token = _check_object_for_token(auth_handler, max_depth=2)\n                            if auth_token:\n                                break\n                        except:\n                            pass\n            \n            # Method D: Try to get from connection's headers/cookies\n            if not auth_token and hasattr(conn, '_rest'):\n                rest_client = conn._rest\n                # Check if there's a headers dict with authorization\n                header_attrs = ['_headers', 'headers', '_request_headers', 'request_headers']\n                for header_attr in header_attrs:\n                    if hasattr(rest_client, header_attr):\n                        try:\n                            headers = getattr(rest_client, header_attr)\n                            if isinstance(headers, dict):\n                                auth_header = headers.get('Authorization') or headers.get('authorization')\n                                if auth_header and isinstance(auth_header, str):\n                                    if auth_header.startswith('Bearer '):\n                                        auth_token = auth_header[7:]  # Remove 'Bearer ' prefix\n                                    else:\n                                        auth_token = auth_header\n                                    if auth_token:\n                                        break\n                        except:\n                            pass\n    \n    except Exception as e:\n        # Silently fail - we'll handle missing token in the UI\n        pass\n    \n    return auth_token\n\ndef main():\n    st.title(\" Natural Language Query for Semantic Views\")\n    st.markdown(\"Ask questions in plain English about your semantic view data\")\n    st.markdown(\"*Using [Cortex Analyst REST API](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst/rest-api)*\")\n    \n    # Check if session is available (Snowflake notebook context)\n    # In Snowflake notebooks, session is typically available as a global variable\n    if 'session' not in globals():\n        st.error(\" Snowflake session not available. Please run this in a Snowflake notebook.\")\n        st.info(\" Make sure you're running this in a Snowflake notebook with `session` available\")\n        return\n    \n    # Get account and region info early - cache it for the session\n    @st.cache_data\n    def get_account_info():\n        \"\"\"Get account and region from the current Snowflake session\"\"\"\n        try:\n            account_info = session.sql(\"SELECT CURRENT_ACCOUNT() as account, CURRENT_REGION() as region\").collect()\n            if account_info and len(account_info) > 0:\n                account = account_info[0]['ACCOUNT']\n                region = account_info[0]['REGION']\n                return account, region\n        except Exception:\n            pass\n        return None, None\n    \n    # Pre-populate account and region first (needed for token generation)\n    account, region = get_account_info()\n    \n    # Get token early - cache it for the session\n    @st.cache_data\n    def get_cached_token(account_val, region_val):\n        \"\"\"Get auth token from session - cached, tries extraction then generation\"\"\"\n        # First try to extract existing token\n        token = get_auth_token(session)\n        \n        # If extraction failed and we have account/region, try generating one\n        if not token and account_val and region_val:\n            try:\n                token = generate_oauth_token_from_session(session, account_val, region_val)\n            except:\n                pass\n        \n        return token\n    \n    # Check if _snowflake API is available (required for authentication)\n    if account and region:\n        if not SNOWFLAKE_API_AVAILABLE:\n            st.error(\" `_snowflake` module not available. This app requires running in a Snowflake notebook.\")\n            st.info(\" The `_snowflake` module provides automatic authentication for REST API calls.\")\n            return\n    else:\n        st.warning(\" Could not retrieve account information. Some features may not work.\")\n    \n    # Get available semantic views in the schema\n    @st.cache_data\n    def get_semantic_views(schema_name):\n        \"\"\"Get list of available semantic views in the schema\"\"\"\n        try:\n            # Handle schema name (could be \"DATABASE.SCHEMA\" or just \"SCHEMA\")\n            if '.' in schema_name:\n                database, schema = schema_name.split('.', 1)\n                show_sql = f\"SHOW SEMANTIC VIEWS IN SCHEMA {database}.{schema}\"\n            else:\n                # Try to use current database context\n                show_sql = f\"SHOW SEMANTIC VIEWS IN SCHEMA {schema_name}\"\n            \n            result = session.sql(show_sql).collect()\n            \n            if result and len(result) > 0:\n                # Convert to DataFrame\n                views_df = pd.DataFrame([dict(row.asDict()) for row in result])\n                \n                # Try to find the name column\n                name_col = None\n                for col in ['name', 'semantic_view_name', 'view_name', 'NAME', 'SEMANTIC_VIEW_NAME']:\n                    if col in views_df.columns:\n                        name_col = col\n                        break\n                \n                if name_col:\n                    views = views_df[name_col].dropna().unique().tolist()\n                else:\n                    # Fallback: use first column\n                    views = views_df.iloc[:, 0].dropna().unique().tolist()\n                \n                # Create full qualified names\n                full_names = []\n                for view in views:\n                    full_name = f\"{schema_name}.{view}\" if '.' not in view else view\n                    full_names.append(full_name)\n                \n                return full_names, views_df\n            else:\n                return [], pd.DataFrame()\n                \n        except Exception as e:\n            st.error(f\" Error fetching semantic views: {str(e)}\")\n            return [], pd.DataFrame()\n    \n    # Schema selection\n    schema_input = st.text_input(\n        \" Schema:\",\n        value=DEFAULT_SCHEMA,\n        help=\"Enter the schema path (e.g., DATABASE.SCHEMA)\"\n    )\n    \n    # Get semantic views\n    with st.spinner(\" Loading semantic views...\"):\n        semantic_views, views_df = get_semantic_views(schema_input)\n    \n    if not semantic_views:\n        st.warning(f\" No semantic views found in {schema_input}\")\n        st.info(\" Make sure the schema name is correct and contains semantic views\")\n        \n        # Show debug info if available\n        if not views_df.empty:\n            with st.expander(\" Debug: SHOW SEMANTIC VIEWS Result\"):\n                st.dataframe(views_df)\n        return\n    \n    # Semantic view selection\n    selected_view = st.selectbox(\n        \" Select Semantic View:\",\n        semantic_views,\n        help=\"Choose a semantic view to query\",\n        index=0 if semantic_views else None\n    )\n    \n    if selected_view:\n        st.markdown(\"---\")\n        \n        # Natural language question input\n        st.subheader(\" Ask Your Question\")\n        question = st.text_area(\n            \"Enter your question:\",\n            height=100,\n            placeholder=\"e.g., What are the top 5 departments by average salary?\",\n            help=\"Type your question in natural language\"\n        )\n        \n        # Answer button\n        col1, col2 = st.columns([1, 4])\n        with col1:\n            answer_button = st.button(\" Answer!\", type=\"primary\", use_container_width=True)\n        \n        if answer_button and question:\n            if not question.strip():\n                st.warning(\" Please enter a question\")\n            else:\n                # Generate SQL from natural language question using Cortex Analyst REST API\n                generated_sql = None  # Initialize outside try block\n                \n                try:\n                    with st.spinner(\" Generating SQL from your question...\"):\n                        # Use Snowflake's built-in API request method (no token needed!)\n                        if not SNOWFLAKE_API_AVAILABLE:\n                            st.error(\" `_snowflake` module not available. Make sure you're running this in a Snowflake notebook.\")\n                            st.info(\" The `_snowflake` module is automatically available in Snowflake notebooks.\")\n                            return\n                        \n                        # Build request body for Cortex Analyst API\n                        # According to Snowflake Labs example: https://github.com/Snowflake-Labs/sfguide-getting-started-with-cortex-analyst\n                        # Note: API requires exactly one of: semantic_model, semantic_model_file, or semantic_view\n                        request_body = {\n                            \"messages\": [\n                                {\n                                    \"role\": \"user\",\n                                    \"content\": [\n                                        {\n                                            \"type\": \"text\",\n                                            \"text\": question\n                                        }\n                                    ]\n                                }\n                            ],\n                            \"semantic_view\": selected_view\n                        }\n                        \n                        # Use Snowflake's built-in API request method\n                        # This automatically handles authentication - no token needed!\n                        API_ENDPOINT = \"/api/v2/cortex/analyst/message\"\n                        API_TIMEOUT = 50000  # in milliseconds\n                        \n                        resp = _snowflake.send_snow_api_request(\n                            \"POST\",  # method\n                            API_ENDPOINT,  # path\n                            {},  # headers (empty - auth is handled automatically)\n                            {},  # params\n                            request_body,  # body\n                            None,  # request_guid\n                            API_TIMEOUT,  # timeout in milliseconds\n                        )\n                        \n                        # Parse response\n                        # Content is a string with serialized JSON object\n                        parsed_content = json.loads(resp[\"content\"])\n                        \n                        # Check if the response is successful\n                        if resp[\"status\"] >= 400:\n                            # Error response\n                            error_msg = f\"\"\"\n An Analyst API error has occurred \n\n* response code: `{resp['status']}`\n* request-id: `{parsed_content.get('request_id', 'N/A')}`\n* error code: `{parsed_content.get('error_code', 'N/A')}`\n\nMessage:\n```\n{parsed_content.get('message', 'Unknown error')}\n```\n                            \"\"\"\n                            st.error(error_msg)\n                            generated_sql = None\n                        else:\n                            # Success - extract response data\n                            response_data = parsed_content\n                            \n                            # Extract SQL from response\n                            # Response structure: message.content[] with type \"sql\" containing \"statement\"\n                            text_response = None\n                            \n                            if 'message' in response_data and 'content' in response_data['message']:\n                                for content_block in response_data['message']['content']:\n                                    if content_block.get('type') == 'sql':\n                                        generated_sql = content_block.get('statement', '')\n                                    elif content_block.get('type') == 'text':\n                                        text_response = content_block.get('text', '')\n                            \n                            # Show text interpretation if available\n                            if text_response:\n                                with st.expander(\" Interpretation\", expanded=False):\n                                    st.write(text_response)\n                            \n                            # Show warnings if any\n                            if 'warnings' in response_data and response_data['warnings']:\n                                for warning in response_data['warnings']:\n                                    st.warning(f\" {warning.get('message', 'Warning')}\")\n                            \n                            if generated_sql:\n                                # Show generated SQL\n                                with st.expander(\" Generated SQL Query\", expanded=False):\n                                    st.code(generated_sql, language='sql')\n                                \n                                # Show response metadata if available\n                                if 'response_metadata' in response_data:\n                                    with st.expander(\" Response Metadata\", expanded=False):\n                                        st.json(response_data['response_metadata'])\n                            else:\n                                # Check if suggestions were provided\n                                suggestions_found = False\n                                if 'message' in response_data and 'content' in response_data['message']:\n                                    for content_block in response_data['message']['content']:\n                                        if content_block.get('type') == 'suggestions':\n                                            st.info(\" Your question might be ambiguous. Here are some suggestions:\")\n                                            suggestions = content_block.get('suggestions', [])\n                                            for i, suggestion in enumerate(suggestions, 1):\n                                                st.write(f\"{i}. {suggestion}\")\n                                            suggestions_found = True\n                                \n                                if not suggestions_found:\n                                    st.error(\" No SQL generated. Check the response for details.\")\n                                    with st.expander(\" Full Response\"):\n                                        st.json(response_data)\n                                    generated_sql = None  # Ensure it's None if no SQL generated\n                        \n                        # Execute the query if SQL was generated\n                        if generated_sql:\n                            with st.spinner(\" Executing query...\"):\n                                try:\n                                    result = session.sql(generated_sql).collect()\n                                    \n                                    if result and len(result) > 0:\n                                        # Convert to DataFrame\n                                        df = pd.DataFrame([dict(row.asDict()) for row in result])\n                                        \n                                        # Display results\n                                        st.subheader(\" Results\")\n                                        st.dataframe(df, use_container_width=True)\n                                        \n                                        # Show summary\n                                        st.success(f\" Query executed successfully! Returned {len(df)} rows.\")\n                                        \n                                        # Show query details\n                                        with st.expander(\" Query Details\"):\n                                            st.code(generated_sql, language='sql')\n                                            st.write(f\"**Rows returned:** {len(df)}\")\n                                            st.write(f\"**Columns:** {', '.join(df.columns)}\")\n                                        \n                                    else:\n                                        st.info(\" Query executed but returned no results.\")\n                                        \n                                except Exception as e:\n                                    st.error(f\" Error executing query: {str(e)}\")\n                                    st.info(\" The generated SQL might need adjustment. Check the generated SQL above.\")\n                                    import traceback\n                                    with st.expander(\" Error Details\"):\n                                        st.code(traceback.format_exc(), language='python')\n                        \n                        else:\n                            st.error(\" Could not generate SQL from Cortex Analyst API\")\n                            st.info(\" Check the API response above for details.\")\n                    \n                except Exception as e:\n                    st.error(f\" Error generating SQL: {str(e)}\")\n                    st.info(\" Make sure you're running in a Snowflake notebook and that Cortex Analyst is available in your account.\")\n                    import traceback\n                    with st.expander(\" Error Details\"):\n                        st.code(traceback.format_exc(), language='python')\n    \n    # Show available semantic views info\n    with st.expander(\" About This App\"):\n        st.markdown(\"\"\"\n        **How to use:**\n        1. Select a semantic view from the dropdown\n        2. Type your question in natural language\n        3. Click \"Answer!\" to generate and execute the query\n        \n        **Example questions:**\n        - \"What are the top 10 departments by total employees?\"\n        - \"Show me average salary by job title\"\n        - \"Which locations have the highest attrition rates?\"\n        - \"List the top 5 employees by salary\"\n        \n        **Note:** This app uses the [Cortex Analyst REST API](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst/rest-api) \n        to generate SQL from natural language questions. The API automatically understands your semantic view \n        structure and generates appropriate queries.\n        \n        **Authentication:** The app attempts to automatically retrieve your authentication token from the session.\n        If that fails, you can manually enter an OAuth token when prompted.\n        \"\"\")\n\n# Run the Streamlit app\nif __name__ == \"__main__\":\n    main()\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8474d0a9-6b31-43e6-812b-d55ec602b5d8",
   "metadata": {
    "name": "md_enable_and_configure_snowflake_intelligence",
    "collapsed": false
   },
   "source": "## Enable and Configure Snowflake Intelligence\n\nHere, we'll set up the infrastructure for Snowflake Intelligence by creating network rules, external access integration, and granting necessary privileges to enable agent creation. \n\nFinally, we'll create an intelligent agent called `\"Agentic_Analytics_VHOL_Chatbot\"` that can answer cross-functional business questions by querying four different semantic views (Finance, Sales, HR, and Marketing) using natural language."
  },
  {
   "cell_type": "code",
   "id": "1426bf26-f130-4163-ba8f-57204b3032c0",
   "metadata": {
    "language": "sql",
    "name": "sql_enable_and_configure_snowflake_intelligence"
   },
   "outputs": [],
   "source": "-- Set role, database and schema\nUSE ROLE accountadmin;\nUSE DATABASE SV_VHOL_DB;\nUSE SCHEMA VHOL_SCHEMA;\n\n-- Create the AGENTS schema\nCREATE OR REPLACE SCHEMA SV_VHOL_DB.AGENTS;\n\n-- Create network rule in the correct schema\nCREATE OR REPLACE NETWORK RULE SV_VHOL_DB.AGENTS.Snowflake_intelligence_WebAccessRule\n  MODE = EGRESS\n  TYPE = HOST_PORT\n  VALUE_LIST = ('0.0.0.0:80', '0.0.0.0:443');\n\n-- Create external access integration\nCREATE OR REPLACE EXTERNAL ACCESS INTEGRATION Snowflake_intelligence_ExternalAccess_Integration\n  ALLOWED_NETWORK_RULES = (SV_VHOL_DB.AGENTS.Snowflake_intelligence_WebAccessRule)\n  ENABLED = true;\n\n-- Grant privileges\nGRANT ALL PRIVILEGES ON DATABASE SV_VHOL_DB TO ROLE agentic_analytics_vhol_role;\nGRANT ALL PRIVILEGES ON SCHEMA SV_VHOL_DB.AGENTS TO ROLE agentic_analytics_vhol_role;\nGRANT ALL PRIVILEGES ON SCHEMA SV_VHOL_DB.VHOL_SCHEMA TO ROLE agentic_analytics_vhol_role;\nGRANT CREATE AGENT ON SCHEMA SV_VHOL_DB.AGENTS TO ROLE agentic_analytics_vhol_role;\nGRANT USAGE ON INTEGRATION Snowflake_intelligence_ExternalAccess_Integration TO ROLE agentic_analytics_vhol_role;\nGRANT USAGE ON NETWORK RULE SV_VHOL_DB.AGENTS.Snowflake_intelligence_WebAccessRule TO ROLE agentic_analytics_vhol_role;\n\n-- Switch to the working role\nUSE ROLE agentic_analytics_vhol_role;\nUSE DATABASE SV_VHOL_DB;\nUSE SCHEMA AGENTS;\n\n-- Create the agent\nCREATE OR REPLACE AGENT SV_VHOL_DB.AGENTS.Agentic_Analytics_VHOL_Chatbot\nWITH PROFILE='{ \"display_name\": \"1-Agentic Analytics VHOL Chatbot\" }'\n    COMMENT='This is an agent that can answer questions about company specific Sales, Marketing, HR & Finance questions.'\nFROM SPECIFICATION $$\n{\n  \"models\": {\n    \"orchestration\": \"\"\n  },\n  \"instructions\": {\n    \"response\": \"Answer user questions about Sales, Marketing, HR, and Finance using the provided semantic views. When appropriate, ask clarifying questions, generate safe SQL via the tools, and summarize results clearly.\"\n  },\n  \"tools\": [\n    {\n      \"tool_spec\": {\n        \"type\": \"cortex_analyst_text_to_sql\",\n        \"name\": \"Query Finance Datamart\",\n        \"description\": \"Allows users to query finance data for revenue & expenses.\"\n      }\n    },\n    {\n      \"tool_spec\": {\n        \"type\": \"cortex_analyst_text_to_sql\",\n        \"name\": \"Query Sales Datamart\",\n        \"description\": \"Allows users to query sales data such as products and sales reps.\"\n      }\n    },\n    {\n      \"tool_spec\": {\n        \"type\": \"cortex_analyst_text_to_sql\",\n        \"name\": \"Query HR Datamart\",\n        \"description\": \"Allows users to query HR data; employee_name includes sales rep names.\"\n      }\n    },\n    {\n      \"tool_spec\": {\n        \"type\": \"cortex_analyst_text_to_sql\",\n        \"name\": \"Query Marketing Datamart\",\n        \"description\": \"Allows users to query campaigns, channels, impressions, and spend.\"\n      }\n    }\n  ],\n  \"tool_resources\": {\n    \"Query Finance Datamart\": {\n      \"semantic_view\": \"SV_VHOL_DB.VHOL_SCHEMA.FINANCE_SEMANTIC_VIEW\"\n    },\n    \"Query HR Datamart\": {\n      \"semantic_view\": \"SV_VHOL_DB.VHOL_SCHEMA.HR_SEMANTIC_VIEW\"\n    },\n    \"Query Marketing Datamart\": {\n      \"semantic_view\": \"SV_VHOL_DB.VHOL_SCHEMA.MARKETING_SEMANTIC_VIEW\"\n    },\n    \"Query Sales Datamart\": {\n      \"semantic_view\": \"SV_VHOL_DB.VHOL_SCHEMA.SALES_SEMANTIC_VIEW\"\n    }\n  }\n}\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64eb752b-37af-4b40-bfb2-193b63af552b",
   "metadata": {
    "language": "sql",
    "name": "bak_enable_and_configure_snowflake_intelligence"
   },
   "outputs": [],
   "source": "-- -- Set role, database and schema\n-- USE ROLE agentic_analytics_vhol_role;\n-- USE DATABASE SV_VHOL_DB;\n-- USE SCHEMA VHOL_SCHEMA;\n\n-- CREATE OR REPLACE SCHEMA SV_VHOL_DB.AGENTS;\n\n\n-- -- NETWORK rule is part of db schema\n-- CREATE OR REPLACE NETWORK RULE Snowflake_intelligence_WebAccessRule\n--   MODE = EGRESS\n--   TYPE = HOST_PORT\n--   VALUE_LIST = ('0.0.0.0:80', '0.0.0.0:443');\n\n\n\n-- -- Grant privileges\n-- GRANT ALL PRIVILEGES ON DATABASE SV_VHOL_DB TO ROLE ACCOUNTADMIN;\n-- GRANT ALL PRIVILEGES ON SCHEMA SV_VHOL_DB.VHOL_SCHEMA TO ROLE ACCOUNTADMIN;\n-- GRANT USAGE ON NETWORK RULE snowflake_intelligence_webaccessrule TO ROLE accountadmin;\n\n-- USE SCHEMA SV_VHOL_DB.VHOL_SCHEMA;\n\n-- use role accountadmin;\n-- CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION Snowflake_intelligence_ExternalAccess_Integration\n-- ALLOWED_NETWORK_RULES = (Snowflake_intelligence_WebAccessRule)\n-- ENABLED = true;\n\n\n-- GRANT USAGE ON DATABASE SV_VHOL_DB TO ROLE agentic_analytics_vhol_role;\n-- GRANT USAGE ON SCHEMA SV_VHOL_DB.agents TO ROLE agentic_analytics_vhol_role;\n-- GRANT CREATE AGENT ON SCHEMA SV_VHOL_DB.agents TO ROLE agentic_analytics_vhol_role;\n\n-- GRANT USAGE ON INTEGRATION Snowflake_intelligence_ExternalAccess_Integration TO ROLE agentic_analytics_vhol_role;\n\n\n-- -- CREATES A SNOWFLAKE INTELLIGENCE AGENT WITH MULTIPLE TOOLS\n-- -- Switch to accountadmin to grant privileges\n-- USE ROLE accountadmin;\n-- USE DATABASE SV_VHOL_DB;\n\n-- -- Ensure the AGENTS schema exists and grant proper privileges\n-- CREATE OR REPLACE SCHEMA SV_VHOL_DB.AGENTS;\n\n-- -- Grant necessary privileges to the role\n-- GRANT ALL PRIVILEGES ON SCHEMA SV_VHOL_DB.AGENTS TO ROLE agentic_analytics_vhol_role;\n-- GRANT CREATE AGENT ON SCHEMA SV_VHOL_DB.AGENTS TO ROLE agentic_analytics_vhol_role;\n-- GRANT USAGE ON SCHEMA SV_VHOL_DB.AGENTS TO ROLE agentic_analytics_vhol_role;\n\n-- -- Also ensure database-level privileges\n-- GRANT USAGE ON DATABASE SV_VHOL_DB TO ROLE agentic_analytics_vhol_role;\n\n-- USE ROLE agentic_analytics_vhol_role;\n-- USE DATABASE SV_VHOL_DB;\n-- USE SCHEMA AGENTS;\n\n-- ]\n\n-- CREATE OR REPLACE AGENT SV_VHOL_DB.AGENTS.Agentic_Analytics_VHOL_Chatbot\n-- WITH PROFILE='{ \"display_name\": \"1-Agentic Analytics VHOL Chatbot\" }'\n--     COMMENT=$$ This is an agent that can answer questions about company specific Sales, Marketing, HR & Finance questions. $$\n-- FROM SPECIFICATION $$\n-- {\n--   \"models\": {\n--     \"orchestration\": \"\"\n--   },\n--   \"instructions\": {\n--     \"response\": \"Answer user questions about Sales, Marketing, HR, and Finance using the provided semantic views. When appropriate, ask clarifying questions, generate safe SQL via the tools, and summarize results clearly.\"\n--   },\n--   \"tools\": [\n--     {\n--       \"tool_spec\": {\n--         \"type\": \"cortex_analyst_text_to_sql\",\n--         \"name\": \"Query Finance Datamart\",\n--         \"description\": \"Allows users to query finance data for revenue & expenses.\"\n--       }\n--     },\n--     {\n--       \"tool_spec\": {\n--         \"type\": \"cortex_analyst_text_to_sql\",\n--         \"name\": \"Query Sales Datamart\",\n--         \"description\": \"Allows users to query sales data such as products and sales reps.\"\n--       }\n--     },\n--     {\n--       \"tool_spec\": {\n--         \"type\": \"cortex_analyst_text_to_sql\",\n--         \"name\": \"Query HR Datamart\",\n--         \"description\": \"Allows users to query HR data; employee_name includes sales rep names.\"\n--       }\n--     },\n--     {\n--       \"tool_spec\": {\n--         \"type\": \"cortex_analyst_text_to_sql\",\n--         \"name\": \"Query Marketing Datamart\",\n--         \"description\": \"Allows users to query campaigns, channels, impressions, and spend.\"\n--       }\n--     }\n--   ],\n--   \"tool_resources\": {\n--     \"Query Finance Datamart\": {\n--       \"semantic_view\": \"SV_VHOL_DB.VHOL_SCHEMA.FINANCE_SEMANTIC_VIEW\"\n--     },\n--     \"Query HR Datamart\": {\n--       \"semantic_view\": \"SV_VHOL_DB.VHOL_SCHEMA.HR_SEMANTIC_VIEW\"\n--     },\n--     \"Query Marketing Datamart\": {\n--       \"semantic_view\": \"SV_VHOL_DB.VHOL_SCHEMA.MARKETING_SEMANTIC_VIEW\"\n--     },\n--     \"Query Sales Datamart\": {\n--       \"semantic_view\": \"SV_VHOL_DB.VHOL_SCHEMA.SALES_SEMANTIC_VIEW\"\n--     }\n--   }\n-- }\n-- $$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f9ac48cf-312e-4c82-86f9-9e411619963a",
   "metadata": {
    "name": "md_go_to_snowflake_intelligence",
    "collapsed": false
   },
   "source": "### Let's Go Talk to Our Data\n[Open the Snowflake Intelligence Agent](https://app.snowflake.com/_deeplink/#/agents/database/SV_VHOL_DB/schema/AGENTS/agent/AGENTIC_ANALYTICS_VHOL_CHATBOT/details)\n\nAsk this question: \n> *\"For each of my campaign channels, can you tell me what products customers and up using if they were exposed to that campaign?\"*"
  },
  {
   "cell_type": "code",
   "id": "210e9c51-4411-4594-a6df-5ae3991f1170",
   "metadata": {
    "language": "sql",
    "name": "sql_cleanup"
   },
   "outputs": [],
   "source": "USE ROLE agentic_analytics_vhol_role;\nUSE DATABASE SV_VHOL_DB;\nUSE SCHEMA VHOL_SCHEMA;\nDROP SEMANTIC VIEW HR_SEMANTIC_VIEW;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d256abc9-ee28-4a19-87a9-d737fc340d0e",
   "metadata": {
    "name": "md_resources",
    "collapsed": false
   },
   "source": "## Resources\n\n### Documentation\n- [Semantic Views SQL Documentation](https://docs.snowflake.com/en/user-guide/views-semantic/sql)\n- [Cortex Analyst Documentation](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst)\n- [Snowflake Intelligence (AI Agents)](https://docs.snowflake.com/en/user-guide/snowflake-intelligence)\n\n### GitHub Repositories\n- [Snowflake AI Demo (NickAkincilar)](https://github.com/NickAkincilar/Snowflake_AI_DEMO)\n- [Getting Started with Cortex Analyst: Augment BI with AI](https://www.snowflake.com/en/developers/guides/getting-started-with-cortex-analyst/)"
  }
 ]
}