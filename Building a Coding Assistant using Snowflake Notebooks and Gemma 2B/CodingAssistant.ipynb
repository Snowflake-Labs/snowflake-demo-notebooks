{
  "metadata": {
    "kernelspec": {
      "display_name": "Streamlit Notebook",
      "name": "streamlit"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Building a Coding Assistant using Snowflake Notebooks and Gemma 2B**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oNbI_6TKaGYW"
      },
      "id": "oNbI_6TKaGYW"
    },
    {
      "cell_type": "markdown",
      "id": "6ab9262e-6c3e-44f6-9b68-d7ae2c158a61",
      "metadata": {
        "name": "cell17",
        "collapsed": false,
        "id": "6ab9262e-6c3e-44f6-9b68-d7ae2c158a61"
      },
      "source": [
        "In this tutorial, we’ll guide you through the process of fine-tuning the Gemma 2B model for coding tasks using Snowflake Notebooks. Snowflake’s platform provides a powerful, scalable environment for machine learning, and by leveraging their integrated notebooks, you can easily customize models to suit your needs. Whether you're enhancing the model for specific programming languages or optimizing its understanding of code structure, this step-by-step guide will help you unlock the potential of Gemma 2B for coding-related tasks. Let’s dive into setting up your environment and getting started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f391d49a-73b3-40a8-9009-7418c3a1e6dd",
      "metadata": {
        "name": "cell23",
        "collapsed": false,
        "id": "f391d49a-73b3-40a8-9009-7418c3a1e6dd"
      },
      "source": [
        "This command installs several essential Python libraries that are used for fine-tuning language models, such as the Gemma 2B model. Here's what each package does:\n",
        "\n",
        "1. **`datasets`**:\n",
        "   - This is the Hugging Face `datasets` library, which provides access to a wide variety of datasets in an easy-to-use format. It is commonly used for NLP tasks and comes with built-in functions for loading, preprocessing, and working with large datasets.\n",
        "   \n",
        "2. **`torch`**:\n",
        "   - This installs **PyTorch**, a popular open-source machine learning framework. PyTorch is widely used for deep learning tasks such as building and training neural networks. You'll need it to fine-tune the Gemma 2B model, as the model will be based on PyTorch’s framework.\n",
        "   \n",
        "3. **`peft`**:\n",
        "   - This stands for **Parameter-Efficient Fine-Tuning**, a library used for fine-tuning large models without requiring extensive computational resources. It allows the tuning of only a subset of parameters in the model while freezing the rest, making it useful for scaling models like Gemma 2B.\n",
        "\n",
        "4. **`accelerate`**:\n",
        "   - This is another Hugging Face library that simplifies the process of distributing models across different devices (such as CPUs and GPUs) for faster training. It's particularly helpful when working with large models and datasets, ensuring efficient use of computational resources.\n",
        "   \n",
        "5. **`bitsandbytes`**:\n",
        "   - This is a library that allows for 8-bit optimizers, helping to reduce memory usage when working with large-scale models. By using 8-bit precision for model parameters, it can reduce the resource requirements significantly, which is beneficial for fine-tuning large models.\n",
        "   \n",
        "6. **`trl`**:\n",
        "   - This stands for **Transformer Reinforcement Learning**, a library that integrates reinforcement learning (RL) techniques with transformer models. It allows fine-tuning models using reward signals, which can be useful for tasks like training models to produce higher-quality code or solving specific coding-related problems.\n",
        "\n",
        "In summary, this command sets up the environment by installing the necessary packages for accessing datasets, training models with PyTorch, fine-tuning models efficiently, and managing memory usage effectively. These tools will work together to help you fine-tune the Gemma 2B model in Snowflake Notebooks."
      ]
    },
    {
      "cell_type": "code",
      "id": "f83bebd6-a868-446b-b937-b8b93f9fea2a",
      "metadata": {
        "language": "python",
        "name": "cell4",
        "id": "f83bebd6-a868-446b-b937-b8b93f9fea2a"
      },
      "outputs": [],
      "source": [
        "!pip install datasets torch peft accelerate bitsandbytes trl"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "3775908f-ca36-4846-8f38-5adca39217f2",
      "metadata": {
        "language": "python",
        "name": "cell1",
        "codeCollapsed": false,
        "id": "3775908f-ca36-4846-8f38-5adca39217f2"
      },
      "source": [
        "from datasets import Dataset, ClassLabel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import sys\n",
        "#from utils import Concatenator\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "import torch\n",
        "import sentencepiece\n",
        "import os\n",
        "import json\n",
        "from transformers import TrainerCallback\n",
        "from contextlib import nullcontext\n",
        "from transformers import default_data_collator, Trainer, TrainingArguments\n",
        "\n",
        "from snowflake.snowpark.session import Session\n",
        "from snowflake.snowpark import VERSION\n",
        "import snowflake.snowpark.functions as F\n",
        "from snowflake.ml.registry import model_registry\n",
        "from snowflake.ml.model import deploy_platforms\n",
        "from snowflake.ml.model.models import llm\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
        "logger.setLevel(logging.ERROR)\n",
        "logger = logging.getLogger(\"snowflake.ml\")\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "597ea10a-04aa-4eb4-92a9-24f5bbbde9de",
      "metadata": {
        "name": "cell26",
        "collapsed": false,
        "id": "597ea10a-04aa-4eb4-92a9-24f5bbbde9de"
      },
      "source": [
        "This block of code imports various libraries and sets up the environment for fine-tuning the Gemma 2B model on Snowflake Notebooks. Here's a breakdown of what each part does:\n",
        "\n",
        "1. **`from datasets import Dataset, ClassLabel`**:\n",
        "   - `Dataset`: This is used to create or manipulate datasets from the Hugging Face `datasets` library.\n",
        "   - `ClassLabel`: A special feature type that handles classification labels in a dataset, converting them between integers and human-readable strings.\n",
        "\n",
        "2. **`from transformers import AutoTokenizer, AutoModelForCausalLM`**:\n",
        "   - `AutoTokenizer`: Automatically loads the appropriate tokenizer for a model, which processes text inputs for the model.\n",
        "   - `AutoModelForCausalLM`: Loads a pre-trained model for causal language modeling tasks, which is suitable for code generation and other text-generation tasks.\n",
        "\n",
        "3. **`import sys`**:\n",
        "   - Imports the system module, allowing you to interact with the Python interpreter and manipulate input/output or modify paths.\n",
        "\n",
        "4. **`#from utils import Concatenator`**:\n",
        "   - This line is commented out, indicating the use of a utility called `Concatenator`, which might be a custom tool for concatenating text or data. It's not being used currently.\n",
        "\n",
        "5. **`import pandas as pd`**:\n",
        "   - Imports `pandas`, a powerful data manipulation library commonly used for handling tabular data.\n",
        "   \n",
        "   - `pd.set_option('display.max_colwidth', None)`: This setting ensures that the entire content of any cell in a pandas DataFrame will be displayed without truncation, useful for viewing long text entries, like code.\n",
        "\n",
        "6. **`import torch`**:\n",
        "   - Imports the PyTorch framework, which is needed for running and training the neural network models.\n",
        "\n",
        "7. **`import sentencepiece`**:\n",
        "   - Imports the `sentencepiece` library, which is used for tokenizing text into subwords. This is commonly used with models trained on large corpora to handle rare words more efficiently.\n",
        "\n",
        "8. **`import os`**:\n",
        "   - Imports the operating system module, allowing interaction with the file system (such as reading/writing files or environment variables).\n",
        "\n",
        "9. **`import json`**:\n",
        "   - Imports the `json` module, which is useful for working with JSON data (like reading/writing model configurations or datasets in JSON format).\n",
        "\n",
        "10. **`from transformers import TrainerCallback`**:\n",
        "    - This imports a class from the `transformers` library that allows you to create custom callbacks for the model training process. You can use callbacks to trigger actions at certain points during training.\n",
        "\n",
        "11. **`from contextlib import nullcontext`**:\n",
        "    - `nullcontext`: This is used to indicate a \"no-op\" context manager, essentially allowing you to ignore context management in certain cases where it is not needed.\n",
        "\n",
        "12. **`from transformers import default_data_collator, Trainer, TrainingArguments`**:\n",
        "    - `default_data_collator`: This is used for batching and preparing data before feeding it into the model during training.\n",
        "    - `Trainer`: A high-level API provided by the `transformers` library that handles the training loop, including model optimization, evaluation, and saving.\n",
        "    - `TrainingArguments`: A configuration class for specifying various training options such as learning rate, batch size, number of epochs, etc.\n",
        "\n",
        "13. **`from snowflake.snowpark.session import Session`**:\n",
        "    - `Session`: This is used to establish a connection to Snowflake via Snowpark. Snowpark is a developer environment that allows you to build and execute data pipelines directly in Snowflake using Python.\n",
        "\n",
        "14. **`from snowflake.snowpark import VERSION`**:\n",
        "    - `VERSION`: Retrieves the version of Snowpark that you are using, useful for compatibility checks or logging.\n",
        "\n",
        "15. **`import snowflake.snowpark.functions as F`**:\n",
        "    - This imports functions from Snowpark that allow you to perform SQL-like operations within Snowflake from your Python code. It typically includes functions for data manipulation and querying.\n",
        "\n",
        "16. **`from snowflake.ml.registry import model_registry`**:\n",
        "    - `model_registry`: Used to register and track machine learning models within Snowflake. It enables version control and tracking of models for deployment and reproducibility.\n",
        "\n",
        "17. **`from snowflake.ml.model import deploy_platforms`**:\n",
        "    - `deploy_platforms`: This allows for deploying machine learning models to different platforms within Snowflake for use in production environments.\n",
        "\n",
        "18. **`from snowflake.ml.model.models import llm`**:\n",
        "    - `llm`: Refers to large language models (LLMs) in Snowflake’s ML platform. This is likely related to deploying or managing such models within Snowflake.\n",
        "\n",
        "19. **`import logging`**:\n",
        "    - Imports Python’s logging module, which is used to configure and manage log messages for better tracking and debugging.\n",
        "\n",
        "20. **`logger = logging.getLogger(\"snowflake.snowpark.session\")` & `logger.setLevel(logging.ERROR)`**:\n",
        "    - These lines create a logger specifically for Snowflake's Snowpark session and set its log level to `ERROR`, meaning it will only log error messages (and not warnings or info-level messages).\n",
        "\n",
        "21. **`logger = logging.getLogger(\"snowflake.ml\")` & `logger.setLevel(logging.ERROR)`**:\n",
        "    - Similarly, this creates a logger for Snowflake’s machine learning components and sets it to only log errors.\n",
        "\n",
        "In summary, this script sets up various libraries for handling data, training machine learning models, interacting with Snowflake, and managing logging. The focus is on preparing for the fine-tuning of the Gemma 2B model within the Snowflake environment."
      ]
    },
    {
      "cell_type": "code",
      "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
      "metadata": {
        "language": "python",
        "name": "cell2",
        "codeCollapsed": false,
        "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9"
      },
      "source": [
        "!huggingface-cli login --token \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "179251f4-6dd0-4317-8c5b-9cb59e0c10dc",
      "metadata": {
        "language": "python",
        "name": "cell6",
        "codeCollapsed": false,
        "id": "179251f4-6dd0-4317-8c5b-9cb59e0c10dc"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "8d4f9272-3541-4c38-8306-dbf0233fa2d1",
      "metadata": {
        "name": "cell27",
        "collapsed": false,
        "id": "8d4f9272-3541-4c38-8306-dbf0233fa2d1"
      },
      "source": [
        "This code snippet involves configuring the model to use 4-bit quantization for efficient memory usage and computation during fine-tuning. Here’s a detailed explanation:\n",
        "\n",
        "### **`from transformers import BitsAndBytesConfig`**\n",
        "- This imports the `BitsAndBytesConfig` class from the `transformers` library. This class is used to configure how the model loads using reduced precision, such as 4-bit quantization, which helps save memory and computational resources.\n",
        "\n",
        "### **`bnb_config = BitsAndBytesConfig(...)`**\n",
        "- This creates a `BitsAndBytesConfig` object called `bnb_config`. The object is configured with specific parameters for loading the model in 4-bit precision.\n",
        "\n",
        "### Configuration options:\n",
        "\n",
        "1. **`load_in_4bit=True`**:\n",
        "   - This tells the model to load using 4-bit precision instead of the standard 16-bit or 32-bit precision. By reducing the precision, it saves a significant amount of memory, making it possible to fine-tune larger models on hardware with limited memory.\n",
        "\n",
        "2. **`bnb_4bit_quant_type=\"nf4\"`**:\n",
        "   - This sets the quantization type to **NF4** (Normal Float 4). NF4 is a type of quantization that is more efficient for machine learning tasks because it improves precision in certain operations compared to regular 4-bit quantization.\n",
        "\n",
        "3. **`bnb_4bit_compute_dtype=torch.bfloat16`**:\n",
        "   - This specifies the data type used for computations as **bfloat16** (Brain Floating Point 16). Bfloat16 is a precision format commonly used in machine learning because it strikes a balance between speed and accuracy, especially when running computations on GPUs.\n",
        "   \n",
        "4. **`bnb_4bit_use_double_quant=True`**:\n",
        "   - **Double quantization** is a technique where the model is quantized twice to further compress the model size. It first quantizes the weights and then quantizes the quantized values. This reduces memory usage even more but requires some extra compute during inference.\n",
        "\n",
        "### Purpose:\n",
        "- The goal of this configuration is to reduce the memory footprint and optimize the model for faster inference and training, which is particularly useful when working with large language models like Gemma 2B."
      ]
    },
    {
      "cell_type": "code",
      "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
      "metadata": {
        "language": "python",
        "name": "cell3",
        "codeCollapsed": false,
        "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81"
      },
      "source": [
        "from transformers import GemmaTokenizer, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}\n",
        "model_id = \"google/gemma-2b\"\n",
        "print('loading tokenizer')\n",
        "tokenizer_id = \"philschmid/gemma-tokenizer-chatml\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
        "print('loading model')\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=device_map)\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "80cd1377-be2b-44b9-bfa5-44874fed9b19",
      "metadata": {
        "name": "cell28",
        "collapsed": false,
        "id": "80cd1377-be2b-44b9-bfa5-44874fed9b19"
      },
      "source": [
        "Here’s a breakdown of what each part of the code does:\n",
        "\n",
        "1. **`from transformers import GemmaTokenizer, AutoModelForCausalLM, AutoTokenizer`**:\n",
        "   - Imports the necessary components for tokenizing and loading the model.\n",
        "\n",
        "2. **`device_map = {\"\": 0}`**:\n",
        "   - Specifies that the entire model will be loaded onto GPU 0 for efficient processing.\n",
        "\n",
        "3. **`model_id = \"google/gemma-2b\"`**:\n",
        "   - Sets the model identifier to load the pre-trained **Gemma 2B** model from Google.\n",
        "\n",
        "4. **`tokenizer_id = \"philschmid/gemma-tokenizer-chatml\"`**:\n",
        "   - Sets the tokenizer identifier to load the pre-trained tokenizer compatible with the model.\n",
        "\n",
        "5. **`tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)`**:\n",
        "   - Loads the tokenizer from the specified pre-trained model (`philschmid/gemma-tokenizer-chatml`).\n",
        "\n",
        "6. **`model = AutoModelForCausalLM.from_pretrained(...)`**:\n",
        "   - Loads the **Gemma 2B** model for causal language modeling, with 4-bit quantization (`bnb_config`) and the model being loaded onto GPU 0.\n",
        "\n",
        "7. **`model.config.use_cache = False`**:\n",
        "   - Disables caching during training to reduce memory usage.\n",
        "\n",
        "8. **`model.config.pretraining_tp = 1`**:\n",
        "   - Sets the tensor parallelism factor to 1, meaning no tensor parallelism is used during model pre-training."
      ]
    },
    {
      "cell_type": "code",
      "id": "2387dc83-4bd3-4edb-806d-e7602939005e",
      "metadata": {
        "language": "python",
        "name": "cell5",
        "codeCollapsed": false,
        "id": "2387dc83-4bd3-4edb-806d-e7602939005e"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = \"lucasmccabe-lmi/CodeAlpaca-20k\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "1efba111-72f1-422e-a55f-bd7328bf9058",
      "metadata": {
        "language": "python",
        "name": "cell7",
        "codeCollapsed": false,
        "id": "1efba111-72f1-422e-a55f-bd7328bf9058"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "c890ff5f-065f-42b6-9597-41c0a774ccde",
      "metadata": {
        "language": "python",
        "name": "cell13",
        "codeCollapsed": false,
        "id": "c890ff5f-065f-42b6-9597-41c0a774ccde"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Conv1D\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "def find_all_linear_names(model):\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, bnb.nn.Linear4bit):\n",
        "            names = name.split(\".\")\n",
        "            # model-specific\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove(\"lm_head\")\n",
        "    return list(lora_module_names)\n",
        "\n",
        "target = find_all_linear_names(model)\n",
        "print(target)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "af2ffa6a-3376-4f56-9937-9e147a14aa01",
      "metadata": {
        "name": "cell29",
        "collapsed": false,
        "id": "af2ffa6a-3376-4f56-9937-9e147a14aa01"
      },
      "source": [
        "Here’s an explanation of what this code does:\n",
        "\n",
        "### **`import torch`** and **`from transformers import Conv1D`**:\n",
        "- These import PyTorch and the 1D convolutional layer (`Conv1D`) from the `transformers` library, though they are not directly used in this snippet.\n",
        "\n",
        "### **`import bitsandbytes as bnb`**:\n",
        "- This imports the `bitsandbytes` library (as `bnb`), which is used for handling 4-bit quantization and memory-efficient operations.\n",
        "\n",
        "### **`find_all_linear_names(model)` function**:\n",
        "This function finds the names of all layers in the model that are of type `bnb.nn.Linear4bit` (quantized linear layers).\n",
        "\n",
        "- **`lora_module_names = set()`**:\n",
        "   - Initializes an empty set to store the names of layers.\n",
        "\n",
        "- **`for name, module in model.named_modules()`**:\n",
        "   - Iterates over all the modules (layers) in the model. For each module, it checks if it is an instance of `bnb.nn.Linear4bit`.\n",
        "\n",
        "- **`if isinstance(module, bnb.nn.Linear4bit)`**:\n",
        "   - If the current module is a 4-bit quantized linear layer (`Linear4bit`), the name of the module is processed.\n",
        "\n",
        "- **`names = name.split(\".\")`**:\n",
        "   - Splits the module name by dots (`.`) to handle hierarchical naming (e.g., `layer.0.linear`).\n",
        "\n",
        "- **`lora_module_names.add(names[0] if len(names) == 1 else names[-1])`**:\n",
        "   - Adds either the first or last part of the split name to the `lora_module_names` set, depending on its length.\n",
        "\n",
        "- **`if \"lm_head\" in lora_module_names:`**:\n",
        "   - If `\"lm_head\"` (the output layer of the model) is in the set, it is removed because it's not needed for the fine-tuning process in some contexts (like 16-bit training).\n",
        "\n",
        "- **`return list(lora_module_names)`**:\n",
        "   - Returns the names of the layers that are quantized 4-bit linear layers as a list.\n",
        "\n",
        "### **`target = find_all_linear_names(model)`**:\n",
        "- Calls the function on the `model` (which was defined earlier) to find all relevant linear layer names.\n",
        "\n",
        "### **`print(target)`**:\n",
        "- Prints the list of identified module names that are 4-bit linear layers.\n",
        "\n",
        "This function helps identify specific layers in the model that use 4-bit quantization, which could be useful for further customization, such as applying parameter-efficient fine-tuning techniques (like LoRA)."
      ]
    },
    {
      "cell_type": "code",
      "id": "3c41d262-d894-4ed2-adc4-a1f0e9edd4f3",
      "metadata": {
        "language": "python",
        "name": "cell8",
        "codeCollapsed": false,
        "id": "3c41d262-d894-4ed2-adc4-a1f0e9edd4f3"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        r=64,\n",
        "        bias=\"none\",\n",
        "        target_modules=target,\n",
        "        task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"gemma-2b-coder\", # directory to save and repository id\n",
        "    num_train_epochs=1,                     # number of training epochs\n",
        "    per_device_train_batch_size=1,          # batch size per device during training\n",
        "    gradient_accumulation_steps=1,          # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
        "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
        "    logging_steps=100,                       # log every 10 steps\n",
        "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
        "    bf16=True,                              # use bfloat16 precision\n",
        "    tf32=True,                              # use tf32 precision\n",
        "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
        "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
        "    push_to_hub=False,                       # push model to hub\n",
        "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
        ")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "5fc9de84-d8a4-4625-89ee-f482b0e0bafe",
      "metadata": {
        "name": "cell30",
        "collapsed": false,
        "id": "5fc9de84-d8a4-4625-89ee-f482b0e0bafe"
      },
      "source": [
        "Here’s a breakdown of what this code does:\n",
        "\n",
        "### **`from peft import LoraConfig, PeftModel`**\n",
        "- Imports the **LoRA (Low-Rank Adaptation)** configuration class `LoraConfig` and `PeftModel` for parameter-efficient fine-tuning, allowing selective fine-tuning of large models.\n",
        "\n",
        "### **`from trl import SFTTrainer`**\n",
        "- Imports `SFTTrainer`, which is used for **supervised fine-tuning (SFT)** of transformer models, often in RL (Reinforcement Learning) setups.\n",
        "\n",
        "### **Loading LoRA Configuration (`LoraConfig`)**:\n",
        "The LoRA configuration specifies how fine-tuning will be applied to the model using low-rank adaptation.\n",
        "\n",
        "- **`lora_alpha=16`**: This is a scaling factor for the LoRA layers. It controls the amount by which the updated weights should be scaled.\n",
        "  \n",
        "- **`lora_dropout=0.05`**: Specifies a dropout rate of 5%, helping to regularize the training and prevent overfitting.\n",
        "\n",
        "- **`r=64`**: Defines the rank of the low-rank matrices used in LoRA. A higher rank means more trainable parameters but requires more resources.\n",
        "\n",
        "- **`bias=\"none\"`**: Indicates that no additional bias parameters will be updated during fine-tuning.\n",
        "\n",
        "- **`target_modules=target`**: Sets the specific modules that will be targeted for LoRA-based fine-tuning, which were identified earlier with `find_all_linear_names()`.\n",
        "\n",
        "- **`task_type=\"CAUSAL_LM\"`**: Specifies that the task is causal language modeling, which is suitable for models like Gemma 2B, which generate text based on prior context.\n",
        "\n",
        "### **Training Arguments (`TrainingArguments`)**:\n",
        "The `TrainingArguments` object configures the model training process with various parameters.\n",
        "\n",
        "- **`output_dir=\"gemma-2b-coder\"`**: Specifies the directory where the model checkpoints and other outputs will be saved.\n",
        "\n",
        "- **`num_train_epochs=1`**: Sets the number of training epochs to 1. The model will go through the entire dataset once.\n",
        "\n",
        "- **`per_device_train_batch_size=1`**: Specifies that each training batch will contain just 1 sample per device (such as a GPU).\n",
        "\n",
        "- **`gradient_accumulation_steps=1`**: Gradients are accumulated for 1 step before a backward pass, helping manage memory with small batch sizes.\n",
        "\n",
        "- **`gradient_checkpointing=True`**: Saves memory during training by storing only essential gradients, useful for training large models.\n",
        "\n",
        "- **`optim=\"adamw_torch_fused\"`**: Uses the AdamW optimizer with fused kernels for faster training on modern GPUs.\n",
        "\n",
        "- **`logging_steps=100`**: Logs training progress every 100 steps, providing insights into the training metrics.\n",
        "\n",
        "- **`save_strategy=\"epoch\"`**: Saves model checkpoints at the end of every epoch.\n",
        "\n",
        "- **`bf16=True`**: Enables bfloat16 (Brain Floating Point 16) precision for faster computation and reduced memory usage on compatible hardware.\n",
        "\n",
        "- **`tf32=True`**: Enables TensorFloat32 precision, which improves speed on certain operations while maintaining accuracy on modern NVIDIA GPUs.\n",
        "\n",
        "- **`learning_rate=2e-4`**: Sets the learning rate for the optimizer to `2e-4`, based on recommendations from the QLoRA paper.\n",
        "\n",
        "- **`max_grad_norm=0.3`**: Clips the gradients to a maximum value of 0.3, also based on QLoRA’s fine-tuning practices.\n",
        "\n",
        "- **`warmup_ratio=0.03`**: Uses 3% of the training steps for learning rate warmup, slowly increasing the learning rate at the start of training.\n",
        "\n",
        "- **`lr_scheduler_type=\"constant\"`**: Uses a constant learning rate throughout training, meaning the learning rate will not change after the warmup.\n",
        "\n",
        "- **`push_to_hub=False`**: This indicates that the model won’t be pushed to Hugging Face’s Model Hub.\n",
        "\n",
        "- **`report_to=\"tensorboard\"`**: Reports training metrics to TensorBoard for real-time visualization and tracking.\n",
        "\n",
        "### Purpose:\n",
        "This configuration prepares the model for fine-tuning using LoRA (for parameter-efficient adaptation) with careful management of memory and computation. The training will be logged to TensorBoard, using both bfloat16 and tf32 precision for optimal performance on GPUs."
      ]
    },
    {
      "cell_type": "code",
      "id": "beba48c4-44a6-4c75-9f5d-6243cbb677f2",
      "metadata": {
        "language": "python",
        "name": "cell10",
        "codeCollapsed": false,
        "id": "beba48c4-44a6-4c75-9f5d-6243cbb677f2"
      },
      "outputs": [],
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "\n",
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['instruction'])):\n",
        "        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "response_template = \"\\n ### Answer:\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "d66475ae-7414-4844-9511-100d26fbce38",
      "metadata": {
        "language": "python",
        "name": "cell11",
        "codeCollapsed": false,
        "id": "d66475ae-7414-4844-9511-100d26fbce38"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "max_seq_length = 1512 # max sequence length for model and packing of the dataset\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    data_collator=collator,\n",
        "    args=args,\n",
        "    packing=False,\n",
        ")\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "1be13e53-f5d4-4cda-abfb-a4c8d4e77ea0",
      "metadata": {
        "name": "cell31",
        "collapsed": false,
        "id": "1be13e53-f5d4-4cda-abfb-a4c8d4e77ea0"
      },
      "source": [
        "Here’s an explanation of what this code does:\n",
        "\n",
        "### **`from trl import SFTTrainer`**\n",
        "- This imports the `SFTTrainer` class, which is used for supervised fine-tuning of transformer models. This class handles the training loop, optimization, and logging.\n",
        "\n",
        "### **`max_seq_length = 1512`**\n",
        "- Sets the maximum sequence length for the input data to **1512 tokens**. This means that any input sequences longer than this will be truncated, and shorter sequences will be padded to fit this length. This parameter is crucial for ensuring the model can handle the input data correctly and for managing memory usage.\n",
        "\n",
        "### **Creating the Trainer (`SFTTrainer`)**:\n",
        "The `trainer` object is instantiated with various parameters needed for training.\n",
        "\n",
        "- **`model=model`**:\n",
        "  - Passes the model (previously loaded and configured) that will be fine-tuned.\n",
        "\n",
        "- **`train_dataset=dataset`**:\n",
        "  - Specifies the dataset to be used for training. The `dataset` variable should contain the processed data suitable for the model.\n",
        "\n",
        "- **`peft_config=peft_config`**:\n",
        "  - Passes the LoRA configuration (`peft_config`) defined earlier, which tells the trainer how to apply low-rank adaptation during training.\n",
        "\n",
        "- **`formatting_func=formatting_prompts_func`**:\n",
        "  - Specifies a formatting function (`formatting_prompts_func`) that is likely used to preprocess the input data or format prompts before feeding them into the model. This function customizes how input examples are structured.\n",
        "\n",
        "- **`data_collator=collator`**:\n",
        "  - Passes a data collator function (`collator`) responsible for combining individual data samples into batches. This is essential for proper batching during training.\n",
        "\n",
        "- **`args=args`**:\n",
        "  - Provides the training arguments (`args`), which were defined earlier, to configure various aspects of the training process (e.g., batch size, learning rate, logging frequency).\n",
        "\n",
        "- **`packing=False`**:\n",
        "  - Indicates that data packing is turned off. Packing typically involves grouping smaller sequences together into a larger batch to utilize the maximum sequence length efficiently. Setting this to `False` means that each sequence will be processed individually without packing.\n",
        "\n",
        "### Purpose:\n",
        "This code sets up the **SFTTrainer** with the specified model, dataset, and configurations, preparing it for the supervised fine-tuning process. The `max_seq_length` helps ensure that the input data fits within the model's capabilities, while the other parameters customize the training process according to the needs of the task and the architecture being used."
      ]
    },
    {
      "cell_type": "code",
      "id": "9f2dcb15-563e-4602-a281-61ae8417ad22",
      "metadata": {
        "language": "python",
        "name": "cell19",
        "codeCollapsed": false,
        "id": "9f2dcb15-563e-4602-a281-61ae8417ad22"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# start training, the model will be automatically saved to the hub and the output directory\n",
        "trainer.train()\n",
        "\n",
        "# save model\n",
        "trainer.save_model()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "2fd955c9-5b72-4867-9bec-09225c6b2749",
      "metadata": {
        "name": "cell32",
        "collapsed": false,
        "id": "2fd955c9-5b72-4867-9bec-09225c6b2749"
      },
      "source": [
        "Here’s an explanation of what this code does:\n",
        "\n",
        "### **`with torch.no_grad():`**\n",
        "- This statement creates a context manager that disables gradient calculation. This is important during inference or evaluation, as it reduces memory consumption and speeds up computations by preventing PyTorch from tracking operations for gradient computation. This is especially useful when you want to free up memory and reduce overhead.\n",
        "\n",
        "### **`torch.cuda.empty_cache()`**\n",
        "- This command clears the unused memory cached by the CUDA allocator. PyTorch uses a caching allocator to manage GPU memory, which can lead to fragmentation over time. By calling `empty_cache()`, you ensure that any memory that is no longer being used is released back to the system, potentially allowing for more efficient memory usage during the training process.\n",
        "\n",
        "### **`trainer.train()`**\n",
        "- This line starts the training process using the previously configured `trainer` object. During this step, the model will:\n",
        "  - Process the training dataset.\n",
        "  - Apply the defined fine-tuning techniques (including LoRA).\n",
        "  - Log metrics according to the specified logging strategy.\n",
        "  - Automatically save model checkpoints to the output directory specified in the `TrainingArguments`.\n",
        "\n",
        "### **`trainer.save_model()`**\n",
        "- After training is complete, this line saves the final model weights and configuration. The model will be saved in the specified output directory (`\"gemma-2b-coder\"`), allowing for later use or deployment. Depending on the setup, this may also push the model to a model hub if configured to do so.\n",
        "\n",
        "### Purpose:\n",
        "Overall, this code snippet efficiently manages GPU memory during training and initiates the training process with the specified parameters. After training is complete, it ensures that the trained model is saved for future use."
      ]
    },
    {
      "cell_type": "code",
      "id": "4aadaf58-0964-46b7-9b5a-fc81b9a4bd41",
      "metadata": {
        "language": "python",
        "name": "cell12",
        "codeCollapsed": false,
        "id": "4aadaf58-0964-46b7-9b5a-fc81b9a4bd41"
      },
      "outputs": [],
      "source": [
        "# Save trained model\n",
        "new_model = \"FinetunedModel\"\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "f32b1077-b33e-4be6-a5c7-3c7519a27993",
      "metadata": {
        "language": "python",
        "name": "cell9",
        "id": "f32b1077-b33e-4be6-a5c7-3c7519a27993"
      },
      "outputs": [],
      "source": [
        "!ls"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "8a372dde-87fb-4c2c-99a9-42728aec610c",
      "metadata": {
        "language": "python",
        "name": "cell14",
        "codeCollapsed": false,
        "id": "8a372dde-87fb-4c2c-99a9-42728aec610c"
      },
      "outputs": [],
      "source": [
        "SNOWFLAKE_DATABASE  = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
        "SNOWFLAKE_SCHEMA    = os.getenv(\"SNOWFLAKE_SCHEMA\")\n",
        "SNOWFLAKE_WAREHOUSE = 'CONTAINER_RUNTIME_WH'\n",
        "MODEL_NAME = \"FineTunedGemma\"\n",
        "MODEL_VERSION = \"FineTunedV1\"\n",
        "DEPLOYMENT_NAME = \"FINETUNED_Gemma\""
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "b6d69973-0aa2-4a21-9d37-68e283c91435",
      "metadata": {
        "language": "python",
        "name": "cell16",
        "codeCollapsed": false,
        "id": "b6d69973-0aa2-4a21-9d37-68e283c91435"
      },
      "outputs": [],
      "source": [
        "from snowflake.snowpark.session import Session"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "d3e44a5a-e780-4351-8af3-d920c0c6653c",
      "metadata": {
        "language": "python",
        "name": "cell15",
        "codeCollapsed": false,
        "id": "d3e44a5a-e780-4351-8af3-d920c0c6653c"
      },
      "outputs": [],
      "source": [
        "# Read the login token supplied automatically by Snowflake. These tokens are short lived and should always be read right before creating any new connection.\n",
        "def get_login_token():\n",
        "  with open(\"/snowflake/session/token\", \"r\") as f:\n",
        "    return f.read()\n",
        "\n",
        "# Construct Snowflake connection params from environment variables.\n",
        "def get_connection_params():\n",
        "  return {\n",
        "    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "    \"host\": os.getenv(\"SNOWFLAKE_HOST\"),\n",
        "    \"warehouse\": SNOWFLAKE_WAREHOUSE,\n",
        "    \"database\": SNOWFLAKE_DATABASE,\n",
        "    \"schema\": SNOWFLAKE_SCHEMA,\n",
        "    \"authenticator\": \"oauth\",\n",
        "    \"token\": get_login_token()\n",
        "  }\n",
        "\n",
        "# Create Snowflake Session object\n",
        "session = Session.builder.configs(get_connection_params()).create()\n",
        "session.sql_simplifier_enabled = True\n",
        "snowpark_version = VERSION\n",
        "\n",
        "# Current Environment Details\n",
        "print('Role                        : {}'.format(session.get_current_role()))\n",
        "print('Database                    : {}'.format(session.get_current_database()))\n",
        "print('Schema                      : {}'.format(session.get_current_schema()))\n",
        "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
        "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "30624a31-749b-4027-877e-4b6cd3e8bfd0",
      "metadata": {
        "language": "python",
        "name": "cell21",
        "id": "30624a31-749b-4027-877e-4b6cd3e8bfd0"
      },
      "outputs": [],
      "source": [
        "SNOWFLAKE_SCHEMA"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "f2a2710c-a1a5-4c01-99b2-1a2bdf7df44f",
      "metadata": {
        "language": "python",
        "name": "cell22",
        "codeCollapsed": false,
        "id": "f2a2710c-a1a5-4c01-99b2-1a2bdf7df44f"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=\"FinetunedModel\", tokenizer=tokenizer, max_length=200)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "e1053cfb-00ec-4934-9e1c-5721110c37eb",
      "metadata": {
        "language": "python",
        "name": "cell18",
        "codeCollapsed": false,
        "id": "e1053cfb-00ec-4934-9e1c-5721110c37eb"
      },
      "outputs": [],
      "source": [
        "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]\n",
        "\n",
        "def test_inference(prompt):\n",
        "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
        "    outputs = pipe(prompt, max_new_tokens=100, do_sample=True, temperature=0.1, top_k=50, top_p=0.95, eos_token_id=eos_token)\n",
        "    return outputs[0]['generated_text'][len(prompt):].strip()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "32ce70a7-3aa5-4f8e-8844-d819edd70972",
      "metadata": {
        "language": "python",
        "name": "cell24",
        "codeCollapsed": false,
        "id": "32ce70a7-3aa5-4f8e-8844-d819edd70972"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "\n",
        "st.title(\"CodingAssistant\")\n",
        "\n",
        "question = st.text_input(\"Enter Question\", label_visibility=\"collapsed\")\n",
        "\n",
        "\n",
        "if question:\n",
        "    st.markdown(test_inference(question))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "b4990b73-09ca-421f-870c-6505ef87c153",
      "metadata": {
        "language": "python",
        "name": "cell25",
        "codeCollapsed": false,
        "id": "b4990b73-09ca-421f-870c-6505ef87c153"
      },
      "outputs": [],
      "source": [],
      "execution_count": null
    }
  ]
}