{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc8479c-f1b6-4b37-8690-1f634ef01679",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell8"
   },
   "source": [
    "# RAG Chatbot for KubeCon Sessions\n",
    "\n",
    "This guide walks through building a Retrieval-Augmented Generation (RAG) chatbot using Snowflake Cortex and Streamlit for KubeCon session data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2734512-27e4-45d5-b335-6b57c18b63b7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell10"
   },
   "source": [
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before proceeding, ensure you have:\n",
    "- A Snowflake account with access to Cortex.\n",
    "- Required permissions to create tables and search services.\n",
    "- Python environment with `streamlit`, `snowflake-core`, and `snowflake-snowpark`.\n",
    "- Download and save the PDF file for KubeCon Schedule: [View the KCCNCEU 2025 Schedule](https://kccnceu2025.sched.com/print?iframe=yes&w=100%&sidebar=yes&bg=no) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5fd333-eeb6-48b0-b7cc-031df1ddff89",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell11"
   },
   "source": [
    "## Step 1: Staging and Listing Available Files in Snowflake:\n",
    "\n",
    "To create a named internal stage using Snowsight, follow these steps:  \n",
    "\n",
    "1. **Sign in to Snowsight.**  \n",
    "2. In the navigation menu, select **Create ¬ª Stage ¬ª Snowflake Managed**.  \n",
    "3. In the **Create Stage** dialog, enter a **Stage Name**.  \n",
    "4. Select the **database and schema** where you want to create the stage.  \n",
    "5. Optionally, **deselect Directory table**.  \n",
    "   - Directory tables allow you to see files on the stage but require a warehouse, which incurs a cost.  \n",
    "   - You can choose to deselect this option now and enable a directory table later.  \n",
    "6. Select the type of **Encryption** supported for all files on your stage.  \n",
    "   - For details, see [Encryption for Internal Stages](#).  \n",
    "   - **Note:** You cannot change the encryption type after creating the stage. \n",
    "\n",
    "To upload files onto your stage, follow these steps:  \n",
    "\n",
    "1. **Sign in to Snowsight.**  \n",
    "2. Select **Data ¬ª Add Data**.  \n",
    "3. On the **Add Data** page, select **Load files into a Stage**.  \n",
    "4. In the **Upload Your Files** dialog, select the files you want to upload.  \n",
    "   - You can upload multiple files at the same time.  \n",
    "5. Select the **database schema** where you created the stage, then select the **stage**.  \n",
    "6. Optionally, select or create a **path** where you want to save your files within the stage.  \n",
    "7. Click **Upload**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b575d-341f-48c6-95d0-0ec6bee89c78",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "--list the staged file(s)\n",
    "ls @FAWAZG_SCHEMA.KUBECON;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2e529-b08b-4b98-a91a-6855e670c46c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell12"
   },
   "source": [
    "# Step 2: Parsing KubeCon Session Document\n",
    "\n",
    "The `PARSE_DOCUMENT` function extracts text, data, and layout elements from documents. It can be used for:\n",
    "\n",
    "1. Powering **RAG pipelines** for Cortex Search.\n",
    "2. Enabling **LLM processing** like document summarization or translation using Cortex AI Functions.\n",
    "3. Performing **zero-shot entity extraction** with Cortex AI Structured Outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170f929-6275-4055-b025-6c760ee7e109",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE FAWAZG_DB.FAWAZG_SCHEMA.KUBECON_PARSED_CONTENT AS SELECT \n",
    "      relative_path,\n",
    "      TO_VARCHAR(\n",
    "        SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n",
    "          @FAWAZG_SCHEMA.KUBECON, \n",
    "          relative_path, \n",
    "          {'mode': 'LAYOUT'}\n",
    "        ) :content\n",
    "      ) AS parsed_text\n",
    "    FROM directory(@FAWAZG_SCHEMA.KUBECON)\n",
    "    WHERE relative_path LIKE '%.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be400d91-e883-42d7-b26e-eb78a7219479",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "-- check the results of results Step 2: Parsing KubeCon Session Document\n",
    "SELECT * FROM FAWAZG_DB.FAWAZG_SCHEMA.KUBECON_PARSED_CONTENT LIMIT 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63055906-7df0-405f-8ee9-fbf05b8a20ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell13"
   },
   "source": [
    "# Step 3: Chunking the Parsed Content\n",
    "\n",
    "The `SPLIT_TEXT_RECURSIVE_CHARACTER` function splits text into smaller chunks for text embedding or search indexing. It works as follows:\n",
    "\n",
    "- Splits text based on separators (default or custom).\n",
    "- Recursively splits chunks longer than the specified `chunk_size`.\n",
    "- Example: With `format='none'`, it first splits on `\\n\\n` (paragraphs), then `\\n` (line breaks), repeating until all chunks are under the `chunk_size`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004e517-c9d3-4217-b091-74fc5750865f",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE FAWAZG_DB.FAWAZG_SCHEMA.KUBECON_CHUNKED_CONTENT (\n",
    "    file_name VARCHAR,\n",
    "    CHUNK VARCHAR\n",
    ");\n",
    "\n",
    "INSERT INTO FAWAZG_DB.FAWAZG_SCHEMA.KUBECON_CHUNKED_CONTENT (file_name, CHUNK)\n",
    "SELECT\n",
    "    relative_path,\n",
    "    c.value AS CHUNK\n",
    "FROM\n",
    "    FAWAZG_DB.FAWAZG_SCHEMA.KUBECON_PARSED_CONTENT,\n",
    "    LATERAL FLATTEN( input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n",
    "        parsed_text,\n",
    "        'markdown',\n",
    "        300,\n",
    "        250\n",
    "    )) c;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3fd5b-5002-4b46-b0d7-232327da5fc4",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "-- check the resuls of Step 3: Chunking the Parsed Content\n",
    "SELECT * FROM FAWAZG_DB.FAWAZG_SCHEMA.KUBECON_CHUNKED_CONTENT LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60365f-b73b-4bed-8c00-bcd3987d7570",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell14"
   },
   "source": [
    "# Step 4: Creating a Search Service in Snowflake Cortex\n",
    "This command triggers the creation of the search service for your data with the following behavior:\n",
    "\n",
    "- **Queries** will search for matches in the `transcript_text` column.\n",
    "- **TARGET_LAG** sets the search service to check for updates to `support_transcripts` approximately once per day.\n",
    "- The **warehouse** `cortex_search_wh` will be used to materialize query results initially and when the base table updates.\n",
    "\n",
    "![Cortex Search RAG](https://docs.snowflake.com/en/_images/cortex-search-rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72983c92-cd1f-4b6d-bc06-c65b038f3999",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE CORTEX SEARCH SERVICE FAWAZG_DB.FAWAZG_SCHEMA.KUBECON_SEARCH_SERVICE\n",
    "    ON chunk\n",
    "    WAREHOUSE = fawazg_wh\n",
    "    TARGET_LAG = '1 minute'\n",
    "    EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n",
    "    AS (\n",
    "    SELECT\n",
    "        file_name,\n",
    "        chunk\n",
    "    FROM FAWAZG_DB.FAWAZG_SCHEMA.KUBECON_CHUNKED_CONTENT\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ea569-dba2-402e-a727-ecb28d10e789",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "-- Query Step 4 with SQL\n",
    "SELECT PARSE_JSON(\n",
    "  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n",
    "      'FAWAZG_DB.FAWAZG_SCHEMA.KUBECON_SEARCH_SERVICE',\n",
    "      '{\n",
    "         \"query\": \"Any talks about Snowflake?\",\n",
    "         \"columns\":[\n",
    "            \"file_name\",\n",
    "            \"CHUNK\"\n",
    "         ],\n",
    "         \"limit\":1\n",
    "      }'\n",
    "  )\n",
    ")['results'] as results;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c2360-691b-4456-9303-ae1130e15a6a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell15"
   },
   "source": [
    "# Step 5: Building the KubeCon Chatbot with Streamlit\n",
    "\n",
    "1. **Imports and Setup**  \n",
    "   Imports necessary libraries: `streamlit` for UI, `Root` and `get_active_session` for Snowflake interaction.\n",
    "\n",
    "2. **Initialize Chatbot and Service Metadata**  \n",
    "   Fetches Cortex Search service metadata and initializes conversation state. Provides options to clear chat history or use it in the conversation.\n",
    "\n",
    "3. **Query the Search Service**  \n",
    "   Executes a search query on the selected Cortex Search service and retrieves relevant context documents for the chatbot.\n",
    "\n",
    "4. **Create and Process Prompts**  \n",
    "   Constructs prompts by combining chat history, search context, and the user‚Äôs question. Sends this prompt to the Snowflake model (`cortex.complete`) for response generation.\n",
    "\n",
    "5. **Main Function and Chat Interaction**  \n",
    "   Displays chat history, handles user input, and processes queries. Uses the generated response from the model to continue the conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ec9ac-6242-488d-bdd0-9c51e81004da",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from snowflake.core import Root # requires snowflake>=0.8.0\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "## Initialize Chatbot\n",
    "\n",
    "def init_chatbot():\n",
    "    if \"service_metadata\" not in st.session_state:\n",
    "        services = session.sql(\"SHOW CORTEX SEARCH SERVICES;\").collect()\n",
    "        service_metadata = []\n",
    "        if services:\n",
    "            for s in services:\n",
    "                svc_name = s[\"name\"]\n",
    "                svc_search_col = session.sql(\n",
    "                    f\"DESC CORTEX SEARCH SERVICE {svc_name};\"\n",
    "                ).collect()[0][\"search_column\"]\n",
    "                service_metadata.append(\n",
    "                    {\"name\": svc_name, \"search_column\": svc_search_col}\n",
    "                )\n",
    "\n",
    "        st.session_state.service_metadata = service_metadata\n",
    "\n",
    "\n",
    "    st.sidebar.button(\"Clear conversation\", key=\"clear_conversation\")\n",
    "    st.sidebar.toggle(\"Use chat history\", key=\"use_chat_history\", value=True)\n",
    "\n",
    "    \n",
    "    if st.session_state.clear_conversation or \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "## Query the Search Service\n",
    "def query_cortex_search_service(query):\n",
    "    db, schema = session.get_current_database(), session.get_current_schema()\n",
    "\n",
    "    cortex_search_service = (\n",
    "        root.databases[db]\n",
    "        .schemas[schema]\n",
    "        .cortex_search_services[st.session_state.selected_cortex_search_service]\n",
    "    )\n",
    "\n",
    "    context_documents = cortex_search_service.search(\n",
    "        query, columns=[], limit=st.session_state.num_retrieved_chunks\n",
    "    )\n",
    "    results = context_documents.results\n",
    "\n",
    "    service_metadata = st.session_state.service_metadata\n",
    "    search_col = [s[\"search_column\"] for s in service_metadata\n",
    "                    if s[\"name\"] == st.session_state.selected_cortex_search_service][0]\n",
    "\n",
    "    context_str = \"\"\n",
    "    for i, r in enumerate(results):\n",
    "        context_str += f\"Context document {i+1}: {r[search_col]} \\n\" + \"\\n\"\n",
    "\n",
    "   \n",
    "    return context_str\n",
    "    \n",
    "## Get the chat history\n",
    "def get_chat_history():\n",
    "    start_index = max(\n",
    "        0, len(st.session_state.messages) - st.session_state.num_chat_messages\n",
    "    )\n",
    "    return st.session_state.messages[start_index : len(st.session_state.messages) - 1]\n",
    "\n",
    "def complete(model, prompt):\n",
    "    return session.sql(\"SELECT snowflake.cortex.complete(?,?)\", (model, prompt)).collect()[0][0]\n",
    "\n",
    "def make_chat_history_summary(chat_history, question):\n",
    "    prompt = f\"\"\"\n",
    "        [INST]\n",
    "        Based on the chat history below and the question, generate a query that extend the question\n",
    "        with the chat history provided. The query should be in natural language.\n",
    "        Answer with only the query. Do not add any explanation.\n",
    "\n",
    "        <chat_history>\n",
    "        {chat_history}\n",
    "        </chat_history>\n",
    "        <question>\n",
    "        {question}\n",
    "        </question>\n",
    "        [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    summary = complete(st.session_state.model_name, prompt)\n",
    "\n",
    "   \n",
    "\n",
    "    return summary\n",
    "\n",
    "def create_prompt(user_question):\n",
    "    \"\"\"\n",
    "    Create a prompt for the language model by combining the user question with context retrieved\n",
    "    from the cortex search service and chat history (if enabled). Format the prompt according to\n",
    "    the expected input format of the model.\n",
    "\n",
    "    Args:\n",
    "        user_question (str): The user's question to generate a prompt for.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt for the language model.\n",
    "    \"\"\"\n",
    "    if st.session_state.use_chat_history:\n",
    "        chat_history = get_chat_history()\n",
    "        if chat_history != []:\n",
    "            question_summary = make_chat_history_summary(chat_history, user_question)\n",
    "            prompt_context = query_cortex_search_service(question_summary)\n",
    "        else:\n",
    "            prompt_context = query_cortex_search_service(user_question)\n",
    "    else:\n",
    "        prompt_context = query_cortex_search_service(user_question)\n",
    "        chat_history = \"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "            [INST]\n",
    "            You are a helpful AI chat assistant with RAG capabilities. When a user asks you a question,\n",
    "            you will also be given context provided between <context> and </context> tags. Use that context\n",
    "            with the user's chat history provided in the between <chat_history> and </chat_history> tags\n",
    "            to provide a summary that addresses the user's question. Ensure the answer is coherent, concise,\n",
    "            and directly relevant to the user's question.\n",
    "\n",
    "            If the user asks a generic question which cannot be answered with the given context or chat_history,\n",
    "            just say \"I don't know the answer to that question.\n",
    "\n",
    "            Don't saying things like \"according to the provided context\".\n",
    "\n",
    "            <chat_history>\n",
    "            {chat_history}\n",
    "            </chat_history>\n",
    "            <context>\n",
    "            {prompt_context}\n",
    "            </context>\n",
    "            <question>\n",
    "            {user_question}\n",
    "            </question>\n",
    "            [/INST]\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def main():\n",
    "    st.title(f\":speech_balloon: KubeCon 2025 Chatbot with Snowflake Cortex and Unstructured Data\")\n",
    "    init_chatbot()\n",
    "    icons = {\"assistant\": \"‚ùÑÔ∏è\", \"user\": \"üë§\"}\n",
    "\n",
    "    # Display chat messages from history on app rerun\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"], avatar=icons[message[\"role\"]]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    disable_chat = (\n",
    "        \"service_metadata\" not in st.session_state\n",
    "        or len(st.session_state.service_metadata) == 0\n",
    "    )\n",
    "    if question := st.chat_input(\"Any talks about Snowflake?\", disabled=disable_chat):\n",
    "        # Add user message to chat history\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
    "        # Display user message in chat message container\n",
    "        with st.chat_message(\"user\", avatar=icons[\"user\"]):\n",
    "            st.markdown(question.replace(\"$\", \"\\$\"))\n",
    "\n",
    "        # Display assistant response in chat message container\n",
    "        with st.chat_message(\"assistant\", avatar=icons[\"assistant\"]):\n",
    "            message_placeholder = st.empty()\n",
    "            question = question.replace(\"'\", \"\")\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                generated_response = complete(\n",
    "                    st.session_state.model_name, create_prompt(question)\n",
    "                )\n",
    "                message_placeholder.markdown(generated_response)\n",
    "\n",
    "        st.session_state.messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": generated_response}\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    session = get_active_session()\n",
    "    st.session_state.model_name = \"snowflake-arctic\"\n",
    "    st.session_state.num_chat_messages = 5\n",
    "    st.session_state.num_retrieved_chunks = 5\n",
    "    st.session_state.selected_cortex_search_service  = \"KUBECON_SEARCH_SERVICE\"\n",
    "    root = Root(session)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c01cf6-5d7d-47a6-a665-349a86eecb03",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell16"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This guide outlines how to build a RAG-based chatbot using Snowflake Cortex and Streamlit to query and retrieve KubeCon session data efficiently. This notebook demonstrates how to use Snowflake Cortex for creating a chatbot that can query parsed KubeCon session data. It starts by listing the staged files and parsing the session documents using the `PARSE_DOCUMENT` function to extract content. The parsed text is then chunked into smaller pieces using `SPLIT_TEXT_RECURSIVE_CHARACTER` to optimize it for search indexing. Afterward, a Cortex search service is created on the chunked content, and queries can be run against this service to retrieve relevant information. In the final step, Streamlit is used to build a chatbot interface, enabling users to interact with the system and ask questions about the parsed content.\n",
    "\n",
    "For more information on how to get started with Snowflake Cortex, including Retrieval Augmented Generation (RAG) applications, refer to the following links:  \n",
    "- [Snowflake Quickstarts](https://quickstarts.snowflake.com/)  \n",
    "- [RAG Applications with Snowflake](https://www.snowflake.com/en/fundamentals/rag/)  \n",
    "- [Cortex Search Overview](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "lastEditStatus": {
   "authorEmail": "fawaz.ghali@snowflake.com",
   "authorId": "5057414526494",
   "authorName": "FAWAZG",
   "lastEditTime": 1743684063913,
   "notebookId": "a5udaqirmeklixc7tm4l",
   "sessionId": "b19d4347-c938-4fe6-8c87-28d2a705aa99"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
