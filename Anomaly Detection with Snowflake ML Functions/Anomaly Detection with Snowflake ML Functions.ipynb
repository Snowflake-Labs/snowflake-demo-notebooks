{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "hl5ok2sp7tox4j6afrdg",
   "authorId": "5057414526494",
   "authorName": "FAWAZG",
   "authorEmail": "fawaz.ghali@snowflake.com",
   "sessionId": "608b7394-7e64-4001-ac69-6e0063d95f28",
   "lastEditTime": 1743080734229
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2326bf1b-1da1-406f-a7c9-ef0362c6a2ef",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "# Effortless and Trusted Anomaly Detection with Snowflake ML Functions\n\nAnomaly detection is the process of identifying **outliers** in data, especially in **time-series** datasets where data points are indexed over time. Outliers are data points that deviate significantly from expected patterns and, if unaddressed, can distort **statistical analyses** and models. By detecting and removing anomalies, we improve the accuracy and reliability of our models. The process typically involves training a model on historical data to recognize normal patterns and using that model to spot data points that fall outside of these patterns. Anomaly detection improves **data integrity**\n\nThis Notebook is designed to help you get up to speed with Anomaly Detection ML Functions in Snowflake ([link](https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection)). We will work through an example using data from a bank marketing dataset ([link](https://archive.ics.uci.edu/dataset/222/bank+marketing)). We will build an anomaly detection model to understand if certain education groups have anomalies regarding the duration of the last contact by the bank. We will wrap up this Notebook by showcasing how you can use **Tasks** to schedule your model training process and utilize the email notification integration to send out a report on trending food items.\n\nLet's get started!\n\n\n\n\n\n\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "69548257-ddb3-414e-8c7d-a97c62ab6ab3",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "# Step 1: Setting Up Snowflake Environment\n\nBefore working with data in Snowflake, it's essential to set up the **necessary infrastructure**. This includes defining user roles, creating a database and schema for organizing data, and setting up a compute warehouse to process queries efficiently. The following steps ensure that the environment is correctly configured:\n\n- **Assign Role:** First, use the `ACCOUNTADMIN` role, which has the highest level of access in Snowflake. This ensures that you have the necessary permissions to create and modify databases, schemas, and warehouses. If a different role has sufficient privileges, it can be used instead.  \n\n- **Create Database and Schema:** A **database** is where all your data is stored, and a **schema** helps organize different tables and objects within the database. In this setup, we create a database named `fawazghali_db` and a schema called `fawazghali_schema`. The `OR REPLACE` option ensures that if they already exist, they are replaced with fresh instances.  \n\n- **Select Database and Schema:** To make sure all subsequent SQL commands operate within the correct context, we explicitly set `fawazghali_db` as the active database and `fawazghali_schema` as the active schema. This avoids confusion and ensures that queries and table creations happen in the right location.  \n\n- **Create and Use Warehouse:** A **warehouse** in Snowflake is a virtual compute engine that processes queries and computations. We create a warehouse named `fawazghali_wh`, replacing any existing instance. After creation, we set it as the active warehouse to ensure all queries utilize this compute resource efficiently.  \n\nBy completing these setup steps, Snowflake is properly configured, allowing for smooth data storage, retrieval, and processing. ðŸš€  \n"
  },
  {
   "cell_type": "code",
   "id": "9c78025a-47c1-46bc-ad23-074a1b24e605",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "-- Using accountadmin is often suggested for fawazghali_dbs, but any role with sufficient privledges can work\nUSE ROLE ACCOUNTADMIN;\n\n-- Create development database, schema for our work: \nCREATE OR REPLACE DATABASE fawazghali_db;\nCREATE OR REPLACE SCHEMA fawazghali_schema;\n\n-- Use appropriate resources: \nUSE DATABASE fawazghali_db;\nUSE SCHEMA fawazghali_schema;\n\n-- Create warehouse to work with: \nCREATE OR REPLACE WAREHOUSE fawazghali_wh;\nUSE WAREHOUSE fawazghali_wh;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a8859a8c-71b0-4d40-b472-0d1b35ce89ab",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "# Step 2: Create an External Stage for AWS S3\n\nIn this step, we create an external stage that connects to an AWS S3 bucket where our data is stored. This stage will be used to load data into Snowflake.\n\n- **Stage Name**: `s3_fawazghali_load`\n- **Comment**: A description for the stage connection (e.g., \"fawazghali_db S3 Stage Connection\").\n- **S3 URL**: Specifies the location of the data on AWS S3 (e.g., `s3://sfquickstarts/hol_snowflake_cortex_ml_for_sql/`).\n- **File Format**: We specify the previously created file format (`csv_ff`) for reading CSV files. This ensures that the data will be processed correctly when loaded.\n\nThe external stage allows Snowflake to access the data in the specified S3 bucket and is an important step before ingesting the data into Snowflake tables."
  },
  {
   "cell_type": "code",
   "id": "48ec22a9-4df4-4ad2-a53a-69db987c4f87",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "-- Create a csv file format to be used to ingest from the stage: \nCREATE OR REPLACE FILE FORMAT fawazghali_db.fawazghali_schema.csv_ff\n    TYPE = 'csv'\n    SKIP_HEADER = 1,\n    COMPRESSION = AUTO;\n\n-- Create an external stage pointing to AWS S3 for loading our data:\nCREATE OR REPLACE STAGE s3_fawazghali_load \n    COMMENT = 'fawazghali_db S3 Stage Connection'\n    URL = 's3://sfquickstarts/hol_snowflake_cortex_ml_for_sql/'\n    FILE_FORMAT = fawazghali_db.fawazghali_schema.csv_ff;\n\n-- Define our table schema\nCREATE OR REPLACE TABLE fawazghali_db.fawazghali_schema.bank_marketing(\n    CUSTOMER_ID TEXT,\n    AGE NUMBER,\n    JOB TEXT, \n    MARITAL TEXT, \n    EDUCATION TEXT, \n    DEFAULT TEXT, \n    HOUSING TEXT, \n    LOAN TEXT, \n    CONTACT TEXT, \n    MONTH TEXT, \n    DAY_OF_WEEK TEXT, \n    DURATION NUMBER(4, 0), \n    CAMPAIGN NUMBER(2, 0), \n    PDAYS NUMBER(3, 0), \n    PREVIOUS NUMBER(1, 0), \n    POUTCOME TEXT, \n    EMPLOYEE_VARIATION_RATE NUMBER(2, 1), \n    CONSUMER_PRICE_INDEX NUMBER(5, 3), \n    CONSUMER_CONFIDENCE_INDEX NUMBER(3,1), \n    EURIBOR_3_MONTH_RATE NUMBER(4, 3),\n    NUMBER_EMPLOYEES NUMBER(5, 1),\n    CLIENT_SUBSCRIBED BOOLEAN,\n    TIMESTAMP TIMESTAMP_NTZ(9)\n);\n\n-- Ingest data from S3 into our table:\nCOPY INTO fawazghali_db.fawazghali_schema.bank_marketing\nFROM @s3_fawazghali_load/customers.csv;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3cbbd317-08dc-4e58-9ca2-0a6df02b2fb5",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "## Step 3: View a Sample of the Ingested Data\n\nIn this step, we query the Snowflake table to view a sample of the data that has been ingested. This helps us verify that the data was loaded correctly from the external stage.\n\n- **Query**: We use a `SELECT` statement to retrieve the first 10 rows from the `bank_marketing` table.\n- **Purpose**: The goal is to check if the data is available and looks as expected after ingestion.\n\nBy running this query, we can ensure that the data is properly loaded into the Snowflake table and ready for further analysis.\n"
  },
  {
   "cell_type": "code",
   "id": "45847f90-0ef6-4925-a7e4-9f2df6c4de6d",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "-- View a sample of the ingested data: \nSELECT * FROM fawazghali_db.fawazghali_schema.bank_marketing LIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9f58f5f7-922b-4b15-ac6e-fa7e0750a46b",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "## Step 4: Building the Anomaly Detection Model\n\nIn this step, we create a view containing the training data that will be used to build the anomaly detection model.\n\n- **Training Data**: The view, named `fawazghali_anomaly_training_set`, selects data from the `bank_marketing` table.\n- **Filtering Data**: The data is filtered to include only records where the `timestamp` is older than the most recent record by at least 12 months. This ensures that the training data consists of historical data.\n- **Purpose**: The goal is to prepare a training dataset that excludes recent data, which can be used for building the anomaly detection model.\n\nAfter creating the view, we query the `fawazghali_anomaly_training_set` view to confirm the number of rows in the training set, ensuring that the dataset is properly filtered and ready for use in the model.\n\n\n\n\n\n\n\n"
  },
  {
   "cell_type": "code",
   "id": "f414c9ea-012b-47b2-bf5b-c635e1ed9536",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "-- Create a view containing our training data\nCREATE OR REPLACE VIEW fawazghali_anomaly_training_set AS (\n    SELECT *\n    FROM fawazghali_db.fawazghali_schema.bank_marketing\n    WHERE timestamp < (SELECT MAX(timestamp) FROM fawazghali_db.fawazghali_schema.bank_marketing) - interval '12 Month'\n);\n\nselect count(*) from fawazghali_anomaly_training_set;\n\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc06775e-ed79-4f94-9b5c-755352f73083",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "## Step 5: Create a View for Anomaly Inference\n\nIn this step, we create a view containing the data on which we want to make inferences for anomaly detection.\n\n- **Inference Data**: The view, named `fawazghali_anomaly_analysis_set`, selects data from the `bank_marketing` table.\n- **Filtering Data**: The data is filtered to include only records where the `timestamp` is more recent than the most recent record in the `fawazghali_anomaly_training_set` view. This ensures that the inference data consists of the latest data, which has not been used in the training set.\n- **Purpose**: The goal is to prepare a dataset that will be used for making predictions or detecting anomalies in the most recent data.\n\nAfter creating the view, we query the `fawazghali_anomaly_analysis_set` view to confirm the number of rows in the analysis set, ensuring that the dataset is correctly filtered and ready for anomaly detection.\n"
  },
  {
   "cell_type": "code",
   "id": "fb9dbfbc-2209-43cb-ab16-393ef4a19340",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "\n-- Create a view containing the data we want to make inferences on\nCREATE OR REPLACE VIEW fawazghali_anomaly_analysis_set AS (\n    SELECT *\n    FROM fawazghali_db.fawazghali_schema.bank_marketing\n    WHERE timestamp > (SELECT MAX(timestamp) FROM fawazghali_anomaly_training_set)\n);\nselect count(*) from fawazghali_anomaly_analysis_set;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "232c873d-a8e2-457f-a922-800a9a30072b",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "e64d4fe8-529a-43cf-b04c-e90120bfdbb8",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "## Step 6: Create the Anomaly Detection Model\n\nIn this step, we create the anomaly detection model using the `UNSUPERVISED` method. The model will analyze the data to detect anomalies.\n\n- **Model Creation**: We use the `CREATE OR REPLACE snowflake.ml.anomaly_detection` command to create the model, named `fawazghali_anomaly_model`. The model is built using the following parameters:\n  - `INPUT_DATA`: The view `fawazghali_anomaly_training_set`, which contains the training data.\n  - `SERIES_COLNAME`: The column used for time series analysis, in this case, `EDUCATION`.\n  - `TIMESTAMP_COLNAME`: The column representing the timestamp, which is `TIMESTAMP`.\n  - `TARGET_COLNAME`: The target variable for anomaly detection, here itâ€™s `DURATION`.\n  - `LABEL_COLNAME`: The column for labels (if available). In this case, it is left empty, implying the model is unsupervised, but labels could be passed if desired.\n\n- **Time Considerations**: The creation of the model might take a few minutes, depending on the size of the warehouse and data. Please be patient during this process.\n\nOnce the model is created, it will be ready to detect anomalies in future data.\n\n\n\n\n\n\n\n"
  },
  {
   "cell_type": "code",
   "id": "264e12b7-a16d-4515-8887-9010c0ad828f",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "\n-- Create the model: UNSUPERVISED method, however can pass labels as well; this could take few minutes depending on the wharehouse size; please be patient \nCREATE OR REPLACE snowflake.ml.anomaly_detection fawazghali_anomaly_model(\n    INPUT_DATA => SYSTEM$REFERENCE('VIEW', 'fawazghali_anomaly_training_set'),\n    SERIES_COLNAME => 'EDUCATION',\n    TIMESTAMP_COLNAME => 'TIMESTAMP',\n    TARGET_COLNAME => 'DURATION',\n    LABEL_COLNAME => ''\n); \n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4694b6ca-8c10-413e-82f6-8b31ed985933",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "## Step 7: Call the Anomaly Detection Model and Store Results\n\nIn this step, we call the anomaly detection model to identify anomalies in the data and store the results in a table.\n\n- **Model Call**: The `DETECT_ANOMALIES` function is invoked with the following parameters:\n  - `INPUT_DATA`: The view `fawazghali_anomaly_analysis_set`, which contains the data for inference.\n  - `SERIES_COLNAME`: The column used for time series analysis, in this case, `EDUCATION`.\n  - `TIMESTAMP_COLNAME`: The column representing the timestamp, which is `TIMESTAMP`.\n  - `TARGET_COLNAME`: The target variable for anomaly detection, here it is `DURATION`.\n  - `CONFIG_OBJECT`: An object specifying additional configuration options like the prediction interval (`0.95`).\n\n- **Storing Results**: After the model runs, the results are stored in a table `fawazghali_anomalies`. We use `RESULT_SCAN(-1)` to retrieve the output of the last function call and create a new table with the results.\n\n- **Querying Anomalies**: We then query the `fawazghali_anomalies` table to identify the series with the highest number of anomalies, specifically those with `is_anomaly = 1`. The result is grouped and ordered to find the series with the most detected anomalies.\n\nThis process allows us to detect and review anomalies in the latest data based on the trained model.\n"
  },
  {
   "cell_type": "code",
   "id": "fbb4f1e2-e07a-46b4-8d86-40754435fa69",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "\n-- Call the model and store the results into table; this could take few minutes depending on the wharehouse size; please be patient\nCALL fawazghali_anomaly_model!DETECT_ANOMALIES(\n    INPUT_DATA => SYSTEM$REFERENCE('VIEW', 'fawazghali_anomaly_analysis_set'),\n    SERIES_COLNAME => 'EDUCATION',\n    TIMESTAMP_COLNAME => 'TIMESTAMP',\n    TARGET_COLNAME => 'DURATION',\n    CONFIG_OBJECT => {'prediction_interval': 0.95}\n);\n\n\n-- Create a table from the results\nCREATE OR REPLACE TABLE fawazghali_anomalies AS (\n    SELECT *\n    FROM TABLE(RESULT_SCAN(-1))\n);\n\n\n\nSELECT series, is_anomaly, count(is_anomaly) AS num_records\nFROM fawazghali_anomalies\nWHERE is_anomaly =1\nGROUP BY ALL\nORDER BY num_records DESC\nLIMIT 1;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3be61cba-7ee9-43e5-8c6e-8e7bb5aa8824",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": "# Conclusion  \n\nIn this notebook, we explored **Anomaly Detection** using **Snowflake ML Functions**, a powerful toolset designed to identify **outliers** in datasets efficiently. We examined how Snowflake's built-in functions simplify anomaly detection in **time-series** and other structured data, ensuring **data integrity** and **model reliability**.  \n\n## Key takeaways:  \n- **Anomaly detection** helps in identifying data points that significantly deviate from expected patterns.  \n- **Snowflake ML Functions** provide an effortless and scalable approach to implementing anomaly detection.  \n- **Practical use case**: We demonstrated anomaly detection on a **bank marketing dataset**, showing how Snowflake can help uncover outliers in real-world data.  \n\nBy leveraging Snowflake's capabilities, organizations can **automate anomaly detection**, enhance **data-driven decision-making**, and ensure **high-quality insights**.  \n\n## Resources  \n\nTo explore further, refer to the following resources:  \n\n1. **Snowflake Quickstarts**: Hands-on guides for implementing ML solutions in Snowflake.  \n   - [Quickstarts](https://quickstarts.snowflake.com/)  \n\n2. **Anomaly Detection ML Functions Documentation**: Official documentation covering Snowflake's anomaly detection features.  \n   - [Anomaly Detection ML Functions](https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection)  \n\n3. **SQL Reference for Anomaly Detection**: Detailed SQL syntax and examples for implementing anomaly detection in Snowflake.  \n   - [SQL Reference for Anomaly Detection](https://docs.snowflake.com/en/sql-reference/classes/anomaly_detection)  "
  }
 ]
}
