{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "tj2xtxqot6wsxrqxt3mf",
   "authorId": "1160199988574",
   "authorName": "KNJERU",
   "authorEmail": "kim.njeru@snowflake.com",
   "sessionId": "cf69ad7f-f9e9-422d-8536-69ea5c233cb4",
   "lastEditTime": 1753377029962
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33405285-7c8e-4b9b-b06b-4c473e7fa4cf",
   "metadata": {
    "name": "create_wh",
    "collapsed": false
   },
   "source": "# Create Gen2 Warehouses\n\nCreate 3 POC warehouses for testing with auto-suspend and auto-resume.\n- Gen 1: Size n\n- Gen 2: Size n\n- Gen 2: Size n-1\n"
  },
  {
   "cell_type": "code",
   "id": "e259c232-dbb8-4af1-ae51-8fa3b1a5b14c",
   "metadata": {
    "language": "sql",
    "name": "sql_create_wh",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Create Gen1:Size n\nCREATE OR REPLACE WAREHOUSE POC_GEN1_S WAREHOUSE_SIZE = SMALL\nAUTO_RESUME=TRUE AUTO_SUSPEND=60 INITIALLY_SUSPENDED=TRUE RESOURCE_CONSTRAINT = STANDARD_GEN_1 COMMENT = 'My Small POC Gen1 WH';\n\n--Create Gen2:Size n\nCREATE OR REPLACE WAREHOUSE POC_GEN2_S WAREHOUSE_SIZE = SMALL\nAUTO_RESUME=TRUE AUTO_SUSPEND=60 INITIALLY_SUSPENDED=TRUE RESOURCE_CONSTRAINT = STANDARD_GEN_2 COMMENT = 'My Small POC Gen2 WH';\n\n--Create Gen2:Size n-1\nCREATE OR REPLACE WAREHOUSE POC_GEN2_XS WAREHOUSE_SIZE = XSMALL\nAUTO_RESUME=TRUE AUTO_SUSPEND=60 INITIALLY_SUSPENDED=TRUE RESOURCE_CONSTRAINT = STANDARD_GEN_2 COMMENT = 'My XSmall POC Gen2 WH';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e22228f5-764f-434b-87fb-b08ea81ecd3d",
   "metadata": {
    "name": "invalidate_cache",
    "collapsed": false
   },
   "source": "# Invalidate Cache\n\nThis is the most critical procedural step for a pure performance comparison. Both the warehouse's local disk cache and Snowflake's global result cache must be deliberately invalidated before each test run.\n\n**Warehouse Cache**: The local SSD cache on a virtual warehouse stores data recently accessed from remote storage. To ensure this cache is cleared and does not influence subsequent runs, the warehouse must be suspended and resumed before each test execution.\n- ALTER WAREHOUSE <poc_warehouse_name> SUSPEND;\n- ALTER WAREHOUSE <poc_warehouse_name> RESUME;\n\n**Result Cache**: Snowflake maintains a result cache that returns the results of previously executed queries without re-computation. For this POC, the result cache must be disabled at the session level to force the query to execute from scratch every time.\n- ALTER SESSION SET USE_CACHED_RESULT = FALSE;\n\n# Tag Workload\n\nEnsure that the **QUERY_TAGS** at the session level are applied fore each workload execution.\n- ALTER SESSION SET QUERY_TAG = 'POC_GEN2_ETL_WORKLOAD_RUN_1';"
  },
  {
   "cell_type": "markdown",
   "id": "8ecd4cb3-46e6-410b-8b8d-288091129188",
   "metadata": {
    "name": "execute_workload",
    "collapsed": false
   },
   "source": "# Execute Workloads\nThe rationale described above is codified in the Python Script below and produces test functions in 4 categories that can be run in threaded or async mode against Snowflake TPC-H Sample Data as described here:https://docs.snowflake.com/en/user-guide/sample-data-tpch\n\n- light\n- medium\n- heavy\n- custom (choose the number of concurrent users and number of runs)\n\nExecute the functions against Gen 1, Gen 2 and downsized Gen2 Warehouses and compare the results."
  },
  {
   "cell_type": "code",
   "id": "bc8b9149-04b9-4247-9a2d-c17266f65eb3",
   "metadata": {
    "language": "python",
    "name": "py_tpch_script",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import pandas as pd\nimport time\nimport asyncio\nimport threading\nimport concurrent.futures\nfrom typing import Dict, List, Tuple\nfrom snowflake.snowpark import Session\nimport statistics\nfrom datetime import datetime\n\n# Kim Njeru 07/24/2025 Script to create test harness with TPC-H queries\n# Get the current session\nsession = Session.builder.getOrCreate()\n\n# TPC-H Queries\nTPCH_QUERIES = {\n    'Q1': \"\"\"\n    SELECT\n        l_returnflag,\n        l_linestatus,\n        sum(l_quantity) as sum_qty,\n        sum(l_extendedprice) as sum_base_price,\n        sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,\n        sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,\n        avg(l_quantity) as avg_qty,\n        avg(l_extendedprice) as avg_price,\n        avg(l_discount) as avg_disc,\n        count(*) as count_order\n    FROM\n        snowflake_sample_data.tpch_sf1.lineitem\n    WHERE\n        l_shipdate <= dateadd(day, -90, '1998-12-01'::date)\n    GROUP BY\n        l_returnflag,\n        l_linestatus\n    ORDER BY\n        l_returnflag,\n        l_linestatus;\n    \"\"\",\n    \n    'Q3': \"\"\"\n    SELECT\n        l_orderkey,\n        sum(l_extendedprice * (1 - l_discount)) as revenue,\n        o_orderdate,\n        o_shippriority\n    FROM\n        snowflake_sample_data.tpch_sf1.customer,\n        snowflake_sample_data.tpch_sf1.orders,\n        snowflake_sample_data.tpch_sf1.lineitem\n    WHERE\n        c_mktsegment = 'BUILDING'\n        AND c_custkey = o_custkey\n        AND l_orderkey = o_orderkey\n        AND o_orderdate < '1995-03-15'::date\n        AND l_shipdate > '1995-03-15'::date\n    GROUP BY\n        l_orderkey,\n        o_orderdate,\n        o_shippriority\n    ORDER BY\n        revenue desc,\n        o_orderdate\n    LIMIT 10;\n    \"\"\",\n    \n    'Q6': \"\"\"\n    SELECT\n        sum(l_extendedprice * l_discount) as revenue\n    FROM\n        snowflake_sample_data.tpch_sf1.lineitem\n    WHERE\n        l_shipdate >= '1994-01-01'::date\n        AND l_shipdate < dateadd(year, 1, '1994-01-01'::date)\n        AND l_discount between 0.06 - 0.01 AND 0.06 + 0.01\n        AND l_quantity < 24;\n    \"\"\",\n    \n    'Q10': \"\"\"\n    SELECT\n        c_custkey,\n        c_name,\n        sum(l_extendedprice * (1 - l_discount)) as revenue,\n        c_acctbal,\n        n_name,\n        c_address,\n        c_phone,\n        c_comment\n    FROM\n        snowflake_sample_data.tpch_sf1.customer,\n        snowflake_sample_data.tpch_sf1.orders,\n        snowflake_sample_data.tpch_sf1.lineitem,\n        snowflake_sample_data.tpch_sf1.nation\n    WHERE\n        c_custkey = o_custkey\n        AND l_orderkey = o_orderkey\n        AND o_orderdate >= '1993-10-01'::date\n        AND o_orderdate < dateadd(month, 3, '1993-10-01'::date)\n        AND l_returnflag = 'R'\n        AND c_nationkey = n_nationkey\n    GROUP BY\n        c_custkey,\n        c_name,\n        c_acctbal,\n        c_phone,\n        n_name,\n        c_address,\n        c_comment\n    ORDER BY\n        revenue desc\n    LIMIT 20;\n    \"\"\"\n}\n\n# WAREHOUSE AND CACHE MANAGEMENT\n\ndef setup_warehouse_and_cache(warehouse_name: str, query_tag: str):\n    \"\"\"Setup warehouse, invalidate cache, disable result cache, and set query tag\"\"\"\n    print(f\"\\n🔧 Setting up warehouse and cache configuration...\")\n    \n    try:\n        # 1. Select warehouse\n        print(f\"📍 Switching to warehouse: {warehouse_name}\")\n        session.sql(f\"USE WAREHOUSE {warehouse_name}\").collect()\n        \n        # 2. Suspend and resume warehouse to invalidate cache\n        print(f\"⏸️  Attempting to suspend warehouse {warehouse_name}...\")\n        try:\n            session.sql(f\"ALTER WAREHOUSE {warehouse_name} SUSPEND\").collect()\n            print(f\"✅ Warehouse suspended\")\n        except Exception as suspend_error:\n            error_msg = str(suspend_error).lower()\n            if \"already suspended\" in error_msg or \"invalid state\" in error_msg:\n                print(f\"ℹ️  Warehouse {warehouse_name} is already suspended\")\n            else:\n                print(f\"⚠️  Suspend warning: {suspend_error}\")\n        \n        print(f\"▶️  Resuming warehouse {warehouse_name}...\")\n        session.sql(f\"ALTER WAREHOUSE {warehouse_name} RESUME\").collect()\n        print(f\"✅ Warehouse resumed\")\n        \n        # Brief pause to ensure warehouse is ready\n        time.sleep(3)\n        \n        # 3. Disable result cache at session level\n        print(f\"🚫 Disabling result cache for this session...\")\n        session.sql(\"ALTER SESSION SET USE_CACHED_RESULT = FALSE\").collect()\n        print(f\"✅ Result cache disabled\")\n        \n        # 4. Set query tag\n        print(f\"🏷️  Setting query tag: {query_tag}\")\n        session.sql(f\"ALTER SESSION SET QUERY_TAG = '{query_tag}'\").collect()\n        print(f\"✅ Query tag set\")\n        \n        print(f\"✅ Setup complete!\")\n        print(f\"   - Warehouse: {warehouse_name}\")\n        print(f\"   - Result Cache: DISABLED\")\n        print(f\"   - Query Tag: {query_tag}\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"❌ Error setting up warehouse and cache: {e}\")\n        return False\n\nasync def setup_warehouse_and_cache_async(warehouse_name: str, query_tag: str):\n    \"\"\"Async version of warehouse setup\"\"\"\n    print(f\"\\n🔧 Setting up warehouse and cache configuration (ASYNC)...\")\n    \n    try:\n        loop = asyncio.get_event_loop()\n        \n        # 1. Select warehouse\n        print(f\"📍 Switching to warehouse: {warehouse_name}\")\n        await loop.run_in_executor(None, lambda: session.sql(f\"USE WAREHOUSE {warehouse_name}\").collect())\n        \n        # 2. Suspend and resume warehouse\n        print(f\"⏸️  Attempting to suspend warehouse {warehouse_name}...\")\n        try:\n            await loop.run_in_executor(None, lambda: session.sql(f\"ALTER WAREHOUSE {warehouse_name} SUSPEND\").collect())\n            print(f\"✅ Warehouse suspended\")\n        except Exception as suspend_error:\n            error_msg = str(suspend_error).lower()\n            if \"already suspended\" in error_msg or \"invalid state\" in error_msg:\n                print(f\"ℹ️  Warehouse {warehouse_name} is already suspended\")\n            else:\n                print(f\"⚠️  Suspend warning: {suspend_error}\")\n        \n        print(f\"▶️  Resuming warehouse {warehouse_name}...\")\n        await loop.run_in_executor(None, lambda: session.sql(f\"ALTER WAREHOUSE {warehouse_name} RESUME\").collect())\n        print(f\"✅ Warehouse resumed\")\n        \n        await asyncio.sleep(3)\n        \n        # 3. Disable result cache\n        print(f\"🚫 Disabling result cache...\")\n        await loop.run_in_executor(None, lambda: session.sql(\"ALTER SESSION SET USE_CACHED_RESULT = FALSE\").collect())\n        \n        # 4. Set query tag\n        print(f\"🏷️  Setting query tag: {query_tag}\")\n        await loop.run_in_executor(None, lambda: session.sql(f\"ALTER SESSION SET QUERY_TAG = '{query_tag}'\").collect())\n        \n        print(f\"✅ Async setup complete!\")\n        return True\n        \n    except Exception as e:\n        print(f\"❌ Error in async setup: {e}\")\n        return False\n\n# CORE BENCHMARK FUNCTIONS\n\ndef run_tpch_benchmark():\n    \"\"\"Run the TPC-H benchmark queries - ORIGINAL SYNCHRONOUS VERSION\"\"\"\n    results = {}\n    timing_results = {}\n    \n    print(\"🚀 Starting TPC-H Benchmark\")\n    print(\"=\" * 40)\n    \n    for query_name, query in TPCH_QUERIES.items():\n        print(f\"\\n🔄 Executing {query_name}...\")\n        start_time = time.time()\n        \n        try:\n            df = session.sql(query).to_pandas()\n            end_time = time.time()\n            execution_time = end_time - start_time\n            \n            results[query_name] = df\n            timing_results[query_name] = execution_time\n            \n            print(f\"✅ {query_name} completed in {execution_time:.2f} seconds\")\n            print(f\"📊 Returned {len(df)} rows\")\n            \n        except Exception as e:\n            print(f\"❌ Error executing {query_name}: {e}\")\n            timing_results[query_name] = 0\n    \n    total_time = sum(timing_results.values())\n    print(f\"\\n📈 EXECUTION SUMMARY:\")\n    print(\"=\" * 40)\n    for query_name, exec_time in timing_results.items():\n        print(f\"{query_name}: {exec_time:.2f}s\")\n    print(f\"\\nTotal execution time: {total_time:.2f}s\")\n    print(f\"Average query time: {total_time/len(timing_results):.2f}s\")\n    \n    return results, timing_results\n\ndef run_single_benchmark(run_id: int, query_tag: str) -> Dict:\n    \"\"\"Execute a single benchmark run for threading\"\"\"\n    start_time = time.time()\n    \n    try:\n        # Set session parameters\n        session.sql(\"ALTER SESSION SET USE_CACHED_RESULT = FALSE\").collect()\n        session.sql(f\"ALTER SESSION SET QUERY_TAG = '{query_tag}_run_{run_id}'\").collect()\n        \n        # Suppress output for load testing\n        import sys\n        from io import StringIO\n        old_stdout = sys.stdout\n        sys.stdout = StringIO()\n        \n        results, timing_results = run_tpch_benchmark()\n        \n        sys.stdout = old_stdout\n        \n        end_time = time.time()\n        total_execution_time = end_time - start_time\n        \n        return {\n            'run_id': run_id,\n            'total_execution_time': total_execution_time,\n            'individual_timings': timing_results,\n            'total_queries': len(timing_results),\n            'status': 'SUCCESS',\n            'error': None,\n            'timestamp': datetime.now()\n        }\n        \n    except Exception as e:\n        sys.stdout = old_stdout\n        end_time = time.time()\n        total_execution_time = end_time - start_time\n        \n        return {\n            'run_id': run_id,\n            'total_execution_time': total_execution_time,\n            'individual_timings': {},\n            'total_queries': 0,\n            'status': 'ERROR',\n            'error': str(e),\n            'timestamp': datetime.now()\n        }\n\nasync def run_single_benchmark_async(run_id: int, query_tag: str) -> Dict:\n    \"\"\"Execute a single benchmark run for async\"\"\"\n    start_time = time.time()\n    \n    try:\n        loop = asyncio.get_event_loop()\n        \n        # Set session parameters\n        await loop.run_in_executor(None, lambda: session.sql(\"ALTER SESSION SET USE_CACHED_RESULT = FALSE\").collect())\n        await loop.run_in_executor(None, lambda: session.sql(f\"ALTER SESSION SET QUERY_TAG = '{query_tag}_run_{run_id}'\").collect())\n        \n        # Run benchmark in executor\n        results, timing_results = await loop.run_in_executor(None, run_tpch_benchmark_silent)\n        \n        end_time = time.time()\n        total_execution_time = end_time - start_time\n        \n        return {\n            'run_id': run_id,\n            'total_execution_time': total_execution_time,\n            'individual_timings': timing_results,\n            'total_queries': len(timing_results),\n            'status': 'SUCCESS',\n            'error': None,\n            'timestamp': datetime.now()\n        }\n        \n    except Exception as e:\n        end_time = time.time()\n        total_execution_time = end_time - start_time\n        \n        return {\n            'run_id': run_id,\n            'total_execution_time': total_execution_time,\n            'individual_timings': {},\n            'total_queries': 0,\n            'status': 'ERROR',\n            'error': str(e),\n            'timestamp': datetime.now()\n        }\n\ndef run_tpch_benchmark_silent():\n    \"\"\"Silent version of benchmark for async execution\"\"\"\n    results = {}\n    timing_results = {}\n    \n    for query_name, query in TPCH_QUERIES.items():\n        start_time = time.time()\n        try:\n            df = session.sql(query).to_pandas()\n            end_time = time.time()\n            execution_time = end_time - start_time\n            \n            results[query_name] = df\n            timing_results[query_name] = execution_time\n        except Exception as e:\n            timing_results[query_name] = 0\n    \n    return results, timing_results\n\n# THREADED LOAD TESTING\n\ndef run_threaded_load_test(warehouse_name: str, concurrent_users: int = 10, total_runs: int = None, test_name: str = \"THREADED_LOAD_TEST\"):\n    \"\"\"Run threaded load test\"\"\"\n    total_runs = total_runs or concurrent_users\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    query_tag = f\"{test_name}_{warehouse_name}_{concurrent_users}users_{timestamp}\"\n    \n    print(f\"🚀 Starting THREADED TPC-H Load Test\")\n    print(f\"📊 Test Name: {test_name}\")\n    print(f\"📊 Warehouse: {warehouse_name}\")\n    print(f\"📊 Concurrent Users: {concurrent_users}\")\n    print(f\"📊 Total Runs: {total_runs}\")\n    print(f\"🏷️  Query Tag: {query_tag}\")\n    print(f\"⏰ Start Time: {datetime.now()}\")\n    print(\"=\" * 60)\n    \n    # Setup warehouse and cache\n    if not setup_warehouse_and_cache(warehouse_name, query_tag):\n        print(\"❌ Failed to setup warehouse and cache. Aborting test.\")\n        return None\n    \n    results = []\n    start_time = time.time()\n    \n    # Use ThreadPoolExecutor\n    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n        futures = [\n            executor.submit(run_single_benchmark, i, query_tag) \n            for i in range(total_runs)\n        ]\n        \n        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n            result = future.result()\n            results.append(result)\n            \n            if (i + 1) % max(1, total_runs // 10) == 0:\n                print(f\"✅ Completed {i + 1}/{total_runs} runs\")\n    \n    end_time = time.time()\n    total_test_time = end_time - start_time\n    \n    analyze_results(results, total_test_time, concurrent_users, warehouse_name, query_tag, \"THREADED\")\n    return results\n\n# ASYNC LOAD TESTING\n\nasync def run_async_load_test(warehouse_name: str, concurrent_users: int = 10, total_runs: int = None, test_name: str = \"ASYNC_LOAD_TEST\"):\n    \"\"\"Run async load test\"\"\"\n    total_runs = total_runs or concurrent_users\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    query_tag = f\"{test_name}_{warehouse_name}_{concurrent_users}users_{timestamp}\"\n    \n    print(f\"🚀 Starting ASYNC TPC-H Load Test\")\n    print(f\"📊 Test Name: {test_name}\")\n    print(f\"📊 Warehouse: {warehouse_name}\")\n    print(f\"📊 Concurrent Users: {concurrent_users}\")\n    print(f\"📊 Total Runs: {total_runs}\")\n    print(f\"🏷️  Query Tag: {query_tag}\")\n    print(f\"⏰ Start Time: {datetime.now()}\")\n    print(\"=\" * 60)\n    \n    # Setup warehouse and cache\n    if not await setup_warehouse_and_cache_async(warehouse_name, query_tag):\n        print(\"❌ Failed to setup warehouse and cache. Aborting test.\")\n        return None\n    \n    start_time = time.time()\n    \n    # Create semaphore to limit concurrent connections\n    semaphore = asyncio.Semaphore(concurrent_users)\n    \n    async def run_with_semaphore(run_id):\n        async with semaphore:\n            return await run_single_benchmark_async(run_id, query_tag)\n    \n    # Create and run all tasks\n    tasks = [run_with_semaphore(i) for i in range(total_runs)]\n    results = []\n    completed = 0\n    \n    for coro in asyncio.as_completed(tasks):\n        result = await coro\n        results.append(result)\n        completed += 1\n        \n        if completed % max(1, total_runs // 10) == 0:\n            print(f\"✅ Completed {completed}/{total_runs} runs\")\n    \n    end_time = time.time()\n    total_test_time = end_time - start_time\n    \n    analyze_results(results, total_test_time, concurrent_users, warehouse_name, query_tag, \"ASYNC\")\n    return results\n\n# RESULTS ANALYSIS\n\ndef analyze_results(results: List[Dict], total_test_time: float, concurrent_users: int, warehouse_name: str, query_tag: str, execution_model: str):\n    \"\"\"Analyze and display load test results\"\"\"\n    \n    successful_runs = [r for r in results if r['status'] == 'SUCCESS']\n    failed_runs = [r for r in results if r['status'] == 'ERROR']\n    \n    print(f\"\\n📈 {execution_model} LOAD TEST RESULTS\")\n    print(\"=\" * 60)\n    print(f\"🎯 Test Configuration:\")\n    print(f\"   - Execution Model: {execution_model}\")\n    print(f\"   - Warehouse: {warehouse_name}\")\n    print(f\"   - Query Tag: {query_tag}\")\n    print(f\"   - Concurrent Users: {concurrent_users}\")\n    print(f\"   - Total Runs: {len(results)}\")\n    print(f\"   - Total Test Time: {total_test_time:.2f}s\")\n    \n    print(f\"\\n📊 Success/Failure:\")\n    print(f\"   - Successful Runs: {len(successful_runs)}\")\n    print(f\"   - Failed Runs: {len(failed_runs)}\")\n    print(f\"   - Success Rate: {len(successful_runs)/len(results)*100:.1f}%\")\n    \n    if successful_runs:\n        benchmark_times = [r['total_execution_time'] for r in successful_runs]\n        \n        print(f\"\\n⏱️  Benchmark Execution Times:\")\n        print(f\"   - Min: {min(benchmark_times):.3f}s\")\n        print(f\"   - Max: {max(benchmark_times):.3f}s\")\n        print(f\"   - Average: {statistics.mean(benchmark_times):.3f}s\")\n        print(f\"   - Median: {statistics.median(benchmark_times):.3f}s\")\n        if len(benchmark_times) > 1:\n            print(f\"   - Std Dev: {statistics.stdev(benchmark_times):.3f}s\")\n        \n        print(f\"\\n🚀 Performance Metrics:\")\n        print(f\"   - Benchmarks per Second: {len(successful_runs)/total_test_time:.2f}\")\n        print(f\"   - Queries per Second: {len(successful_runs) * len(TPCH_QUERIES)/total_test_time:.2f}\")\n        \n        # Individual query performance\n        query_stats = {}\n        for run in successful_runs:\n            for query_name, exec_time in run['individual_timings'].items():\n                if query_name not in query_stats:\n                    query_stats[query_name] = []\n                query_stats[query_name].append(exec_time)\n        \n        print(f\"\\n📊 Individual Query Performance:\")\n        for query_name in sorted(query_stats.keys()):\n            times = query_stats[query_name]\n            print(f\"   {query_name}: Avg={statistics.mean(times):.3f}s, Min={min(times):.3f}s, Max={max(times):.3f}s\")\n    \n    if failed_runs:\n        print(f\"\\n❌ Errors ({len(failed_runs)} total):\")\n        error_counts = {}\n        for run in failed_runs:\n            error = run['error']\n            error_counts[error] = error_counts.get(error, 0) + 1\n        \n        for error, count in error_counts.items():\n            print(f\"   - {error}: {count} times\")\n\n# CONVENIENCE FUNCTIONS\n\ndef light_threaded_test(warehouse_name: str, test_name: str = \"LIGHT_THREADED\"):\n    return run_threaded_load_test(warehouse_name, 10, 20, test_name)\n\ndef medium_threaded_test(warehouse_name: str, test_name: str = \"MEDIUM_THREADED\"):\n    return run_threaded_load_test(warehouse_name, 50, 100, test_name)\n\ndef heavy_threaded_test(warehouse_name: str, test_name: str = \"HEAVY_THREADED\"):\n    return run_threaded_load_test(warehouse_name, 100, 200, test_name)\n\n# Async convenience functions with sync wrappers\ndef run_async_test(coro):\n    \"\"\"Helper to run async functions from sync context\"\"\"\n    try:\n        loop = asyncio.get_event_loop()\n        if loop.is_running():\n            import concurrent.futures\n            with concurrent.futures.ThreadPoolExecutor() as executor:\n                future = executor.submit(asyncio.run, coro)\n                return future.result()\n        else:\n            return loop.run_until_complete(coro)\n    except RuntimeError:\n        return asyncio.run(coro)\n\ndef light_async_test(warehouse_name: str, test_name: str = \"LIGHT_ASYNC\"):\n    return run_async_test(run_async_load_test(warehouse_name, 10, 20, test_name))\n\ndef medium_async_test(warehouse_name: str, test_name: str = \"MEDIUM_ASYNC\"):\n    return run_async_test(run_async_load_test(warehouse_name, 50, 100, test_name))\n\ndef heavy_async_test(warehouse_name: str, test_name: str = \"HEAVY_ASYNC\"):\n    return run_async_test(run_async_load_test(warehouse_name, 100, 200, test_name))\n\n# USAGE INSTRUCTIONS\nprint(\"🚀 TPC-H Load Testing Script Ready!\")\nprint(\"\\n📋 Available Test Functions:\")\nprint(\"=\" * 50)\nprint(\"THREADED TESTS:\")\nprint(\"1. light_threaded_test('WAREHOUSE_NAME', 'TEST_NAME')\")\nprint(\"2. medium_threaded_test('WAREHOUSE_NAME', 'TEST_NAME')\")\nprint(\"3. heavy_threaded_test('WAREHOUSE_NAME', 'TEST_NAME')\")\nprint(\"4. run_threaded_load_test('WH', concurrent_users, total_runs, 'TEST')\")\n\nprint(\"\\nASYNC TESTS:\")\nprint(\"1. light_async_test('WAREHOUSE_NAME', 'TEST_NAME')\")\nprint(\"2. medium_async_test('WAREHOUSE_NAME', 'TEST_NAME')\")\nprint(\"3. heavy_async_test('WAREHOUSE_NAME', 'TEST_NAME')\")\nprint(\"4. run_async_test(run_async_load_test('WH', concurrent_users, total_runs, 'TEST'))\")\n\nprint(\"\\nORIGINAL SYNC:\")\nprint(\"1. results, timings = run_tpch_benchmark()\")\n\nprint(\"\\nExample Usage:\")\nprint(\"# Compare threaded vs async\")\nprint(\"threaded_results = light_threaded_test('POC_GEN2', 'THREADED_TEST')\")\nprint(\"async_results = light_async_test('POC_GEN2', 'ASYNC_TEST')\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "224791ab-d660-4464-88b0-75619cabbb66",
   "metadata": {
    "language": "python",
    "name": "py_run_gen1_threaded",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Update the first variable with the correct Warehouse name\nheavy_test = heavy_threaded_test('POC_GEN1_S','HEAVY')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b512a57a-638c-4913-80db-bae30420ac5d",
   "metadata": {
    "language": "python",
    "name": "py_run_gen2_threaded",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Update the first variable with the correct Warehouse name\nheavy_test = heavy_threaded_test('POC_GEN2_S','HEAVY')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41ab7da8-9062-4fc8-acac-f6adcc7ea850",
   "metadata": {
    "language": "python",
    "name": "py_run_gen2_1_threaded"
   },
   "outputs": [],
   "source": "# Update the first variable with the correct Warehouse name\nheavy_test = heavy_threaded_test('POC_GEN2_XS','HEAVY')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80b1ccf1-0a67-400b-ad87-30e383af802f",
   "metadata": {
    "language": "python",
    "name": "py_run_gen1_async"
   },
   "outputs": [],
   "source": "# Update the first variable with the correct Warehouse name\n# Optionally update the 2nd and 3rd variables with number of concurrent users and number of runs\nasync_custom = run_async_test(run_async_load_test('POC_GEN1', 100, 5, 'ASYNC'))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75f19d0b-6b45-4389-b62c-805d562b321f",
   "metadata": {
    "language": "python",
    "name": "py_run_gen2_async"
   },
   "outputs": [],
   "source": "# Update the first variable with the correct Warehouse name\n# Optionally update the 2nd and 3rd variables with number of concurrent users and number of runs\nasync_custom = run_async_test(run_async_load_test('POC_GEN2', 100, 5, 'ASYNC'))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1bb91747-e518-4b61-9d78-a1ab93d7aa21",
   "metadata": {
    "language": "python",
    "name": "py_run_gen2_1_async"
   },
   "outputs": [],
   "source": "# Update the first variable with the correct Warehouse name\n# Optionally update the 2nd and 3rd variables with number of concurrent users and number of runs\nasync_custom = run_async_test(run_async_load_test('POC_GEN2_XS', 100, 5, 'ASYNC'))",
   "execution_count": null
  }
 ]
}