{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "tcycafykldptowcuoewa",
   "authorId": "3290930229076",
   "authorName": "JSOMMERFELD",
   "authorEmail": "jan.sommerfeld@snowflake.com",
   "sessionId": "6168993f-fd10-4f27-a6c8-90e766a84fda",
   "lastEditTime": 1751832202884
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "_title",
    "collapsed": false
   },
   "source": "# ⏱️ Snowflake Trail - Step 4: Anomaly Detection Setup\n\n---\nscheduled Alerts to check for anomalies and log them to the event table\n\n* Tasks run duration\n* Task run frequency\n* Pipe copy frequency\n* Pipe rows ingested\n* Dynamic Table rows updated"
  },
  {
   "cell_type": "markdown",
   "id": "4d020daf-6dce-490a-ba2c-63e45ea2b3e2",
   "metadata": {
    "name": "_example",
    "collapsed": false
   },
   "source": "### Example: numeric value timeseries anomaly\n\n- we can query the hourly credit usage of our serverless alert and see if there are statistical outliers in that history. \n- we use the rounded timestamp column for sorting\n- the credits value as the numeric value which we want to analyze\n- for each row calculate both average and standard deviation of the values in the previous 50 rows\n- calculate the Z-score as ((current value - average value) / standard deviation) for each row\n- return all rows with a Z-score above 3"
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "anomaly_query_example"
   },
   "source": "-- identifying outliers in the hourly credit consumption history of our serverless NEW_ERRORS alert\n\nwith \nRECORDS as (\n    select\n        date_trunc(hour, START_TIME) as REC_TIMESTAMP,\n        sum(CREDITS_USED) as REC_VALUE,\n        row_number() over (order by REC_TIMESTAMP desc) as REC_NUM\n    from \n        table(SNOWTRAIL_DEMO.INFORMATION_SCHEMA.SERVERLESS_ALERT_HISTORY(\n            DATE_RANGE_START => current_date - 14\n            ))\n    where\n        ALERT_NAME = 'NEW_ERRORS'\n    group by\n        REC_TIMESTAMP\n    ),\nSTATS as(\n    select\n        REC_NUM,\n        REC_TIMESTAMP,\n        REC_VALUE,\n        avg(REC_VALUE) over (order by REC_TIMESTAMP rows between 50 preceding and 1 preceding) as PREV_50_AVG,\n        stddev(REC_VALUE) over (order by REC_TIMESTAMP rows between 50 preceding and 1 preceding) as PREV_50_STDDEV,\n        abs(REC_VALUE - PREV_50_AVG) / nullif(PREV_50_STDDEV, 0) as Z_SCORE\n    from\n        RECORDS \n    )\nselect \n    REC_TIMESTAMP as HOUR_BUCKET,\n    REC_VALUE as CREDITS,\n    PREV_50_AVG,\n    Z_SCORE\nfrom \n    STATS\nwhere\n    abs(Z_SCORE) > 3\norder by\n    REC_TIMESTAMP desc\n;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7ca17f43-0225-4bd6-afbf-3be67140cd85",
   "metadata": {
    "name": "_1_1_Task_duration",
    "collapsed": false
   },
   "source": "# 1. Task anomalies\n \n## 1.1 Task run duration anomaly\n\nFor each Task in the selected Database that ran successfully we get the durations from the avaiable previous (max) 50 runs. \nThen calculate the average and standard deviation for each Task.\nThen we compare the duration of each new Task run to the standard deviation of its previous runs and return the name if it is an outlier (Z-score over 3)."
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "sql",
    "name": "create_Task_duration_anomaly_alert"
   },
   "source": "create or replace alert SNOWTRAIL_DEMO.OBSERV.TASKS_RUN_DURATION_ANOMALY\n--- no warehouse selected to run serverless\nschedule = '360 minutes' \ncomment = 'Duration outliers in this database'\nif (exists(\n        with\n        RUN_HISTORY as(\n            select\n                NAME,\n                SCHEMA_NAME,\n                DATABASE_NAME,\n                SCHEDULED_TIME,\n                timediff(seconds, QUERY_START_TIME, COMPLETED_TIME) as DURATION,\n                avg(DURATION) over (partition by SCHEMA_NAME, NAME order by SCHEDULED_TIME rows between 50 preceding and 1 preceding) as PREV_50_AVG,\n                stddev(DURATION) over (partition by SCHEMA_NAME, NAME order by SCHEDULED_TIME rows between 50 preceding and 1 preceding) as PREV_50_STDDEV,\n                abs(DURATION - PREV_50_AVG) / nullif(PREV_50_STDDEV, 0) as Z_SCORE\n            from\n                table(SNOWTRAIL_DEMO.INFORMATION_SCHEMA.TASK_HISTORY(\n                        SCHEDULED_TIME_RANGE_START => (\n                            coalesce(\n                                SNOWFLAKE.ALERT.LAST_SUCCESSFUL_SCHEDULED_TIME(),\n                                timeadd('DAY', -7, current_timestamp))                              -- if last check is beyond history retention period then use last week instead\n                            ),     \n                        SCHEDULED_TIME_RANGE_END => SNOWFLAKE.ALERT.SCHEDULED_TIME(),               -- considering only past runs\n                        RESULT_LIMIT => 10000))\n            where\n                SCHEMA_NAME is not null     --- ignoring nested Tasks\n                and STATE = 'SUCCEEDED'\n        )        \n        select\n            NAME as TASK_NAME,\n            concat(DATABASE_NAME,'.',SCHEMA_NAME) as DB_SCHEMA,\n            SCHEDULED_TIME,\n            DURATION AS DURATION_IN_S,\n            PREV_50_AVG\n        from\n            RUN_HISTORY\n        where\n            Z_SCORE > 3                 -- threshold for outliers\n        order by\n            SCHEDULED_TIME desc\n          )\n    )\n    \nthen\n    begin\n        let TASK_DURATION_ANOMALIES resultset := (\n            select * from table(result_scan(SNOWFLAKE.ALERT.GET_CONDITION_QUERY_UUID())));      -- get query ID from condition\n        \n        for RECORD in TASK_DURATION_ANOMALIES do    \n            let MESSAGE string := ('Task '||RECORD.TASK_NAME||' in '||RECORD.DB_SCHEMA||' ran for '||RECORD.DURATION_IN_S||' compared to an avg runtime of '||RECORD.PREV_50_AVG||'.');\n              \n            let WARN_MESSAGE string := ('{\"state\":\"ANOMALY_DETECTED\", \"message\":\"'||:MESSAGE||'\"} ');  -- add state to json string\n            \n            select SNOWTRAIL_DEMO.OBSERV.WARN_LOG(:WARN_MESSAGE);       -- using custom logger function from notebook 3\n        end for;\n    end;\n;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4ba392bc-566c-403a-b6cc-0ed50b3a0727",
   "metadata": {
    "language": "sql",
    "name": "resume_task_duration_alert"
   },
   "outputs": [],
   "source": "alter alert SNOWTRAIL_DEMO.OBSERV.TASKS_RUN_DURATION_ANOMALY resume;\n\nexecute alert SNOWTRAIL_DEMO.OBSERV.TASKS_RUN_DURATION_ANOMALY;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37fd6efb-da9f-41ae-be86-37fe73a5df72",
   "metadata": {
    "name": "_1_2_Task_frequency",
    "collapsed": false
   },
   "source": "## 1.2. Task run frequency anomalies\n\nSimilar to the first example for each Task that ran successfully we now take the time since the previous run and compare it to the previous (max) 50 runs. \nThen calculate the average and standard deviation for each Task.\nThen we compare the time diff of each new Task run to the standard deviation of its previous runs and return the name if it is an outlier (Z-score over 3)."
  },
  {
   "cell_type": "code",
   "id": "2e61d6fd-34c4-4153-8fd8-17ae6f4840ff",
   "metadata": {
    "language": "sql",
    "name": "create_task_frequency_alert"
   },
   "outputs": [],
   "source": "create or replace alert SNOWTRAIL_DEMO.OBSERV.TASKS_RUN_FREQUENCY_ANOMALY\n--- no warehouse selected to run serverless\nschedule = '360 minutes'\ncomment = 'Frequency outliers in this database'\nif (exists(\n        with\n        DELTA as(\n            select\n                NAME,\n                SCHEMA_NAME,\n                DATABASE_NAME,\n                QUERY_START_TIME,\n                lead(QUERY_START_TIME) over (partition by SCHEMA_NAME, NAME order by QUERY_START_TIME) as PREV_START_TIME\n            from\n                table(SNOWTRAIL_DEMO.INFORMATION_SCHEMA.TASK_HISTORY(\n                        SCHEDULED_TIME_RANGE_START => (\n                            coalesce(\n                                SNOWFLAKE.ALERT.LAST_SUCCESSFUL_SCHEDULED_TIME(),\n                                timeadd('DAY', -7, current_timestamp))                              -- if last check is beyond history retention period then use last week instead\n                            ),  \n                        SCHEDULED_TIME_RANGE_END => SNOWFLAKE.ALERT.SCHEDULED_TIME(),                   -- considering only past runs\n                        RESULT_LIMIT => 10000))\n            where\n                SCHEMA_NAME is not null     --- ignoring nested Tasks\n                and STATE = 'SUCCEEDED'\n        ),\n        RUN_HISTORY as(\n            select\n                NAME,\n                SCHEMA_NAME,\n                QUERY_START_TIME,\n                timediff(seconds, QUERY_START_TIME, PREV_START_TIME) as START_TIME_DELTA,\n                avg(START_TIME_DELTA) over (partition by SCHEMA_NAME, NAME order by QUERY_START_TIME rows between 50 preceding and 1 preceding) as PREV_50_AVG,\n                stddev(START_TIME_DELTA) over (partition by SCHEMA_NAME, NAME order by QUERY_START_TIME rows between 50 preceding and 1 preceding) as PREV_50_STDDEV,\n                abs(START_TIME_DELTA - PREV_50_AVG) / nullif(PREV_50_STDDEV, 0) as Z_SCORE\n            from\n                DELTA\n        )        \n        select\n            NAME as TASK_NAME,\n            SCHEMA_NAME,\n            -- QUERY_START_TIME,\n            START_TIME_DELTA AS START_TIME_DELTA_IN_S,\n            PREV_50_AVG,\n            -- Z_SCORE\n        from\n            RUN_HISTORY\n        where\n            Z_SCORE > 3       -- threshold for outliers\n        order by\n            QUERY_START_TIME desc\n          )\n    )\n    \nthen   \n    begin\n        let TASK_FREQUENCY_ANOMALIES resultset := (\n            select * from table(result_scan(SNOWFLAKE.ALERT.GET_CONDITION_QUERY_UUID())));      -- get query ID from condition\n        \n        for RECORD in TASK_FREQUENCY_ANOMALIES do    \n            let MESSAGE string := ('Task '||RECORD.TASK_NAME||' in '||RECORD.SCHEMA_NAME||' did NOT run for '||RECORD.START_TIME_DELTA_IN_S||' compared to an avg frequency of '||RECORD.PREV_50_AVG||'.');\n              \n            let WARN_MESSAGE string := ('{\"state\":\"ANOMALY_DETECTED\", \"message\":\"'||:MESSAGE||'\"} ');  -- add state to json string\n            \n            select SNOWTRAIL_DEMO.OBSERV.WARN_LOG(:WARN_MESSAGE);       -- using custom logger function from notebook 3\n        end for;\n    end;\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73e142ae-5caa-4c85-946c-d29a3f8d70f9",
   "metadata": {
    "language": "sql",
    "name": "resume_task_frequency_alert"
   },
   "outputs": [],
   "source": "alter alert SNOWTRAIL_DEMO.OBSERV.TASKS_RUN_FREQUENCY_ANOMALY resume;\n\nexecute alert SNOWTRAIL_DEMO.OBSERV.TASKS_RUN_FREQUENCY_ANOMALY;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae1a40dc-63af-4ae2-b81c-6e5e97a161af",
   "metadata": {
    "name": "_2_1_Pipe_frequency_anomalies",
    "collapsed": false
   },
   "source": "# 2. Pipe Anomalies\n\n## 2.1. Pipe copy frequency anomalies\n\nIn a similar way we can take the timestamp of each successfully copy from a defined Pipe. \nThen we take the time since the previous copy and compare it to gaps between previous copies. \nThen calculate the average and standard deviation and compare the time diff of each new Copy to the standard deviation of the previous gaps and return the timestamp if it is an outlier (Z-score over 3)."
  },
  {
   "cell_type": "code",
   "id": "dda24a4c-0274-4053-a959-53186c2f7139",
   "metadata": {
    "language": "sql",
    "name": "create_pipe_frequency_alert"
   },
   "outputs": [],
   "source": "create or replace alert SNOWTRAIL_DEMO.OBSERV.PIPE_COPY_FREQUENCY_ANOMALY\n--- no warehouse selected to run serverless\nschedule = '360 minutes'\ncomment = 'Frequency outliers for Pipe LOAD_STEADY_WEATHER'\nif (exists(\n        with\n        DELTA as(\n            select\n                PIPE_NAME,\n                PIPE_SCHEMA_NAME,\n                PIPE_CATALOG_NAME,\n                LAST_LOAD_TIME,\n                lead(LAST_LOAD_TIME) over (order by LAST_LOAD_TIME) as PREV_LOAD_TIME\n            from\n                table(SNOWTRAIL_DEMO.INFORMATION_SCHEMA.COPY_HISTORY(\n                        TABLE_NAME => 'SNOWTRAIL_DEMO.PIPELINE.IMPORTED_WEATHER',       -- select name of Pipe target table\n                        START_TIME => (\n                            coalesce(\n                                SNOWFLAKE.ALERT.LAST_SUCCESSFUL_SCHEDULED_TIME(),\n                                timeadd('DAY', -7, current_timestamp))                  -- if last check is beyond history retention period then use last week instead\n                            )  \n                        ))\n            where\n                PIPE_NAME = 'LOAD_DAILY_WEATHER'                                        -- select name of a Pipe\n        ),\n        COPY_HISTORY as(\n            select\n                PIPE_NAME,\n                PIPE_SCHEMA_NAME,\n                PIPE_CATALOG_NAME,\n                LAST_LOAD_TIME,\n                timediff(seconds, LAST_LOAD_TIME, PREV_LOAD_TIME) as TIME_SINCE_LAST_LOAD,\n                avg(TIME_SINCE_LAST_LOAD) over (order by LAST_LOAD_TIME rows between 50 preceding and 1 preceding) as PREV_50_AVG,\n                stddev(TIME_SINCE_LAST_LOAD) over (order by LAST_LOAD_TIME rows between 50 preceding and 1 preceding) as PREV_50_STDDEV,\n                abs(TIME_SINCE_LAST_LOAD - PREV_50_AVG) / nullif(PREV_50_STDDEV, 0) as Z_SCORE\n            from\n                DELTA\n        )        \n        select\n            PIPE_NAME,\n            concat(PIPE_CATALOG_NAME,'.',PIPE_SCHEMA_NAME) as DB_SCHEMA,\n            -- LAST_LOAD_TIME,\n            TIME_SINCE_LAST_LOAD AS TIME_SINCE_LAST_LOAD_IN_S,\n            PREV_50_AVG,\n            -- Z_SCORE\n        from\n            COPY_HISTORY\n        where\n            Z_SCORE > 3       -- threshold for outliers\n        order by\n            LAST_LOAD_TIME desc\n          )\n    )\n    \nthen\n    begin\n        let COPY_FREQUENCY_ANOMALIES resultset := (\n            select * from table(result_scan(SNOWFLAKE.ALERT.GET_CONDITION_QUERY_UUID())));      -- get query ID from condition\n        \n        for RECORD in COPY_FREQUENCY_ANOMALIES do    \n            let MESSAGE string := ('Pipe '||RECORD.PIPE_NAME||' in '||RECORD.DB_SCHEMA||' did NOT load now data for '||RECORD.TIME_SINCE_LAST_LOAD_IN_S||' compared to an avg frequency of '||RECORD.PREV_50_AVG||'.');\n              \n            let WARN_MESSAGE string := ('{\"state\":\"ANOMALY_DETECTED\", \"message\":\"'||:MESSAGE||'\"} ');  -- add state to json string\n            \n            select SNOWTRAIL_DEMO.OBSERV.WARN_LOG(:WARN_MESSAGE);       -- using custom logger function from notebook 3\n        end for;\n    end;\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a327d42-7f53-4d6d-ba71-dd924cfe5087",
   "metadata": {
    "language": "sql",
    "name": "resume_pipe_frequency_alert"
   },
   "outputs": [],
   "source": "alter alert SNOWTRAIL_DEMO.OBSERV.PIPE_COPY_FREQUENCY_ANOMALY resume;\n\nexecute alert SNOWTRAIL_DEMO.OBSERV.PIPE_COPY_FREQUENCY_ANOMALY;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "de2384e7-be87-4c5e-8c9d-814e9ce8b321",
   "metadata": {
    "name": "_2_2_Pipe_row_count_anomaly",
    "collapsed": false
   },
   "source": "## 2.2. Pipe ingestion row count anomaly\n\nSimilar to the ingestion frequency we can now check for row count anomalies from our Pipe: "
  },
  {
   "cell_type": "code",
   "id": "38d3cee3-5fd0-4f6c-a1e3-38276af5e0bf",
   "metadata": {
    "language": "sql",
    "name": "create_pipe_row_count_anomaly_alert"
   },
   "outputs": [],
   "source": "create or replace alert SNOWTRAIL_DEMO.OBSERV.PIPE_ROW_COUNT_ANOMALY\n--- no warehouse selected to run serverless\nschedule = '360 minutes'\ncomment = 'Row count outliers for Pipe LOAD_STEADY_WEATHER'\nif (exists(\n        with \n        COPY_HISTORY as (\n            select\n                PIPE_NAME,\n                CATALOG_NAME,\n                SCHEMA_NAME,\n                LAST_LOAD_TIME,\n                ROW_COUNT as ROWS_COPIED,\n                avg(ROWS_COPIED) over (order by LAST_LOAD_TIME rows between 50 preceding and 1 preceding) as PREV_50_AVG,\n                stddev(ROWS_COPIED) over (order by LAST_LOAD_TIME rows between 50 preceding and 1 preceding) as PREV_50_STDDEV,\n                abs(ROWS_COPIED - PREV_50_AVG) / nullif(PREV_50_STDDEV, 0) as Z_SCORE\n            from\n                table(SNOWTRAIL_DEMO.INFORMATION_SCHEMA.COPY_HISTORY(\n                        TABLE_NAME => 'SNOWTRAIL_DEMO.PIPELINE.IMPORTED_WEATHER',       -- select name of Pipe target table\n                        START_TIME => (\n                            coalesce(\n                                SNOWFLAKE.ALERT.LAST_SUCCESSFUL_SCHEDULED_TIME(),\n                                timeadd('DAY', -7, current_timestamp))                  -- if last check is beyond history retention period then use last week instead\n                            )  \n                        ))\n            where\n                PIPE_NAME = 'LOAD_DAILY_WEATHER'                                        -- select name of a Pipe\n        )\n        select\n            PIPE_NAME,\n            concat(CATALOG_NAME,'.',SCHEMA_NAME) as DB_SCHEMA,\n            LAST_LOAD_TIME,\n            ROWS_COPIED,\n            PREV_50_AVG,\n            -- Z_SCORE\n        from\n            COPY_HISTORY\n        where\n            Z_SCORE > 3       -- threshold for outliers\n          )\n    )\n    \nthen\n    begin\n        let COPY_ROWS_ANOMALIES resultset := (\n            select * from table(result_scan(SNOWFLAKE.ALERT.GET_CONDITION_QUERY_UUID())));      -- get query ID from condition\n        \n        for RECORD in COPY_ROWS_ANOMALIES do    \n            let MESSAGE string := ('Pipe '||RECORD.PIPE_NAME||' in '||RECORD.DB_SCHEMA||' loaded a file with '||RECORD.ROWS_COPIED||' rows compared to an avg row-count of '||RECORD.PREV_50_AVG||'.');\n              \n            let WARN_MESSAGE string := ('{\"state\":\"ANOMALY_DETECTED\", \"message\":\"'||:MESSAGE||'\"} ');  -- add state to json string\n            \n            select SNOWTRAIL_DEMO.OBSERV.WARN_LOG(:WARN_MESSAGE);       -- using custom logger function from notebook 3\n        end for;\n    end;\n;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9ea1403a-fd56-460e-9353-649e435b5077",
   "metadata": {
    "name": "note_copy_history_limitation",
    "collapsed": false
   },
   "source": "⚠️ Note that INFORMATION_SCHEMA.COPY_HISTORY() requires a target table as argument. So we can not just get all Pipe copies from one query like we can for Task runs and Dynamic Table refreshes.\nIf we want to set up one Alert convering row-count anomalies for all Pipes in our database we would have to look up the target tables for each Pipe and add a loop to our query."
  },
  {
   "cell_type": "code",
   "id": "ef729cdb-d5b7-4c91-aa9f-7f6c4c7d2a50",
   "metadata": {
    "language": "sql",
    "name": "resume_pipe_row_count_anomaly_alert"
   },
   "outputs": [],
   "source": "alter alert SNOWTRAIL_DEMO.OBSERV.PIPE_ROW_COUNT_ANOMALY resume;\n\nexecute alert SNOWTRAIL_DEMO.OBSERV.PIPE_ROW_COUNT_ANOMALY;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c7f7913-a160-41bc-9ce9-41e375060550",
   "metadata": {
    "name": "_3_1_Dynamic_Table_row_changes",
    "collapsed": false
   },
   "source": "# 3. Dynamic Tables Anomalies\n\n## 3.1. Dynamic Table refresh row-change anomaly\n\nSimilar to the row count check for Pipe ingestions we can do the same for Dynamic Table refreshes:"
  },
  {
   "cell_type": "code",
   "id": "d4110ba1-f5b0-4136-9a18-ba289d3fade0",
   "metadata": {
    "language": "sql",
    "name": "create_dynamic_table_row_change_anomaly_alert"
   },
   "outputs": [],
   "source": "create or replace alert SNOWTRAIL_DEMO.OBSERV.DT_REFRESH_ROW_COUNT_ANOMALY\n--- no warehouse selected to run serverless\nschedule = '360 minutes'\nif (exists(\n        with \n        REFRESH_HISTORY as (\n            select \n                NAME as DT_NAME,\n                DATABASE_NAME,\n                SCHEMA_NAME,\n                concat(DATABASE_NAME,'.',SCHEMA_NAME,'.',NAME) as FULL_NAME,\n                REFRESH_START_TIME, \n                STATISTICS:numCopiedRows as UPDATED_ROWS,\n                avg(UPDATED_ROWS) over (partition by FULL_NAME order by REFRESH_START_TIME rows between 50 preceding and 1 preceding) as PREV_50_AVG,\n                stddev(UPDATED_ROWS) over (partition by FULL_NAME order by REFRESH_START_TIME rows between 50 preceding and 1 preceding) as PREV_50_STDDEV,\n                abs(UPDATED_ROWS - PREV_50_AVG) / nullif(PREV_50_STDDEV, 0) as Z_SCORE\n            from \n                table(SNOWTRAIL_DEMO.INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n                        DATA_TIMESTAMP_START => (\n                            coalesce(\n                                SNOWFLAKE.ALERT.LAST_SUCCESSFUL_SCHEDULED_TIME(),\n                                timeadd('DAY', -7, current_timestamp))                  -- if last check is beyond history retention period then use last week instead\n                            ), \n                        DATA_TIMESTAMP_END => SNOWFLAKE.ALERT.SCHEDULED_TIME(),\n                        RESULT_LIMIT => 10000\n                    )) \n            where \n                UPDATED_ROWS > 0\n                and REFRESH_TRIGGER = 'SCHEDULED'\n            order by\n                DATA_TIMESTAMP desc\n        )\n        select\n            DT_NAME,\n            concat(DATABASE_NAME,'.',SCHEMA_NAME) as DB_SCHEMA,\n            REFRESH_START_TIME,\n            UPDATED_ROWS,\n            PREV_50_AVG,\n            Z_SCORE\n        from\n            REFRESH_HISTORY\n        where\n            Z_SCORE > 3       -- threshold for outliers\n        )\n    )\nthen\n    begin\n        let REFRESH_ROWS_ANOMALIES resultset := (\n            select * from table(result_scan(SNOWFLAKE.ALERT.GET_CONDITION_QUERY_UUID())));      -- get query ID from condition\n        \n        for RECORD in REFRESH_ROWS_ANOMALIES do    \n            let MESSAGE string := ('Dynamic Table '||RECORD.DT_NAME||' in '||RECORD.DB_SCHEMA||' refreshed with '||RECORD.UPDATED_ROWS||' rows changed compared to an avg of '||RECORD.PREV_50_AVG||' rows.');\n              \n            let WARN_MESSAGE string := ('{\"state\":\"ANOMALY_DETECTED\", \"message\":\"'||:MESSAGE||'\"} ');  -- add state to json string\n            \n            select SNOWTRAIL_DEMO.OBSERV.WARN_LOG(:WARN_MESSAGE);       -- using custom logger function from notebook 3\n        end for;\n    end;\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ab73597-6b5f-4b57-b0d2-964e06c7697e",
   "metadata": {
    "language": "sql",
    "name": "resume_dynamic_table_row_change_anomaly_alert"
   },
   "outputs": [],
   "source": "alter alert SNOWTRAIL_DEMO.OBSERV.DT_REFRESH_ROW_COUNT_ANOMALY resume;\n\nexecute alert SNOWTRAIL_DEMO.OBSERV.DT_REFRESH_ROW_COUNT_ANOMALY;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21542962-57bc-4ec4-a92b-114f44f16dbc",
   "metadata": {
    "name": "_note_on_cost",
    "collapsed": false
   },
   "source": "## Balancing Cost and Latency\n\nThe examples above should serve you as templates for your own pipelines and projects. Keep in mind that they are all scoped to a specific Database. You can also expand them to the entire account by querying SNOWFLAKE.INFORMATION_SCHEMA or reduce the scope to a specific Schema.\nAlso think about what a good schedule would be for your projects. You can run checks ever minute, every hour, every day - depending on the throughput of your pipelines and your business needs. \n\nTo keep an eye on you cost and then find your balance you can check **INFORMATION_SCHEMA.SERVERLESS_ALERT_HISTORY**:\n\n...or set up an anomaly detection Alert on your Alert spent 🙃"
  },
  {
   "cell_type": "code",
   "id": "26b91e10-1623-4a15-be47-285c3052ab79",
   "metadata": {
    "language": "python",
    "name": "check_alert_cost"
   },
   "outputs": [],
   "source": "import streamlit as st\nimport pandas as pd\nimport altair as alt\nsession = get_active_session()\n\nst.header('Serverless Alerts Costs')\n\nSERVERLESS_CREDITS = session.sql(\"\"\"\n                select\n                    ALERT_NAME,\n                    to_date(START_TIME) as DS,\n                    sum(CREDITS_USED) as CREDITS_SPENT\n                from \n                    table(SNOWTRAIL_DEMO.INFORMATION_SCHEMA.SERVERLESS_ALERT_HISTORY(\n                        DATE_RANGE_START => current_date - 7\n                    ))\n                group by \n                    ALERT_NAME,\n                    DS\n                \"\"\").to_pandas()\n\nCHART = alt.Chart(SERVERLESS_CREDITS).mark_bar(size=30).encode(\n        x=alt.X('DS:T', axis=alt.Axis(title= None)), \n        y=alt.Y('CREDITS_SPENT:Q', axis=alt.Axis(title='CREDITS')), \n        color=alt.Color('ALERT_NAME:N')\n        ).properties(height=360, width=720)\n\nst.altair_chart(CHART)",
   "execution_count": null
  }
 ]
}