{"metadata":{"kernelspec":{"display_name":"Streamlit Notebook","name":"streamlit"},"colab":{"provenance":[]}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":["#**RAG based Contextual Search Engine for Molecular Drug Discovery using Snowflake Artic LLM , Snowpark and Langchain**\n","\n"],"metadata":{"id":"woZXuFofZeqJ"},"id":"woZXuFofZeqJ"},{"cell_type":"markdown","source":["We explore how to build a contextual search engine tailored for molecular drug discovery using Retrieval-Augmented Generation (RAG) and Langchain. We’ll leverage the power of the “snowflake-arctic-embed-m” embedding model and Snowflake’s Arctic LLM to create a system that provides precise, context-rich search results, enhancing the efficiency of drug research and discovery."],"metadata":{"id":"zEdfryWnZaHg"},"id":"zEdfryWnZaHg"},{"cell_type":"code","id":"3775908f-ca36-4846-8f38-5adca39217f2","metadata":{"language":"python","name":"cell1","id":"3775908f-ca36-4846-8f38-5adca39217f2"},"source":["# Import python packages\n","import streamlit as st\n","from snowflake.snowpark.context import get_active_session\n","session = get_active_session()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"0f0cdf9c-7b8f-4b46-b595-a087b1ccff04","metadata":{"name":"cell13","collapsed":false,"id":"0f0cdf9c-7b8f-4b46-b595-a087b1ccff04"},"source":["Using Langchain, SnowPark and PyPDF2, we will create a function to extract text from the PDFs. We have chunk size of 256 with chunk overlap of 50"]},{"cell_type":"markdown","source":["##**Data Preparation**\n","To build a contextual search engine, data preparation is critical, especially in the domain of molecular drug discovery, where documents like research papers, reports, and datasets are often in PDF format. This section walks through the process of preparing and chunking PDFs for embedding and search using Snowflake’s Snowpark, Langchain, and embedding models."],"metadata":{"id":"sD4F33T8ZUDH"},"id":"sD4F33T8ZUDH"},{"cell_type":"markdown","source":["##Step 1: Chunking the PDF Data\n","The following Python UDF (User-Defined Function) extracts text from PDFs, chunks the content into manageable pieces, and returns them for embedding. Here’s how it works:\n","\n","Reading the PDF: The function read_pdf reads the PDF file using PyPDF2, extracting text from each page.\n","Chunking the Text: The RecursiveCharacterTextSplitter from Langchain is used to split the text into smaller chunks, ensuring some overlap between chunks to preserve contextual continuity. Parameters like chunk_size (256 characters) and chunk_overlap (50 characters) help maintain context within chunks."],"metadata":{"id":"LEgCSygDZt_V"},"id":"LEgCSygDZt_V"},{"cell_type":"code","id":"8d50cbf4-0c8d-4950-86cb-114990437ac9","metadata":{"language":"sql","name":"cell2","codeCollapsed":false,"id":"8d50cbf4-0c8d-4950-86cb-114990437ac9"},"source":["create or replace function pdf_text_chunker(file_url string)\n","returns table (chunk varchar)\n","language python\n","runtime_version = '3.9'\n","handler = 'pdf_text_chunker'\n","packages = ('snowflake-snowpark-python','PyPDF2', 'langchain')\n","as\n","$$\n","from snowflake.snowpark.types import StringType, StructField, StructType\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from snowflake.snowpark.files import SnowflakeFile\n","import PyPDF2, io\n","import logging\n","import pandas as pd\n","\n","class pdf_text_chunker:\n","\n","    def read_pdf(self, file_url: str) -> str:\n","\n","        logger = logging.getLogger(\"udf_logger\")\n","        logger.info(f\"Opening file {file_url}\")\n","\n","        with SnowflakeFile.open(file_url, 'rb') as f:\n","            buffer = io.BytesIO(f.readall())\n","\n","        reader = PyPDF2.PdfReader(buffer)\n","        text = \"\"\n","        for page in reader.pages:\n","            try:\n","                text += page.extract_text().replace('\\n', ' ').replace('\\0', ' ')\n","            except:\n","                text = \"Unable to Extract\"\n","                logger.warn(f\"Unable to extract from file {file_url}, page {page}\")\n","\n","        return text\n","\n","    def process(self,file_url: str):\n","\n","        text = self.read_pdf(file_url)\n","\n","        text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size = 256 , #Adjust this as you see fit\n","            chunk_overlap  = 50, #This let's text have some form of overlap. Useful for keeping chunks contextual\n","            length_function = len\n","        )\n","\n","        chunks = text_splitter.split_text(text)\n","        df = pd.DataFrame(chunks, columns=['chunks'])\n","\n","        yield from df.itertuples(index=False, name=None)\n","$$;"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Step 2: Creating the Stage\n","The data is staged for processing using Snowflake. A stage is created to securely store and organize the PDF files."],"metadata":{"id":"gDL_toxjZy9G"},"id":"gDL_toxjZy9G"},{"cell_type":"code","id":"c695373e-ac74-4b62-a1f1-08206cbd5c81","metadata":{"language":"sql","name":"cell3","codeCollapsed":false,"id":"c695373e-ac74-4b62-a1f1-08206cbd5c81"},"source":["create or replace stage docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8a62077f-8240-4109-8d92-1cd6f5c1b698","metadata":{"name":"cell14","collapsed":false,"id":"8a62077f-8240-4109-8d92-1cd6f5c1b698"},"source":["Now, once the stage is created, we will upload the file in the stage. We will upload a PDFs of this document - https://www.ema.europa.eu/en/documents/assessment-report/vazkepa-epar-public-assessment-report_en.pdf"]},{"cell_type":"markdown","id":"698f25a1-3bc0-49b1-9899-0a6f985986d7","metadata":{"name":"cell15","collapsed":false,"id":"698f25a1-3bc0-49b1-9899-0a6f985986d7"},"source":["It has information about Vazkepa which can be used to treat CVD(Cardiovascular diseases). This is a 129 pg long document but using RAG, we can do Accelerated Drug Development"]},{"cell_type":"markdown","source":["##Step 3: Storing Chunked Data in a Table\n","Once the PDF files are chunked, the data is inserted into the DOCS_CHUNKS_TABLE:\n","\n","Chunking Text: Text chunks are stored in the CHUNK column.\n","Embedding the Chunks: The SNOWFLAKE.CORTEX.EMBED_TEXT_768 function is used to generate embeddings for each chunk using the \"snowflake-arctic-embed-m\" model.\n","Metadata: Additional columns such as file_url, relative_path, and scoped_file_url help store metadata for each PDF file.\n","The document chunks are uploaded and inserted into the table with their embeddings:"],"metadata":{"id":"omkowxF2Z28h"},"id":"omkowxF2Z28h"},{"cell_type":"code","id":"97674c7e-a180-4c3e-abc8-90d6aeb79d9c","metadata":{"language":"sql","name":"cell4","codeCollapsed":false,"id":"97674c7e-a180-4c3e-abc8-90d6aeb79d9c"},"outputs":[],"source":["create or replace TABLE DOCS_CHUNKS_TABLE (\n","    RELATIVE_PATH VARCHAR(16777216), -- Relative path to the PDF file\n","    SIZE NUMBER(38,0), -- Size of the PDF\n","    FILE_URL VARCHAR(16777216), -- URL for the PDF\n","    SCOPED_FILE_URL VARCHAR(16777216), -- Scoped url (you can choose which one to keep depending on your use case)\n","    CHUNK VARCHAR(16777216), -- Piece of text\n","    CHUNK_VEC VECTOR(FLOAT, 768) );  -- Embedding using the VECTOR data type\n","\n","insert into docs_chunks_table (relative_path, size, file_url,\n","                            scoped_file_url, chunk, chunk_vec)\n","    select relative_path,\n","            size,\n","            file_url,\n","            build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n","            func.chunk as chunk,\n","            SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m',chunk) as chunk_vec\n","    from\n","        directory(@docs),\n","        TABLE(pdf_text_chunker(build_scoped_file_url(@docs, relative_path))) as func;"],"execution_count":null},{"cell_type":"markdown","id":"e5cb6a92-ed21-4382-9343-3cffa5d32caa","metadata":{"name":"cell16","collapsed":false,"id":"e5cb6a92-ed21-4382-9343-3cffa5d32caa"},"source":["Let's query to see how our Vector DB looks like"]},{"cell_type":"code","id":"7aac0561-9df5-4179-990f-c1abbd969190","metadata":{"language":"sql","name":"cell5","codeCollapsed":false,"id":"7aac0561-9df5-4179-990f-c1abbd969190"},"outputs":[],"source":["select relative_path, size, chunk, chunk_vec from docs_chunks_table limit 5;"],"execution_count":null},{"cell_type":"code","id":"e483a7f1-2afb-43ed-aeca-9c30da012205","metadata":{"language":"python","name":"cell6","codeCollapsed":false,"id":"e483a7f1-2afb-43ed-aeca-9c30da012205"},"outputs":[],"source":["import pandas as pd\n","pd.set_option(\"max_colwidth\",None)\n","\n","num_chunks = 10"],"execution_count":null},{"cell_type":"markdown","id":"70669faa-0e3b-47e2-ae08-bee3b17158ba","metadata":{"name":"cell17","collapsed":false,"id":"70669faa-0e3b-47e2-ae08-bee3b17158ba"},"source":["##Retrieval and Augmentation\n","In the search engine, documents are retrieved by calculating the cosine similarity between the query’s embedding and pre-stored document chunks in the docs_chunks_table. This ranks the chunks based on relevance, and the top results are selected.\n","\n","The RAG (Retrieval-Augmented Generation) model uses these relevant chunks as context to generate a more informed, accurate response. By using the “snowflake-arctic-embed-m” model for embeddings, and the Snowflake’s Arctic LLM for generating answers, the system enhances the relevance of retrieved information.\n","\n","We are using Arctic-Embed to create 768 Dimensional Embeddings and would retreive the top 20 results"]},{"cell_type":"code","id":"656ee2cb-3147-4a27-88a6-1e39cffd5044","metadata":{"language":"python","name":"cell7","codeCollapsed":false,"id":"656ee2cb-3147-4a27-88a6-1e39cffd5044"},"outputs":[],"source":["def create_prompt (myquestion):\n","\n","    cmd = \"\"\"\n","     with results as\n","     (SELECT RELATIVE_PATH,\n","       VECTOR_COSINE_SIMILARITY(docs_chunks_table.chunk_vec,\n","                SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m', ?)) as similarity,\n","       chunk\n","     from docs_chunks_table\n","     order by similarity desc\n","     limit ?)\n","     select chunk, relative_path from results\n","     \"\"\"\n","\n","    df_context = session.sql(cmd, params=[myquestion, num_chunks]).to_pandas()\n","\n","    context_lenght = len(df_context) -1\n","\n","    prompt_context = \"\"\n","    for i in range (0, context_lenght):\n","        prompt_context += df_context._get_value(i, 'CHUNK')\n","\n","    prompt_context = prompt_context.replace(\"'\", \"\")\n","    relative_path =  df_context._get_value(0,'RELATIVE_PATH')\n","\n","    prompt = f\"\"\"\n","      'You are an expert assistance extracting information from context provided.\n","       Answer the question based on the context. Be concise and do not hallucinate.\n","      Context: {prompt_context}\n","      Question:\n","       {myquestion}\n","       Answer: '\n","       \"\"\"\n","\n","    return prompt"],"execution_count":null},{"cell_type":"markdown","id":"81e55da9-8cba-4fe3-b905-50a6cd41e812","metadata":{"name":"cell18","collapsed":false,"id":"81e55da9-8cba-4fe3-b905-50a6cd41e812"},"source":["We will then get the final response using Snowflake-Arctic LLMs"]},{"cell_type":"code","id":"7820b968-f754-4879-bfe6-f494f1724529","metadata":{"language":"python","name":"cell8","codeCollapsed":false,"id":"7820b968-f754-4879-bfe6-f494f1724529"},"outputs":[],"source":["from snowflake.snowpark import Session\n","from snowflake.cortex import Complete\n","\n","def getResponse(myquestion):\n","\n","    prompt = create_prompt (myquestion)\n","    response = Complete(\n","        \"snowflake-arctic\",\n","        prompt,\n","        session=session)\n","\n","    return response"],"execution_count":null},{"cell_type":"code","id":"de8fd2cf-09ee-4d34-b48a-2f79c08f0c9e","metadata":{"language":"python","name":"cell9","codeCollapsed":false,"id":"de8fd2cf-09ee-4d34-b48a-2f79c08f0c9e"},"outputs":[],"source":["def display_response (question):\n","    response = getResponse(question)\n","    st.markdown(response)"],"execution_count":null},{"cell_type":"markdown","source":["##Streamlit App"],"metadata":{"id":"C6HZeVP4U4Sw"},"id":"C6HZeVP4U4Sw"},{"cell_type":"markdown","id":"4ba1106c-7647-4d08-8bdb-2ecb9fc6b3ab","metadata":{"name":"cell19","collapsed":false,"id":"4ba1106c-7647-4d08-8bdb-2ecb9fc6b3ab"},"source":["An App based on Streamlit :) We can get response within seconds"]},{"cell_type":"code","id":"0d87d869-9430-4776-8a79-92f0c682ec92","metadata":{"language":"python","name":"cell10","codeCollapsed":false,"id":"0d87d869-9430-4776-8a79-92f0c682ec92"},"outputs":[],"source":["st.title(\"Contextual Search Engine for Drug Discover\")\n","\n","question = st.text_input(\"Enter question\", placeholder=\"What is the Molecular Weights?\", label_visibility=\"collapsed\")\n","\n","if question:\n","    display_response (question)"],"execution_count":null},{"cell_type":"code","id":"43c8fad5-9862-4d21-a945-13ee83fc3616","metadata":{"language":"python","name":"cell11","codeCollapsed":false,"id":"43c8fad5-9862-4d21-a945-13ee83fc3616"},"outputs":[],"source":["st.title(\"Contextual Search Engine for Drug Discover\")\n","\n","question = st.text_input(\"Enter question\", placeholder=\"What is the Molecular Weights?\", label_visibility=\"collapsed\")\n","\n","if question:\n","    display_response (question)"],"execution_count":null},{"cell_type":"code","id":"5cab3833-6b52-45a1-a42f-2c0a44250be0","metadata":{"language":"python","name":"cell12","codeCollapsed":false,"id":"5cab3833-6b52-45a1-a42f-2c0a44250be0"},"outputs":[],"source":[],"execution_count":null}]}