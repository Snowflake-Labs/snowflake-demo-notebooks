{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f31c73-2e0d-4889-9f11-e150b1b04f4d",
   "metadata": {
    "name": "Intro",
    "collapsed": false
   },
   "source": "# GPU Based XGBoost Training\n## In the following notebook we will leverage Snowpark Container Services (SPCS) to run a notebook within Snowflake on a series of GPUs\n\n### * Workflow* \n- Inspect GPU resources available - for this exercise we will use four NVIDIA A10G GPUs\n- Load in data from Snowflake table\n- Set up data for modeling\n- Train two XGBoost models - one trained with CPUs and one leveraging our GPU cluster\n- Compare runtimes and results of our models\n\n\n### * Key Takeaways* \n- SPCS allows users to run notebook workloads that execute on containers, rather than virtual warehouses in Snowflake\n- GPUs can greatly speed up model training jobs ðŸ”¥\n- Bringing in third party python libraries offers flexibility to leverage great contirbutions to the OSS ecosystem\n\n\n### Note - In order to successfully run !pip installs make sure you have enabled the external access integration with pypi\n- Do so by clicking on the drop down of the ðŸŸ¢ Active kernel settings button, clicking Edit Compute Settings, then turning on the PYPI_ACCESS_INTEGRATION radio button in the external access tab"
  },
  {
   "cell_type": "code",
   "id": "7fb3d295-20a0-47ec-b41c-1762beaf5a92",
   "metadata": {
    "language": "python",
    "name": "pip_install"
   },
   "outputs": [],
   "source": "!pip install seaborn",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports",
    "codeCollapsed": false,
    "collapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport sys\nimport seaborn\n\n# Snowpark ML\nfrom snowflake.ml.modeling.xgboost import XGBRegressor, XGBClassifier\nfrom snowflake.ml._internal.utils import identifier\n\n# Snowpark session\nfrom snowflake.snowpark import DataFrame\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\nsession",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "gpu_check",
    "codeCollapsed": false,
    "collapsed": false
   },
   "source": "import torch\n\n# Get the list of GPUs\nif torch.cuda.is_available():\n    # Get the number of GPUs\n    num_gpus = torch.cuda.device_count()\n\n    print(f'{num_gpus} GPU Device(s) Found')\n    # Print the list of GPUs\n    for i in range(num_gpus):\n        print(\"Name:\", torch.cuda.get_device_name(i), \"  Index:\", i)\nelse:\n    print(\"No GPU available\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "load_data",
    "codeCollapsed": false,
    "collapsed": false
   },
   "source": "#Load in data from Snowflake table into a Snowpark dataframe\ntable = \"XGB_GPU_DATABASE.XGB_GPU_SCHEMA.VEHICLES_TABLE\"\ndf = session.table(table)\ndf.count(), len(df.columns)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b6991bfd-9939-4637-9891-5f6a700332dd",
   "metadata": {
    "language": "python",
    "name": "sales_price_descriptive_statistics",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#Note the maximum price - a $3B car must be quite a spectacle, but we don't want to use that for our model\ndf.select('PRICE').describe()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e95fea0-54cf-4194-b56c-a608561bc6d4",
   "metadata": {
    "language": "python",
    "name": "filter_to_100k",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#Lets filter down to cars $100k or less - note that we only filter out ~1% of our data here\ndf = df.filter(col('PRICE')<100000)\ndf.select('PRICE').describe()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0911e6e-c9cf-4d73-9f00-1c04b568ef49",
   "metadata": {
    "language": "python",
    "name": "schema",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "#View data schema\nlist(df.schema)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab207b33-dcb4-432f-b3e3-e1b284ab0dfa",
   "metadata": {
    "language": "python",
    "name": "drop_columns",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "#Drop some columns that won't be helpful for modeling\ndrop_cols = [\"ID\",\"URL\", \"REGION_URL\", \"IMAGE_URL\", \"DESCRIPTION\", \"VIN\", \"POSTING_DATE\", 'COUNTY']\ndf = df.drop(drop_cols)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7028e36-11e1-492a-8711-500a3bd7dee4",
   "metadata": {
    "language": "python",
    "name": "fill_null",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "#Fill NULL values with \"NA\" for string columns and 0 for numerical columns\nstring_type = df.select('REGION').schema[0].datatype\nstring_cols = df.select([col.name for col in df.schema if col.datatype ==string_type]).columns\nnon_string_cols = df.drop(string_cols).columns\n\ndf = df.fillna(\"NA\", subset=string_cols)\ndf = df.fillna(0, subset= non_string_cols)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb410942-555c-4bf4-9145-1e20b6e53f7c",
   "metadata": {
    "language": "python",
    "name": "infrequent_category_casting",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "#Use pandas to find the top 1000 car models and cast any model values to 'INFREQUENT' to avoid excessive dimensionality\ndf_pd = df.to_pandas()\ntop_n_models = df_pd.MODEL.value_counts().keys()[0:1000]\ndf_pd['MODEL'] = df_pd.MODEL.apply(lambda x: x if x in top_n_models else 'INFREQUENT')\ndf = session.create_dataframe(df_pd)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ea0b207-0f32-46eb-ae7c-1e2cb4089693",
   "metadata": {
    "language": "python",
    "name": "increase_data_size",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "#Union the data to itself a few times to go from 400k rows to 1.7M rows. This lab's purpose is to test performance so we want to have a decently large dataset!\nfor i in range(1,3):\n    df = df.unionAll(df)\n\ndf.count()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a86dfd16-2fab-435a-8d84-008251951c30",
   "metadata": {
    "language": "python",
    "name": "one_hot_encoding",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "import snowflake.ml.modeling.preprocessing as snowml\n\nOHE_COLS = string_cols\nOHE_POST_COLS = [i+\"_OHE\" for i in OHE_COLS]\n\n\n# Encode categoricals to numeric columns\nsnowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols = OHE_COLS, drop_input_cols=True)\ntransformed_df = snowml_ohe.fit(df).transform(df)\ntransformed_df.columns",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "024c25ed-6cc3-414b-8360-e0b9651ef98c",
   "metadata": {
    "language": "python",
    "name": "rename_columns",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#Rename columns to avoid issues with \" characters later on\n\n#Create dict replacing bad column names\nrenaming_dict = {}\nfor n, col in enumerate(transformed_df.columns):\n    double_quote_spot = col.find('\"')\n    if double_quote_spot==0:\n        renaming_dict[col] = col[double_quote_spot+1:col.find(\"_\")]+f\"__{n}\"\n    else:\n        renaming_dict[col] = col\n\n\n#Create new df with renamed and sorted columns\ndf_renamed = transformed_df.rename(renaming_dict)\ndf_renamed = df_renamed.select(sorted(df_renamed.columns))\ndf_renamed.columns[0:20]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e49a0391-a323-4920-b0a4-48976091f299",
   "metadata": {
    "language": "python",
    "name": "train_test_split",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Split the data into train and test sets (note this may take up to 3-4 minutes)\ntrain, test = df_renamed.random_split(weights=[0.8, 0.2], seed=0)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "40833ad7-521a-49df-afce-b7046693f685",
   "metadata": {
    "name": "model_training_md",
    "collapsed": false
   },
   "source": "## Model Training\n\n### Now that our data is all set up - we will train a CPU-based and GPU-based Snowpark Optimized XGBoost model\n#### The parameter that instructs our model to leverage GPUs is *tree_method*. \n--- When *tree_method* is set to *hist* the model will not attempt to use GPUs\n\n--- When *tree_method* is set to *gpu_hist* the model will leverage any available GPUs found\n\n--- Snowflake offers the ability to leverage multi-GPU training (i.e. using all 4 of our A10G GPUs we have available) for optimized performance"
  },
  {
   "cell_type": "code",
   "id": "e9999894-82c7-4812-8aba-0f6c465461f1",
   "metadata": {
    "language": "python",
    "name": "model_definition",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "#Train both a CPU and GPU based XGB Regressor - note that we are using n_estimators=1000 to intentionally make this a more compute intensive training job\n\n\ncpu_snowpark_xgb = XGBRegressor(\n    input_cols=train.drop(\"PRICE\").columns,\n    label_cols=train.select(\"PRICE\").columns,\n    output_cols=\"PREDICTED_PRICE\",\n    tree_method=\"hist\",\n    predictor= \"cpu_predictor\",\n    n_estimators=1000\n)\n\n\n\ngpu_snowpark_xgb = XGBRegressor(\n    input_cols=train.drop(\"PRICE\").columns,\n    label_cols=train.select(\"PRICE\").columns,\n    output_cols=\"PREDICTED_PRICE\",\n    tree_method=\"gpu_hist\",\n    predictor= \"gpu_predictor\",\n    n_estimators=1000\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ef8e5b7-ee5b-45ef-a1b5-0106e3f7ee55",
   "metadata": {
    "language": "python",
    "name": "gc",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "#Clear cache to make sure we have as much free memory as possible for modeling\n\nimport gc\n\ngc.collect()\n\ntorch.cuda.empty_cache()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ba65ac8-3dd2-4d35-9e6f-daa5f205aec6",
   "metadata": {
    "name": "model_training_details",
    "collapsed": false
   },
   "source": "## While the model is training, you can see a live look at resource utilization by hovering your mouse over the ðŸŸ¢ Active button that controls the kernel settings for your notebook.\n### Notice both the memory and CPU utilziation for the cpu training job, and the GPU utilization for the GPU training job"
  },
  {
   "cell_type": "code",
   "id": "0c4f1cec-7d7e-4367-87c7-e9ce86041ecb",
   "metadata": {
    "language": "python",
    "name": "cpu_train",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "import time\nstart_time = time.time()\ncpu_snowpark_xgb.fit(train)\nend_time = time.time()\nprint(\"TRAINING TIME:\", end_time - start_time)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c26aa31-304b-4e41-8b89-24fb544aacb3",
   "metadata": {
    "language": "python",
    "name": "gpu_train",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "import time\nstart_time = time.time()\ngpu_snowpark_xgb.fit(train)\nend_time = time.time()\nprint(\"TRAINING TIME:\", end_time - start_time)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f2c0874e-67be-43fd-bbd1-1b7385d4bd99",
   "metadata": {
    "name": "model_training_takeaways",
    "collapsed": false
   },
   "source": "## While results aren't entirely determinstic, you should have seen a 3-4x speedup in model training from CPU to GPU training. \n### Investigate in the logs from the two above cells where you see the message *[RayXGBoost] Finished XGBoost training* and look to the end of the line to see the pure training time for that model"
  },
  {
   "cell_type": "code",
   "id": "dee76334-33c1-4e18-b445-3ad8fc2016b4",
   "metadata": {
    "language": "python",
    "name": "cpu_predict",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "#Compute predictions on gtest set for cpu model\nimport time\nstart_time = time.time()\ncpu_test_preds = cpu_snowpark_xgb.predict(test)\nend_time = time.time()\nprint(\"Inference TIME:\", end_time - start_time)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e45d8089-b4ab-43a0-a202-39d621511a5e",
   "metadata": {
    "language": "python",
    "name": "gpu_predict",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "#Compute predictions on test set for gpu model\n\nimport time\n\nstart_time = time.time()\ngpu_test_preds = gpu_snowpark_xgb.predict(test)\nend_time = time.time()\nprint(\"Inference TIME:\", end_time - start_time)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed9397bd-3510-4a07-ae2f-93e4cdc41ec5",
   "metadata": {
    "name": "final_steps_md",
    "collapsed": false
   },
   "source": "## Finally now that our models have been trained and predictions have been generated, we will carry out a few final steps\n- Compute performance metrics\n- Visualize predicted vs. actuals"
  },
  {
   "cell_type": "code",
   "id": "8ce52114-7abb-439c-bdfb-994550e516c4",
   "metadata": {
    "language": "python",
    "name": "compute_perf_metrics"
   },
   "outputs": [],
   "source": "import numpy as np\nfrom snowflake.ml.modeling.metrics import r2_score, mean_squared_error\nprint('R^2 Score:', r2_score(df=gpu_test_preds, y_true_col_name= 'PRICE', y_pred_col_name='PREDICTED_PRICE'))\nprint('RMSE:', np.sqrt(mean_squared_error(df=gpu_test_preds, y_true_col_names= 'PRICE', y_pred_col_names='PREDICTED_PRICE')))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3feb47a6-fd69-4524-bc78-716c3ff5118c",
   "metadata": {
    "language": "python",
    "name": "preds_vs_actuals_chart",
    "collapsed": false
   },
   "outputs": [],
   "source": "#In our visualization below we can see that outside of 0 (our filled NA value) there is a reasonably tight correlation between predicted and actual prices for cars \nimport seaborn as sns\n\nresults_df = gpu_test_preds.select(['PRICE', 'PREDICTED_PRICE']).to_pandas()\n\nsns.scatterplot(x=results_df.PRICE, y = results_df.PREDICTED_PRICE)",
   "execution_count": null
  }
 ]
}