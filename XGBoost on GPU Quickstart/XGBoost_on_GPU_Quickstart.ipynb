{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f31c73-2e0d-4889-9f11-e150b1b04f4d",
   "metadata": {
    "collapsed": false,
    "name": "Intro"
   },
   "source": [
    "# GPU Based XGBoost Training\n",
    "## In the following notebook we will leverage Snowpark Container Services (SPCS) to run a notebook within Snowflake on a series of GPUs\n",
    "\n",
    "### * Workflow* \n",
    "- Inspect GPU resources available - for this exercise we will use four NVIDIA A10G GPUs\n",
    "- Load in data from Snowflake table\n",
    "- Set up data for modeling\n",
    "- Train two XGBoost models - one trained with CPUs and one leveraging our GPU cluster\n",
    "- Compare runtimes and results of our models\n",
    "\n",
    "\n",
    "### * Key Takeaways* \n",
    "- SPCS allows users to run notebook workloads that execute on containers, rather than virtual warehouses in Snowflake\n",
    "- GPUs can greatly speed up model training jobs ðŸ”¥\n",
    "- Bringing in third party python libraries offers flexibility to leverage great contirbutions to the OSS ecosystem\n",
    "\n",
    "\n",
    "### Note - In order to successfully run !pip installs make sure you have enabled the external access integration with pypi\n",
    "- Do so by clicking on the drop down of the ðŸŸ¢ Active kernel settings button, clicking Edit Compute Settings, then turning on the PYPI_ACCESS_INTEGRATION radio button in the external access tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3d295-20a0-47ec-b41c-1762beaf5a92",
   "metadata": {
    "language": "python",
    "name": "pip_install"
   },
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import sys\n",
    "import seaborn\n",
    "\n",
    "# Snowpark ML\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor, XGBClassifier\n",
    "from snowflake.ml._internal.utils import identifier\n",
    "\n",
    "# Snowpark session\n",
    "from snowflake.snowpark import DataFrame\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "gpu_check"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get the list of GPUs\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    print(f'{num_gpus} GPU Device(s) Found')\n",
    "    # Print the list of GPUs\n",
    "    for i in range(num_gpus):\n",
    "        print(\"Name:\", torch.cuda.get_device_name(i), \"  Index:\", i)\n",
    "else:\n",
    "    print(\"No GPU available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "load_data"
   },
   "outputs": [],
   "source": [
    "#Load in data from Snowflake table into a Snowpark dataframe\n",
    "table = \"XGB_GPU_DATABASE.XGB_GPU_SCHEMA.VEHICLES_TABLE\"\n",
    "df = session.table(table)\n",
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6991bfd-9939-4637-9891-5f6a700332dd",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "sales_price_descriptive_statistics"
   },
   "outputs": [],
   "source": [
    "#Note the maximum price - a $3B car must be quite a spectacle, but we don't want to use that for our model\n",
    "df.select('PRICE').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95fea0-54cf-4194-b56c-a608561bc6d4",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "filter_to_100k"
   },
   "outputs": [],
   "source": [
    "#Lets filter down to cars $100k or less - note that we only filter out ~1% of our data here\n",
    "df = df.filter(col('PRICE')<100000)\n",
    "df.select('PRICE').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0911e6e-c9cf-4d73-9f00-1c04b568ef49",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "schema"
   },
   "outputs": [],
   "source": [
    "#View data schema\n",
    "list(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab207b33-dcb4-432f-b3e3-e1b284ab0dfa",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "drop_columns"
   },
   "outputs": [],
   "source": [
    "#Drop some columns that won't be helpful for modeling\n",
    "drop_cols = [\"ID\",\"URL\", \"REGION_URL\", \"IMAGE_URL\", \"DESCRIPTION\", \"VIN\", \"POSTING_DATE\", 'COUNTY']\n",
    "df = df.drop(drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7028e36-11e1-492a-8711-500a3bd7dee4",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "fill_null"
   },
   "outputs": [],
   "source": [
    "#Fill NULL values with \"NA\" for string columns and 0 for numerical columns\n",
    "string_type = df.select('REGION').schema[0].datatype\n",
    "string_cols = df.select([col.name for col in df.schema if col.datatype ==string_type]).columns\n",
    "non_string_cols = df.drop(string_cols).columns\n",
    "\n",
    "df = df.fillna(\"NA\", subset=string_cols)\n",
    "df = df.fillna(0, subset= non_string_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb410942-555c-4bf4-9145-1e20b6e53f7c",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "infrequent_category_casting"
   },
   "outputs": [],
   "source": [
    "#Use pandas to find the top 1000 car models and cast any model values to 'INFREQUENT' to avoid excessive dimensionality\n",
    "df_pd = df.to_pandas()\n",
    "top_n_models = df_pd.MODEL.value_counts().keys()[0:1000]\n",
    "df_pd['MODEL'] = df_pd.MODEL.apply(lambda x: x if x in top_n_models else 'INFREQUENT')\n",
    "df = session.create_dataframe(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0b207-0f32-46eb-ae7c-1e2cb4089693",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "increase_data_size"
   },
   "outputs": [],
   "source": [
    "#Union the data to itself a few times to go from 400k rows to 1.7M rows. This lab's purpose is to test performance so we want to have a decently large dataset!\n",
    "for i in range(1,3):\n",
    "    df = df.unionAll(df)\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86dfd16-2fab-435a-8d84-008251951c30",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "one_hot_encoding"
   },
   "outputs": [],
   "source": [
    "import snowflake.ml.modeling.preprocessing as snowml\n",
    "\n",
    "OHE_COLS = string_cols\n",
    "OHE_POST_COLS = [i+\"_OHE\" for i in OHE_COLS]\n",
    "\n",
    "\n",
    "# Encode categoricals to numeric columns\n",
    "snowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols = OHE_COLS, drop_input_cols=True)\n",
    "transformed_df = snowml_ohe.fit(df).transform(df)\n",
    "transformed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c25ed-6cc3-414b-8360-e0b9651ef98c",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "rename_columns"
   },
   "outputs": [],
   "source": [
    "#Rename columns to avoid issues with \" characters later on\n",
    "\n",
    "#Create dict replacing bad column names\n",
    "renaming_dict = {}\n",
    "for n, col in enumerate(transformed_df.columns):\n",
    "    double_quote_spot = col.find('\"')\n",
    "    if double_quote_spot==0:\n",
    "        renaming_dict[col] = col[double_quote_spot+1:col.find(\"_\")]+f\"__{n}\"\n",
    "    else:\n",
    "        renaming_dict[col] = col\n",
    "\n",
    "\n",
    "#Create new df with renamed and sorted columns\n",
    "df_renamed = transformed_df.rename(renaming_dict)\n",
    "df_renamed = df_renamed.select(sorted(df_renamed.columns))\n",
    "df_renamed.columns[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a0391-a323-4920-b0a4-48976091f299",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "train_test_split"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets (note this may take up to 3-4 minutes)\n",
    "train, test = df_renamed.random_split(weights=[0.95, 0.05], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40833ad7-521a-49df-afce-b7046693f685",
   "metadata": {
    "collapsed": false,
    "name": "model_training_md"
   },
   "source": [
    "## Model Training\n",
    "\n",
    "### Now that our data is all set up - we will train a CPU-based and GPU-based Snowpark Optimized XGBoost model\n",
    "#### The parameter that instructs our model to leverage GPUs is *tree_method*. \n",
    "--- When *tree_method* is set to *hist* the model will not attempt to use GPUs\n",
    "\n",
    "--- When *tree_method* is set to *gpu_hist* the model will leverage any available GPUs found\n",
    "\n",
    "--- Snowflake offers the ability to leverage multi-GPU training (i.e. using all 4 of our A10G GPUs we have available) for optimized performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9999894-82c7-4812-8aba-0f6c465461f1",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "model_definition"
   },
   "outputs": [],
   "source": [
    "#Train both a CPU and GPU based XGB Regressor - note that we are using n_estimators=1000 to intentionally make this a more compute intensive training job\n",
    "\n",
    "\n",
    "cpu_snowpark_xgb = XGBRegressor(\n",
    "    input_cols=train.drop(\"PRICE\").columns,\n",
    "    label_cols=train.select(\"PRICE\").columns,\n",
    "    output_cols=\"PREDICTED_PRICE\",\n",
    "    tree_method=\"hist\",\n",
    "    predictor= \"cpu_predictor\",\n",
    "    n_estimators=1000\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "gpu_snowpark_xgb = XGBRegressor(\n",
    "    input_cols=train.drop(\"PRICE\").columns,\n",
    "    label_cols=train.select(\"PRICE\").columns,\n",
    "    output_cols=\"PREDICTED_PRICE\",\n",
    "    tree_method=\"gpu_hist\",\n",
    "    predictor= \"gpu_predictor\",\n",
    "    n_estimators=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8e5b7-ee5b-45ef-a1b5-0106e3f7ee55",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "gc"
   },
   "outputs": [],
   "source": [
    "#Clear cache to make sure we have as much free memory as possible for modeling\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba65ac8-3dd2-4d35-9e6f-daa5f205aec6",
   "metadata": {
    "collapsed": false,
    "name": "model_training_details"
   },
   "source": [
    "## While the model is training, you can see a live look at resource utilization by hovering your mouse over the ðŸŸ¢ Active button that controls the kernel settings for your notebook.\n",
    "### Notice both the memory and CPU utilziation for the cpu training job, and the GPU utilization for the GPU training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f1cec-7d7e-4367-87c7-e9ce86041ecb",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cpu_train"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "cpu_snowpark_xgb.fit(train)\n",
    "end_time = time.time()\n",
    "print(\"TRAINING TIME:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26aa31-304b-4e41-8b89-24fb544aacb3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "gpu_train"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "gpu_snowpark_xgb.fit(train)\n",
    "end_time = time.time()\n",
    "print(\"TRAINING TIME:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0874e-67be-43fd-bbd1-1b7385d4bd99",
   "metadata": {
    "collapsed": false,
    "name": "model_training_takeaways"
   },
   "source": [
    "## While results aren't entirely determinstic, you should have seen a 3-4x speedup in model training from CPU to GPU training. \n",
    "### Investigate in the logs from the two above cells where you see the message *[RayXGBoost] Finished XGBoost training* and look to the end of the line to see the pure training time for that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee76334-33c1-4e18-b445-3ad8fc2016b4",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cpu_predict"
   },
   "outputs": [],
   "source": [
    "#Compute predictions on test set for cpu model\n",
    "import time\n",
    "start_time = time.time()\n",
    "cpu_test_preds = cpu_snowpark_xgb.predict(test)\n",
    "end_time = time.time()\n",
    "print(\"Inference TIME:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d8089-b4ab-43a0-a202-39d621511a5e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "gpu_predict"
   },
   "outputs": [],
   "source": [
    "#Compute predictions on test set for gpu model\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "gpu_test_preds = gpu_snowpark_xgb.predict(test)\n",
    "end_time = time.time()\n",
    "print(\"Inference TIME:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9397bd-3510-4a07-ae2f-93e4cdc41ec5",
   "metadata": {
    "collapsed": false,
    "name": "final_steps_md"
   },
   "source": [
    "## Finally now that our models have been trained and predictions have been generated, we will carry out a few final steps\n",
    "- Compute performance metrics\n",
    "- Visualize predicted vs. actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce52114-7abb-439c-bdfb-994550e516c4",
   "metadata": {
    "language": "python",
    "name": "compute_perf_metrics"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from snowflake.ml.modeling.metrics import r2_score, mean_squared_error\n",
    "print('R^2 Score:', r2_score(df=gpu_test_preds, y_true_col_name= 'PRICE', y_pred_col_name='PREDICTED_PRICE'))\n",
    "print('RMSE:', np.sqrt(mean_squared_error(df=gpu_test_preds, y_true_col_names= 'PRICE', y_pred_col_names='PREDICTED_PRICE')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb47a6-fd69-4524-bc78-716c3ff5118c",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "preds_vs_actuals_chart"
   },
   "outputs": [],
   "source": [
    "#In our visualization below we can see that outside of 0 (our filled NA value) there is a reasonably tight correlation between predicted and actual prices for cars \n",
    "import seaborn as sns\n",
    "\n",
    "results_df = gpu_test_preds.select(['PRICE', 'PREDICTED_PRICE']).to_pandas()\n",
    "\n",
    "sns.scatterplot(x=results_df.PRICE, y = results_df.PREDICTED_PRICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
