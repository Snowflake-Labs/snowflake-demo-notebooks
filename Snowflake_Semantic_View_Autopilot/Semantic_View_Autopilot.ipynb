{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a483a81-fc9f-4ce1-9235-9083103011c8",
   "metadata": {
    "collapsed": false,
    "name": "title"
   },
   "source": [
    "# Snowflake Semantic View Autopilot \n",
    "# Virtual Hands-On Lab\n",
    "\n",
    "In this hands-on lab, you'll learn how to use **Semantic View Autopilot** to automatically generate semantic views from existing BI artifacts like Tableau workbooks. You'll also explore how to query semantic views using **Standard SQL** and generate Tableau Data Source (.tds) files for seamless BI integration.\n",
    "\n",
    "## What You'll Build:\n",
    "- **Marketing Data Foundation**: Dimension tables (product, region, campaign, channel) and a fact table for campaign performance\n",
    "- **Automated Semantic View**: Use Autopilot to generate a semantic view from a Tableau workbook\n",
    "- **Standard SQL Queries**: Query semantic views using familiar ANSI-style SQL\n",
    "- **Tableau Integration**: Generate .tds files that connect Tableau directly to semantic views\n",
    "\n",
    "## Key Technologies:\n",
    "- Semantic View Autopilot\n",
    "- Standard SQL for Semantic Views\n",
    "- Snowflake Stored Procedures (Python)\n",
    "- Streamlit for interactive downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6a376-f43c-49e7-8f9f-ec616e96b204",
   "metadata": {
    "collapsed": false,
    "name": "details"
   },
   "source": [
    "## Step 1: Load The Base Campaign Data\n",
    "\n",
    "This section creates the database, schema, and loads sample marketing data that we'll use throughout this lab. The data model follows a star schema pattern commonly used in analytics:\n",
    "\n",
    "#### Dimension Tables\n",
    "| Table | Description |\n",
    "|-------|-------------|\n",
    "| `product_dim` | Product information including name, category, and vertical |\n",
    "| `region_dim` | Geographic regions for campaign targeting |\n",
    "| `campaign_dim` | Marketing campaigns with names and objectives |\n",
    "| `channel_dim` | Marketing channels (e.g., Facebook, Google, Email) |\n",
    "\n",
    "#### Fact Table\n",
    "| Table | Description |\n",
    "|-------|-------------|\n",
    "| `marketing_campaign_fact` | Campaign performance metrics including spend, leads generated, and impressions |\n",
    "\n",
    "> **Note**: This script takes approximately 1 minute to run. It creates the database `SVA_VHOL_DB`, loads data from a GitHub repository, and sets up the necessary infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f4fb42-f777-4a03-acc1-5e5757e6975c",
   "metadata": {
    "language": "sql",
    "name": "load_marketing_data"
   },
   "outputs": [],
   "source": [
    "-- =============================================================================\n",
    "-- This script borrows heavily from the Snowflake Intelligence end-to-end demo:\n",
    "-- https://github.com/NickAkincilar/Snowflake_AI_DEMO\n",
    "-- Expected runtime: ~1 minute\n",
    "-- =============================================================================\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- SETUP: Role, Database, and Schema\n",
    "-- -----------------------------------------------------------------------------\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "CREATE OR REPLACE DATABASE SVA_VHOL_DB;\n",
    "USE DATABASE SVA_VHOL_DB;\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS SVA_VHOL_SCHEMA;\n",
    "USE SCHEMA SVA_VHOL_SCHEMA;\n",
    "\n",
    "-- Optional: Grant public access\n",
    "GRANT USAGE ON DATABASE SVA_VHOL_DB TO ROLE PUBLIC;\n",
    "GRANT USAGE ON SCHEMA SVA_VHOL_DB.SVA_VHOL_SCHEMA TO ROLE PUBLIC;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- FILE FORMAT: CSV Configuration\n",
    "-- -----------------------------------------------------------------------------\n",
    "CREATE OR REPLACE FILE FORMAT CSV_FORMAT\n",
    "    TYPE                         = 'CSV'\n",
    "    FIELD_DELIMITER              = ','\n",
    "    RECORD_DELIMITER             = '\\n'\n",
    "    SKIP_HEADER                  = 1\n",
    "    FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "    TRIM_SPACE                   = TRUE\n",
    "    ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE\n",
    "    ESCAPE                       = 'NONE'\n",
    "    ESCAPE_UNENCLOSED_FIELD      = '\\134'\n",
    "    DATE_FORMAT                  = 'YYYY-MM-DD'\n",
    "    TIMESTAMP_FORMAT             = 'YYYY-MM-DD HH24:MI:SS'\n",
    "    NULL_IF                      = ('NULL', 'null', '', 'N/A', 'n/a');\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- GIT INTEGRATION: Connect to GitHub Repository\n",
    "-- -----------------------------------------------------------------------------\n",
    "CREATE OR REPLACE API INTEGRATION git_api_integration\n",
    "    API_PROVIDER       = git_https_api\n",
    "    API_ALLOWED_PREFIXES = ('https://github.com/NickAkincilar/')\n",
    "    ENABLED            = TRUE;\n",
    "\n",
    "CREATE OR REPLACE GIT REPOSITORY SVA_VHOL_REPO\n",
    "    API_INTEGRATION = git_api_integration\n",
    "    ORIGIN          = 'https://github.com/NickAkincilar/Snowflake_AI_DEMO.git';\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- STAGE: Internal Storage for Data Files\n",
    "-- -----------------------------------------------------------------------------\n",
    "CREATE OR REPLACE STAGE INTERNAL_DATA_STAGE\n",
    "    FILE_FORMAT = CSV_FORMAT\n",
    "    COMMENT     = 'Internal stage for copied demo data files'\n",
    "    DIRECTORY   = (ENABLE = TRUE)\n",
    "    ENCRYPTION  = (TYPE = 'SNOWFLAKE_SSE');\n",
    "\n",
    "ALTER GIT REPOSITORY SVA_VHOL_REPO FETCH;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- COPY DATA FROM GIT TO INTERNAL STAGE\n",
    "-- -----------------------------------------------------------------------------\n",
    "COPY FILES\n",
    "    INTO @INTERNAL_DATA_STAGE/demo_data/\n",
    "    FROM @SVA_VHOL_REPO/branches/main/demo_data/;\n",
    "\n",
    "COPY FILES\n",
    "    INTO @INTERNAL_DATA_STAGE/unstructured_docs/\n",
    "    FROM @SVA_VHOL_REPO/branches/main/unstructured_docs/;\n",
    "\n",
    "LS @INTERNAL_DATA_STAGE;\n",
    "\n",
    "ALTER STAGE INTERNAL_DATA_STAGE REFRESH;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- DIMENSION TABLES\n",
    "-- -----------------------------------------------------------------------------\n",
    "\n",
    "-- Product Dimension\n",
    "CREATE OR REPLACE TABLE product_dim (\n",
    "    product_key   INT PRIMARY KEY,\n",
    "    product_name  VARCHAR(200) NOT NULL,\n",
    "    category_key  INT NOT NULL,\n",
    "    category_name VARCHAR(100),\n",
    "    vertical      VARCHAR(50)\n",
    ");\n",
    "\n",
    "-- Region Dimension\n",
    "CREATE OR REPLACE TABLE region_dim (\n",
    "    region_key  INT PRIMARY KEY,\n",
    "    region_name VARCHAR(100) NOT NULL\n",
    ");\n",
    "\n",
    "-- Campaign Dimension\n",
    "CREATE OR REPLACE TABLE campaign_dim (\n",
    "    campaign_key  INT PRIMARY KEY,\n",
    "    campaign_name VARCHAR(300) NOT NULL,\n",
    "    objective     VARCHAR(100)\n",
    ");\n",
    "\n",
    "-- Channel Dimension\n",
    "CREATE OR REPLACE TABLE channel_dim (\n",
    "    channel_key  INT PRIMARY KEY,\n",
    "    channel_name VARCHAR(100) NOT NULL\n",
    ");\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- FACT TABLE\n",
    "-- -----------------------------------------------------------------------------\n",
    "CREATE OR REPLACE TABLE marketing_campaign_fact (\n",
    "    campaign_fact_id INT PRIMARY KEY,\n",
    "    date             DATE NOT NULL,\n",
    "    campaign_key     INT NOT NULL,\n",
    "    product_key      INT NOT NULL,\n",
    "    channel_key      INT NOT NULL,\n",
    "    region_key       INT NOT NULL,\n",
    "    spend            DECIMAL(10,2) NOT NULL,\n",
    "    leads_generated  INT NOT NULL,\n",
    "    impressions      INT NOT NULL\n",
    ");\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- LOAD DIMENSION DATA\n",
    "-- -----------------------------------------------------------------------------\n",
    "COPY INTO product_dim\n",
    "    FROM @INTERNAL_DATA_STAGE/demo_data/product_dim.csv\n",
    "    FILE_FORMAT = CSV_FORMAT\n",
    "    ON_ERROR    = 'CONTINUE';\n",
    "\n",
    "COPY INTO region_dim\n",
    "    FROM @INTERNAL_DATA_STAGE/demo_data/region_dim.csv\n",
    "    FILE_FORMAT = CSV_FORMAT\n",
    "    ON_ERROR    = 'CONTINUE';\n",
    "\n",
    "COPY INTO campaign_dim\n",
    "    FROM @INTERNAL_DATA_STAGE/demo_data/campaign_dim.csv\n",
    "    FILE_FORMAT = CSV_FORMAT\n",
    "    ON_ERROR    = 'CONTINUE';\n",
    "\n",
    "COPY INTO channel_dim\n",
    "    FROM @INTERNAL_DATA_STAGE/demo_data/channel_dim.csv\n",
    "    FILE_FORMAT = CSV_FORMAT\n",
    "    ON_ERROR    = 'CONTINUE';\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- LOAD FACT DATA\n",
    "-- -----------------------------------------------------------------------------\n",
    "COPY INTO marketing_campaign_fact\n",
    "    FROM @INTERNAL_DATA_STAGE/demo_data/marketing_campaign_fact.csv\n",
    "    FILE_FORMAT = CSV_FORMAT\n",
    "    ON_ERROR    = 'CONTINUE';\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- VERIFICATION: Check Data Loads\n",
    "-- -----------------------------------------------------------------------------\n",
    "SHOW GIT REPOSITORIES;\n",
    "\n",
    "SELECT 'DIMENSION TABLES' AS category, '' AS table_name, NULL AS row_count\n",
    "UNION ALL SELECT '', 'product_dim',  COUNT(*) FROM product_dim\n",
    "UNION ALL SELECT '', 'campaign_dim', COUNT(*) FROM campaign_dim\n",
    "UNION ALL SELECT '', 'channel_dim',  COUNT(*) FROM channel_dim\n",
    "UNION ALL SELECT 'FACT TABLES', '', NULL\n",
    "UNION ALL SELECT '', 'marketing_campaign_fact', COUNT(*) FROM marketing_campaign_fact;\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b0ffea",
   "metadata": {},
   "source": [
    "## Step 2: Seed Queries for Semantic View Autopilot (Optional)\n",
    "\n",
    "The Semantic View Autopilot analyzes your **query history** to understand how your data is used in practice. By running these seed queries before creating your semantic view, you provide valuable context that helps Autopilot:\n",
    "\n",
    "- **Suggest model improvements**: Identify commonly used JOINs, aggregations, and filters\n",
    "- **Generate verified queries**: Pre-populate the semantic view with known-good query patterns\n",
    "- **Infer business logic**: Understand calculated metrics like cost-per-lead (CPL) and conversion rates\n",
    "\n",
    "### What These Queries Cover:\n",
    "1. **Overall marketing performance** by month\n",
    "2. **Channel efficiency analysis** (cost per lead by channel)\n",
    "3. **Campaign performance ranking** (top campaigns by leads)\n",
    "4. **Regional analysis** and budget allocation\n",
    "5. **Product conversion rates** and category analysis\n",
    "6. **Trend analysis** (week-over-week growth, anomaly detection)\n",
    "7. **Efficiency frontier** (best performing segments)\n",
    "\n",
    "> **Tip**: If you want to see Autopilot's suggestions in action, run this cell before creating your semantic view in the next step. Otherwise, skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f7b66-d637-4f7e-bb19-27beea1c0dca",
   "metadata": {
    "language": "sql",
    "name": "optional_seed_queris"
   },
   "outputs": [],
   "source": [
    "-- =============================================================================\n",
    "-- MARKETING SEMANTIC VIEW: Analytic Query Playbook\n",
    "-- These queries seed the Autopilot with common business patterns\n",
    "-- =============================================================================\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 1: Overall Marketing Performance by Month\n",
    "-- -----------------------------------------------------------------------------\n",
    "SELECT\n",
    "    DATE_TRUNC('month', mcf.date)                                   AS month,\n",
    "    SUM(mcf.spend)                                                  AS total_spend,\n",
    "    SUM(mcf.impressions)                                            AS total_impressions,\n",
    "    SUM(mcf.leads_generated)                                        AS total_leads,\n",
    "    SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0)            AS cost_per_lead,\n",
    "    SUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0)      AS lead_conversion_rate\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "GROUP BY 1\n",
    "ORDER BY 1;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 2: Channel Efficiency Analysis\n",
    "-- -----------------------------------------------------------------------------\n",
    "SELECT\n",
    "    cd.channel_name,\n",
    "    SUM(mcf.spend)                                        AS total_spend,\n",
    "    SUM(mcf.leads_generated)                              AS total_leads,\n",
    "    SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0)  AS cost_per_lead\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "    JOIN CHANNEL_DIM cd ON mcf.channel_key = cd.channel_key\n",
    "GROUP BY 1\n",
    "HAVING SUM(mcf.leads_generated) > 0\n",
    "ORDER BY cost_per_lead ASC, total_leads DESC;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 3: Channel Trend (Month-over-Month)\n",
    "-- -----------------------------------------------------------------------------\n",
    "SELECT\n",
    "    DATE_TRUNC('month', mcf.date)                                   AS month,\n",
    "    cd.channel_name,\n",
    "    SUM(mcf.spend)                                                  AS total_spend,\n",
    "    SUM(mcf.impressions)                                            AS total_impressions,\n",
    "    SUM(mcf.leads_generated)                                        AS total_leads,\n",
    "    SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0)            AS cpl,\n",
    "    SUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0)      AS conv_rate\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "    JOIN CHANNEL_DIM cd ON mcf.channel_key = cd.channel_key\n",
    "GROUP BY 1, 2\n",
    "ORDER BY 1, 2;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 4: Top Campaigns by Leads\n",
    "-- -----------------------------------------------------------------------------\n",
    "SELECT\n",
    "    c.campaign_name,\n",
    "    c.objective,\n",
    "    SUM(mcf.leads_generated)                                        AS total_leads,\n",
    "    SUM(mcf.spend)                                                  AS total_spend,\n",
    "    SUM(mcf.impressions)                                            AS total_impressions,\n",
    "    SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0)            AS cpl,\n",
    "    SUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0)      AS conv_rate\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "    JOIN CAMPAIGN_DIM c ON mcf.campaign_key = c.campaign_key\n",
    "GROUP BY 1, 2\n",
    "ORDER BY total_leads DESC\n",
    "LIMIT 20;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 5: Objectives Performance\n",
    "-- -----------------------------------------------------------------------------\n",
    "SELECT\n",
    "    c.objective,\n",
    "    SUM(mcf.spend)                                                  AS total_spend,\n",
    "    SUM(mcf.impressions)                                            AS total_impressions,\n",
    "    SUM(mcf.leads_generated)                                        AS total_leads,\n",
    "    SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0)            AS cpl,\n",
    "    SUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0)      AS conv_rate\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "    JOIN CAMPAIGN_DIM c ON mcf.campaign_key = c.campaign_key\n",
    "GROUP BY 1\n",
    "ORDER BY cpl ASC;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 6: Performance by Region\n",
    "-- -----------------------------------------------------------------------------\n",
    "WITH totals AS (\n",
    "    SELECT \n",
    "        SUM(spend)           AS all_spend, \n",
    "        SUM(leads_generated) AS all_leads \n",
    "    FROM MARKETING_CAMPAIGN_FACT\n",
    ")\n",
    "SELECT\n",
    "    rd.region_name,\n",
    "    SUM(mcf.spend)                                        AS spend,\n",
    "    SUM(mcf.leads_generated)                              AS leads,\n",
    "    SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0)  AS cpl,\n",
    "    SUM(mcf.spend) / NULLIF(t.all_spend, 0)               AS spend_share,\n",
    "    SUM(mcf.leads_generated) / NULLIF(t.all_leads, 0)     AS leads_share\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "    JOIN REGION_DIM rd ON mcf.region_key = rd.region_key\n",
    "    CROSS JOIN totals t\n",
    "GROUP BY 1, t.all_spend, t.all_leads\n",
    "ORDER BY leads DESC;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 7: Products with Best Conversion\n",
    "-- -----------------------------------------------------------------------------\n",
    "SELECT\n",
    "    pd.product_name,\n",
    "    pd.category_name,\n",
    "    pd.vertical,\n",
    "    SUM(mcf.impressions)                                            AS impressions,\n",
    "    SUM(mcf.leads_generated)                                        AS leads,\n",
    "    SUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0)      AS conv_rate\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "    JOIN PRODUCT_DIM pd ON mcf.product_key = pd.product_key\n",
    "GROUP BY 1, 2, 3\n",
    "HAVING SUM(mcf.impressions) > 0\n",
    "ORDER BY conv_rate DESC\n",
    "LIMIT 20;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 8: Category x Channel CPL Heatmap\n",
    "-- -----------------------------------------------------------------------------\n",
    "SELECT\n",
    "    pd.category_name,\n",
    "    cd.channel_name,\n",
    "    SUM(mcf.spend)                                        AS total_spend,\n",
    "    SUM(mcf.leads_generated)                              AS total_leads,\n",
    "    SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0)  AS cpl\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "    JOIN PRODUCT_DIM pd ON mcf.product_key = pd.product_key\n",
    "    JOIN CHANNEL_DIM cd ON mcf.channel_key = cd.channel_key\n",
    "GROUP BY 1, 2\n",
    "ORDER BY pd.category_name, cpl ASC;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 9: Vertical by Region Leads\n",
    "-- -----------------------------------------------------------------------------\n",
    "SELECT\n",
    "    pd.vertical,\n",
    "    rd.region_name,\n",
    "    SUM(mcf.leads_generated)                              AS total_leads,\n",
    "    SUM(mcf.spend)                                        AS total_spend,\n",
    "    SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0)  AS cpl\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "    JOIN PRODUCT_DIM pd ON mcf.product_key = pd.product_key\n",
    "    JOIN REGION_DIM rd ON mcf.region_key = rd.region_key\n",
    "GROUP BY 1, 2\n",
    "ORDER BY total_leads DESC;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 10: Worst CPL Segments\n",
    "-- -----------------------------------------------------------------------------\n",
    "SELECT\n",
    "    c.campaign_name,\n",
    "    cd.channel_name,\n",
    "    SUM(mcf.leads_generated)                              AS leads,\n",
    "    SUM(mcf.spend)                                        AS spend,\n",
    "    SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0)  AS cpl\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "    JOIN CAMPAIGN_DIM c ON mcf.campaign_key = c.campaign_key\n",
    "    JOIN CHANNEL_DIM cd ON mcf.channel_key = cd.channel_key\n",
    "GROUP BY 1, 2\n",
    "HAVING SUM(mcf.leads_generated) >= 50\n",
    "ORDER BY cpl DESC\n",
    "LIMIT 10;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 11: Budget Allocation by Objective and Channel\n",
    "-- -----------------------------------------------------------------------------\n",
    "WITH obj_totals AS (\n",
    "    SELECT\n",
    "        c.objective,\n",
    "        SUM(mcf.spend) AS objective_spend\n",
    "    FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "        JOIN CAMPAIGN_DIM c ON mcf.campaign_key = c.campaign_key\n",
    "    GROUP BY 1\n",
    ")\n",
    "SELECT\n",
    "    c.objective,\n",
    "    cd.channel_name,\n",
    "    SUM(mcf.spend)                                          AS spend,\n",
    "    SUM(mcf.spend) / NULLIF(ot.objective_spend, 0)          AS spend_share_within_objective\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "    JOIN CAMPAIGN_DIM c ON mcf.campaign_key = c.campaign_key\n",
    "    JOIN CHANNEL_DIM cd ON mcf.channel_key = cd.channel_key\n",
    "    JOIN obj_totals ot ON c.objective = ot.objective\n",
    "GROUP BY 1, 2, ot.objective_spend\n",
    "ORDER BY c.objective, spend_share_within_objective DESC;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 12: Facebook vs Non-Facebook Efficiency Over Time\n",
    "-- -----------------------------------------------------------------------------\n",
    "SELECT\n",
    "    DATE_TRUNC('month', mcf.date) AS month,\n",
    "    SUM(CASE WHEN cd.channel_name = 'Facebook'  THEN mcf.spend ELSE 0 END)           AS facebook_spend,\n",
    "    SUM(CASE WHEN cd.channel_name <> 'Facebook' THEN mcf.spend ELSE 0 END)           AS non_facebook_spend,\n",
    "    SUM(CASE WHEN cd.channel_name = 'Facebook'  THEN mcf.leads_generated ELSE 0 END) AS facebook_leads,\n",
    "    SUM(CASE WHEN cd.channel_name <> 'Facebook' THEN mcf.leads_generated ELSE 0 END) AS non_facebook_leads,\n",
    "    SUM(CASE WHEN cd.channel_name = 'Facebook'  THEN mcf.spend ELSE 0 END)\n",
    "        / NULLIF(SUM(CASE WHEN cd.channel_name = 'Facebook' THEN mcf.leads_generated ELSE 0 END), 0) \n",
    "                                                                                     AS facebook_cpl,\n",
    "    SUM(CASE WHEN cd.channel_name <> 'Facebook' THEN mcf.spend ELSE 0 END)\n",
    "        / NULLIF(SUM(CASE WHEN cd.channel_name <> 'Facebook' THEN mcf.leads_generated ELSE 0 END), 0) \n",
    "                                                                                     AS non_facebook_cpl\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "    JOIN CHANNEL_DIM cd ON mcf.channel_key = cd.channel_key\n",
    "GROUP BY 1\n",
    "ORDER BY 1;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 13: Campaign Flighting (Duration and Totals)\n",
    "-- -----------------------------------------------------------------------------\n",
    "SELECT\n",
    "    c.campaign_name,\n",
    "    c.objective,\n",
    "    MIN(mcf.date)                                                   AS first_active_date,\n",
    "    MAX(mcf.date)                                                   AS last_active_date,\n",
    "    DATEDIFF('day', MIN(mcf.date), MAX(mcf.date)) + 1               AS active_days,\n",
    "    SUM(mcf.spend)                                                  AS total_spend,\n",
    "    SUM(mcf.leads_generated)                                        AS total_leads,\n",
    "    SUM(mcf.impressions)                                            AS total_impressions,\n",
    "    SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0)            AS cpl,\n",
    "    SUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0)      AS conv_rate\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "    JOIN CAMPAIGN_DIM c ON mcf.campaign_key = c.campaign_key\n",
    "GROUP BY 1, 2\n",
    "ORDER BY last_active_date DESC, total_spend DESC;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 14: Week-over-Week Growth\n",
    "-- -----------------------------------------------------------------------------\n",
    "WITH weekly AS (\n",
    "    SELECT\n",
    "        DATE_TRUNC('week', date) AS week,\n",
    "        SUM(spend)               AS spend,\n",
    "        SUM(leads_generated)     AS leads\n",
    "    FROM MARKETING_CAMPAIGN_FACT\n",
    "    GROUP BY 1\n",
    ")\n",
    "SELECT\n",
    "    week,\n",
    "    spend,\n",
    "    leads,\n",
    "    (spend - LAG(spend) OVER (ORDER BY week)) / NULLIF(LAG(spend) OVER (ORDER BY week), 0) AS spend_wow_pct,\n",
    "    (leads - LAG(leads) OVER (ORDER BY week)) / NULLIF(LAG(leads) OVER (ORDER BY week), 0) AS leads_wow_pct\n",
    "FROM weekly\n",
    "ORDER BY week;\n",
    "\n",
    "--- Query 15: Best â€˜scaledâ€™ segments (lowest CPL with high lead volume, region x channel)\n",
    "--- SQL:\n",
    "\n",
    "SELECT\n",
    "rd.region_name,\n",
    "cd.channel_name,\n",
    "SUM(mcf.leads_generated) AS leads,\n",
    "SUM(mcf.spend) AS spend,\n",
    "SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0) AS cpl\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "JOIN REGION_DIM rd\n",
    "ON mcf.region_key = rd.region_key\n",
    "JOIN CHANNEL_DIM cd\n",
    "ON mcf.channel_key = cd.channel_key\n",
    "GROUP BY 1, 2\n",
    "HAVING SUM(mcf.leads_generated) >= 100\n",
    "ORDER BY cpl ASC, leads DESC\n",
    "LIMIT 20;\n",
    "\n",
    "--- Query 16: Objective mix by region (what are we running where?)\n",
    "--- SQL:\n",
    "\n",
    "SELECT\n",
    "rd.region_name,\n",
    "c.objective,\n",
    "SUM(mcf.spend) AS spend,\n",
    "SUM(mcf.leads_generated) AS leads,\n",
    "SUM(mcf.impressions) AS impressions,\n",
    "SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0) AS cpl,\n",
    "SUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0) AS conv_rate\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "JOIN REGION_DIM rd\n",
    "ON mcf.region_key = rd.region_key\n",
    "JOIN CAMPAIGN_DIM c\n",
    "ON mcf.campaign_key = c.campaign_key\n",
    "GROUP BY 1, 2\n",
    "ORDER BY rd.region_name, spend DESC;\n",
    "\n",
    "--- Query 17: Pareto view (cumulative leads vs cumulative spend by campaign)\n",
    "--- SQL:\n",
    "\n",
    "WITH by_campaign AS (\n",
    "SELECT\n",
    "c.campaign_name,\n",
    "SUM(mcf.leads_generated) AS leads,\n",
    "SUM(mcf.spend) AS spend\n",
    "FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "JOIN CAMPAIGN_DIM c\n",
    "ON mcf.campaign_key = c.campaign_key\n",
    "GROUP BY 1\n",
    "),\n",
    "ranked AS (\n",
    "SELECT\n",
    "*,\n",
    "SUM(leads) OVER () AS total_leads,\n",
    "SUM(spend) OVER () AS total_spend,\n",
    "SUM(leads) OVER (ORDER BY leads DESC) AS cum_leads,\n",
    "SUM(spend) OVER (ORDER BY leads DESC) AS cum_spend\n",
    "FROM by_campaign\n",
    ")\n",
    "SELECT\n",
    "campaign_name,\n",
    "leads,\n",
    "spend,\n",
    "cum_leads / NULLIF(total_leads, 0) AS cumulative_leads_share,\n",
    "cum_spend / NULLIF(total_spend, 0) AS cumulative_spend_share\n",
    "FROM ranked\n",
    "ORDER BY leads DESC;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 18: Anomaly Detection (High CPL Days via Z-Score)\n",
    "-- -----------------------------------------------------------------------------\n",
    "WITH daily AS (\n",
    "    SELECT\n",
    "        date,\n",
    "        SUM(spend)                                        AS spend,\n",
    "        SUM(leads_generated)                              AS leads,\n",
    "        SUM(spend) / NULLIF(SUM(leads_generated), 0)      AS cpl\n",
    "    FROM MARKETING_CAMPAIGN_FACT\n",
    "    GROUP BY 1\n",
    "),\n",
    "stats AS (\n",
    "    SELECT\n",
    "        AVG(cpl)         AS avg_cpl,\n",
    "        STDDEV_SAMP(cpl) AS std_cpl\n",
    "    FROM daily\n",
    "    WHERE cpl IS NOT NULL\n",
    "),\n",
    "scored AS (\n",
    "    SELECT\n",
    "        d.date,\n",
    "        d.spend,\n",
    "        d.leads,\n",
    "        d.cpl,\n",
    "        (d.cpl - s.avg_cpl) / NULLIF(s.std_cpl, 0) AS cpl_zscore\n",
    "    FROM daily d\n",
    "        CROSS JOIN stats s\n",
    "    WHERE d.cpl IS NOT NULL\n",
    ")\n",
    "SELECT *\n",
    "FROM scored\n",
    "WHERE ABS(cpl_zscore) >= 2\n",
    "ORDER BY ABS(cpl_zscore) DESC, date DESC;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 19: Diminishing Returns Curve (CPL by Spend Bucket)\n",
    "-- -----------------------------------------------------------------------------\n",
    "WITH daily AS (\n",
    "    SELECT\n",
    "        date,\n",
    "        SUM(spend)           AS spend,\n",
    "        SUM(leads_generated) AS leads\n",
    "    FROM MARKETING_CAMPAIGN_FACT\n",
    "    GROUP BY 1\n",
    "),\n",
    "bucketed AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        WIDTH_BUCKET(spend, 0, (SELECT MAX(spend) FROM daily), 10) AS spend_bucket\n",
    "    FROM daily\n",
    ")\n",
    "SELECT\n",
    "    spend_bucket,\n",
    "    MIN(spend)                         AS min_spend_in_bucket,\n",
    "    MAX(spend)                         AS max_spend_in_bucket,\n",
    "    SUM(spend)                         AS total_spend,\n",
    "    SUM(leads)                         AS total_leads,\n",
    "    SUM(spend) / NULLIF(SUM(leads), 0) AS cpl\n",
    "FROM bucketed\n",
    "GROUP BY 1\n",
    "ORDER BY 1;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- Query 20: Efficiency Frontier (Top 5 Campaigns per Channel)\n",
    "-- -----------------------------------------------------------------------------\n",
    "WITH campaign_channel AS (\n",
    "    SELECT\n",
    "        cd.channel_name,\n",
    "        c.campaign_name,\n",
    "        SUM(mcf.spend)                                        AS spend,\n",
    "        SUM(mcf.leads_generated)                              AS leads,\n",
    "        SUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0)  AS cpl\n",
    "    FROM MARKETING_CAMPAIGN_FACT mcf\n",
    "        JOIN CHANNEL_DIM cd ON mcf.channel_key = cd.channel_key\n",
    "        JOIN CAMPAIGN_DIM c ON mcf.campaign_key = c.campaign_key\n",
    "    GROUP BY 1, 2\n",
    "    HAVING SUM(mcf.leads_generated) >= 50\n",
    "),\n",
    "ranked AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY channel_name\n",
    "            ORDER BY cpl ASC NULLS LAST, leads DESC\n",
    "        ) AS rn\n",
    "    FROM campaign_channel\n",
    ")\n",
    "SELECT\n",
    "    channel_name,\n",
    "    campaign_name,\n",
    "    spend,\n",
    "    leads,\n",
    "    cpl\n",
    "FROM ranked\n",
    "WHERE rn <= 5\n",
    "ORDER BY channel_name, rn;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcef61a-8236-4f57-9698-45452918b1de",
   "metadata": {
    "collapsed": false,
    "name": "use_sva"
   },
   "source": [
    "## Step 3: Create a Semantic View Using Autopilot\n",
    "\n",
    "Now comes the exciting part: using the **Semantic View Wizard** to automatically generate a semantic view from an existing Tableau workbook. This demonstrates how Autopilot can accelerate semantic layer creation by importing business logic from your existing BI artifacts.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. **Navigate to the Semantic View Wizard**\n",
    "   - Go to **AI & ML > Analyst** from the left side menu in Snowflake\n",
    "   - Select `SVA_VHOL_DB.SVA_VHOL_SCHEMA` from the dropdown\n",
    "   - Click **\"Create New Semantic View\"**\n",
    "   \n",
    "   > **Note**: Use the `ACCOUNTADMIN` role or a role with ownership rights on `SVA_VHOL_DB.SVA_VHOL_SCHEMA`\n",
    "\n",
    "2. **Import from Tableau Workbook**\n",
    "   - Name your semantic view: `SVA_MARKETING_SV`\n",
    "   - Click **Next**\n",
    "   - Select **\"Tableau Files\"** as your context\n",
    "   - Choose stage: `SVA_VHOL_SCHEMA.INTERNAL_DATA_STAGE`\n",
    "   - Navigate to: `/unstructured_docs/BI_dashboards/`\n",
    "   - Select: `CampaignMetrics.twb`\n",
    "   - Click **\"Create and Save\"**\n",
    "\n",
    "3. **Test Your Semantic View**\n",
    "   - Go to the **\"Playground\"** tab in the right panel\n",
    "   - Ask: *\"Show me the top 10 most expensive products based on cost per lead\"*\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** You just went from \"zero-to-semantic-view\" in under 1 minute!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e578bf-e43b-4eea-b0fa-23e1aece7e2a",
   "metadata": {
    "collapsed": false,
    "name": "use_standard_sql_and_tableau"
   },
   "source": [
    "## Step 4: Query Semantic Views with Standard SQL\n",
    "\n",
    "**Semantic View Standard SQL** allows you to write familiar ANSI-style SQL queries against semantic views. This is a powerful feature that enables BI tools like Tableau to connect directly to semantic views without requiring specialized connectors.\n",
    "\n",
    "### What You'll Learn:\n",
    "- Write and run a **Standard SQL query** against the `SVA_MARKETING_SV` semantic view\n",
    "- Understand the syntax differences (using `AGG()` for metric aggregation)\n",
    "- Generate a **Tableau Data Source (.tds) file** for seamless BI integration\n",
    "\n",
    "### Key Syntax Rules for Standard SQL:\n",
    "\n",
    "| Element | Standard SQL Syntax |\n",
    "|---------|---------------------|\n",
    "| Dimensions/Facts | Select directly: `product_name` |\n",
    "| Metrics | Must use `AGG()`: `AGG(cost_per_lead)` |\n",
    "| Grouping | Required when selecting metrics: `GROUP BY ALL` |\n",
    "| Table Reference | Use semantic view name directly: `FROM SVA_MARKETING_SV` |\n",
    "\n",
    "> **Why Standard SQL?** It enables existing BI tools to query semantic views without modification, making adoption seamless for organizations with established Tableau or Power BI workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c902f32-d80d-4138-9506-edf6fbf1802e",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "-- -----------------------------------------------------------------------------\n",
    "-- SETUP: Set Context\n",
    "-- -----------------------------------------------------------------------------\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "USE DATABASE SVA_VHOL_DB;\n",
    "USE SCHEMA SVA_VHOL_SCHEMA;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- STANDARD SQL QUERY: Query the Semantic View\n",
    "-- -----------------------------------------------------------------------------\n",
    "SELECT\n",
    "    product_name,                        -- Dimensions/facts: select directly\n",
    "    AGG(cost_per_lead) AS total_cost_per_lead  -- Metrics: must use AGG()\n",
    "FROM SVA_MARKETING_SV                    -- Reference semantic view directly\n",
    "GROUP BY ALL                             -- Required when selecting metrics\n",
    "ORDER BY total_cost_per_lead DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe5969-2f19-40ef-858a-1d800a790f55",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "## Step 5: Create a TDS Generator Stored Procedure\n",
    "\n",
    "To enable Tableau users to easily connect to semantic views, we'll create a **stored procedure** that automatically generates Tableau Data Source (.tds) files. This procedure:\n",
    "\n",
    "### What It Does:\n",
    "1. **Reads semantic view metadata** using `DESCRIBE SEMANTIC VIEW`\n",
    "2. **Parses dimensions, facts, and metrics** from the metadata\n",
    "3. **Generates valid TDS XML** with proper field mappings, data types, and folder organization\n",
    "4. **Returns downloadable content** ready for use in Tableau\n",
    "\n",
    "### How It Works:\n",
    "- **Dimensions** â†’ Tableau dimensions (organized in folders by source table)\n",
    "- **Facts** (numeric) â†’ Tableau measures with Sum aggregation\n",
    "- **Metrics** â†’ Tableau measures with pre-defined aggregation (uses Minimum to preserve calculated values)\n",
    "\n",
    "> **Note**: This is a Python stored procedure that runs entirely within Snowflake. No external dependencies required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50eb28-4183-44fe-bb31-b909ee38ec45",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": true,
    "language": "sql",
    "name": "create_tds_generation_udf"
   },
   "outputs": [],
   "source": [
    "-- =============================================================================\n",
    "-- TDS GENERATOR: Stored Procedure to Create Tableau Data Source Files\n",
    "-- Run this entire script in Snowsight web interface\n",
    "-- =============================================================================\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- SETUP: Set Context\n",
    "-- -----------------------------------------------------------------------------\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "USE DATABASE SVA_VHOL_DB;\n",
    "USE SCHEMA SVA_VHOL_SCHEMA;\n",
    "\n",
    "-- -----------------------------------------------------------------------------\n",
    "-- CREATE PROCEDURE: Generate TDS from Semantic View\n",
    "-- -----------------------------------------------------------------------------\n",
    "CREATE OR REPLACE PROCEDURE generate_tds_from_semantic_view(semantic_view_name STRING)\n",
    "    RETURNS STRING\n",
    "    LANGUAGE PYTHON\n",
    "    RUNTIME_VERSION = '3.9'\n",
    "    PACKAGES = ('snowflake-snowpark-python', 'pandas')\n",
    "    HANDLER = 'generate_tds_procedure'\n",
    "AS $$\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom\n",
    "from dataclasses import dataclass\n",
    "import uuid\n",
    "\n",
    "@dataclass\n",
    "class SemanticField:\n",
    "    \"\"\"Represents a field from semantic view metadata\"\"\"\n",
    "    name: str\n",
    "    parent_table: str\n",
    "    data_type: str\n",
    "    object_kind: str  # DIMENSION, FACT, METRIC\n",
    "    access_modifier: str  # PUBLIC, PRIVATE\n",
    "    expression: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class TableInfo:\n",
    "    \"\"\"Represents table information from semantic view\"\"\"\n",
    "    name: str\n",
    "    database: str\n",
    "    schema: str\n",
    "    base_table: str\n",
    "\n",
    "@dataclass\n",
    "class Relationship:\n",
    "    \"\"\"Represents a relationship between tables\"\"\"\n",
    "    name: str\n",
    "    ref_table: str  # Referenced table\n",
    "    table: str      # Referencing table  \n",
    "    ref_key: str    # Referenced key\n",
    "    foreign_key: str # Foreign key\n",
    "\n",
    "class UDFSemanticViewParser:\n",
    "    \"\"\"Lightweight parser for UDF use\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_df: pd.DataFrame):\n",
    "        self.metadata_df = metadata_df\n",
    "        self.tables: Dict[str, TableInfo] = {}\n",
    "        self.fields: List[SemanticField] = []\n",
    "        self.relationships: List[Relationship] = []\n",
    "        \n",
    "    def parse(self):\n",
    "        \"\"\"Parse the metadata and return organized data\"\"\"\n",
    "        self._parse_tables()\n",
    "        self._parse_fields()\n",
    "        self._parse_relationships()\n",
    "        return self.tables, self.fields, self.relationships\n",
    "    \n",
    "    def _parse_tables(self):\n",
    "        \"\"\"Extract table information\"\"\"\n",
    "        # Handle quoted column names from Snowflake\n",
    "        cols = self.metadata_df.columns\n",
    "        object_kind_col = None\n",
    "        object_name_col = None\n",
    "        \n",
    "        for col in cols:\n",
    "            if col.strip('\"').upper() == 'OBJECT_KIND':\n",
    "                object_kind_col = col\n",
    "            elif col.strip('\"').upper() == 'OBJECT_NAME':\n",
    "                object_name_col = col\n",
    "        \n",
    "        if not object_kind_col or not object_name_col:\n",
    "            return  # Skip if we can't find the columns\n",
    "        \n",
    "        table_rows = self.metadata_df[self.metadata_df[object_kind_col] == 'TABLE']\n",
    "        \n",
    "        # Find property columns\n",
    "        property_col = None\n",
    "        property_value_col = None\n",
    "        \n",
    "        for col in cols:\n",
    "            if col.strip('\"').upper() == 'PROPERTY':\n",
    "                property_col = col\n",
    "            elif col.strip('\"').upper() == 'PROPERTY_VALUE':\n",
    "                property_value_col = col\n",
    "        \n",
    "        if not property_col or not property_value_col:\n",
    "            return  # Skip if we can't find the columns\n",
    "        \n",
    "        for table_name in table_rows[object_name_col].unique():\n",
    "            table_data = table_rows[table_rows[object_name_col] == table_name]\n",
    "            \n",
    "            table_info = TableInfo(name=table_name, database=\"\", schema=\"\", base_table=\"\")\n",
    "            \n",
    "            for _, row in table_data.iterrows():\n",
    "                if row[property_col] == 'BASE_TABLE_DATABASE_NAME':\n",
    "                    table_info.database = row[property_value_col]\n",
    "                elif row[property_col] == 'BASE_TABLE_SCHEMA_NAME':\n",
    "                    table_info.schema = row[property_value_col]\n",
    "                elif row[property_col] == 'BASE_TABLE_NAME':\n",
    "                    table_info.base_table = row[property_value_col]\n",
    "            \n",
    "            self.tables[table_name] = table_info\n",
    "    \n",
    "    def _parse_fields(self):\n",
    "        \"\"\"Extract field information for dimensions, facts, and metrics\"\"\"\n",
    "        # Handle quoted column names from Snowflake\n",
    "        cols = self.metadata_df.columns\n",
    "        object_kind_col = None\n",
    "        object_name_col = None\n",
    "        parent_entity_col = None  # parent_entity is a column, not a property!\n",
    "        property_col = None\n",
    "        property_value_col = None\n",
    "        \n",
    "        for col in cols:\n",
    "            col_upper = col.strip('\"').upper()\n",
    "            if col_upper == 'OBJECT_KIND':\n",
    "                object_kind_col = col\n",
    "            elif col_upper == 'OBJECT_NAME':\n",
    "                object_name_col = col\n",
    "            elif col_upper == 'PARENT_ENTITY':\n",
    "                parent_entity_col = col\n",
    "            elif col_upper == 'PROPERTY':\n",
    "                property_col = col\n",
    "            elif col_upper == 'PROPERTY_VALUE':\n",
    "                property_value_col = col\n",
    "        \n",
    "        if not all([object_kind_col, object_name_col, property_col, property_value_col]):\n",
    "            return  # Skip if we can't find all required columns\n",
    "        \n",
    "        field_kinds = ['DIMENSION', 'FACT', 'METRIC']\n",
    "        \n",
    "        for kind in field_kinds:\n",
    "            field_rows = self.metadata_df[self.metadata_df[object_kind_col] == kind]\n",
    "            \n",
    "            for field_name in field_rows[object_name_col].unique():\n",
    "                field_data = field_rows[field_rows[object_name_col] == field_name]\n",
    "                \n",
    "                # Get parent_table from the parent_entity column (it's a column, not a property!)\n",
    "                parent_table = \"\"\n",
    "                if parent_entity_col and len(field_data) > 0:\n",
    "                    parent_table = field_data.iloc[0][parent_entity_col]\n",
    "                    if pd.isna(parent_table):\n",
    "                        parent_table = \"\"\n",
    "                \n",
    "                field = SemanticField(\n",
    "                    name=field_name,\n",
    "                    parent_table=str(parent_table) if parent_table else \"\",\n",
    "                    data_type=\"\",\n",
    "                    object_kind=kind,\n",
    "                    access_modifier=\"PUBLIC\",\n",
    "                    expression=\"\"\n",
    "                )\n",
    "                \n",
    "                for _, row in field_data.iterrows():\n",
    "                    if row[property_col] == 'DATA_TYPE':\n",
    "                        field.data_type = row[property_value_col]\n",
    "                    elif row[property_col] == 'ACCESS_MODIFIER':\n",
    "                        field.access_modifier = row[property_value_col]\n",
    "                    elif row[property_col] == 'EXPRESSION':\n",
    "                        field.expression = row[property_value_col]\n",
    "                \n",
    "                self.fields.append(field)\n",
    "    \n",
    "    def _parse_relationships(self):\n",
    "        \"\"\"Extract relationship information from RELATIONSHIP objects\"\"\"\n",
    "        # Handle quoted column names from Snowflake\n",
    "        cols = self.metadata_df.columns\n",
    "        object_kind_col = None\n",
    "        object_name_col = None\n",
    "        property_col = None\n",
    "        property_value_col = None\n",
    "        \n",
    "        for col in cols:\n",
    "            col_upper = col.strip('\"').upper()\n",
    "            if col_upper == 'OBJECT_KIND':\n",
    "                object_kind_col = col\n",
    "            elif col_upper == 'OBJECT_NAME':\n",
    "                object_name_col = col\n",
    "            elif col_upper == 'PROPERTY':\n",
    "                property_col = col\n",
    "            elif col_upper == 'PROPERTY_VALUE':\n",
    "                property_value_col = col\n",
    "        \n",
    "        if not all([object_kind_col, object_name_col, property_col, property_value_col]):\n",
    "            return  # Skip if we can't find all required columns\n",
    "        \n",
    "        relationship_rows = self.metadata_df[self.metadata_df[object_kind_col] == 'RELATIONSHIP']\n",
    "        \n",
    "        for relationship_name in relationship_rows[object_name_col].unique():\n",
    "            rel_data = relationship_rows[relationship_rows[object_name_col] == relationship_name]\n",
    "            \n",
    "            relationship = Relationship(\n",
    "                name=relationship_name,\n",
    "                ref_table=\"\",\n",
    "                table=\"\", \n",
    "                ref_key=\"\",\n",
    "                foreign_key=\"\"\n",
    "            )\n",
    "            \n",
    "            for _, row in rel_data.iterrows():\n",
    "                if row[property_col] == 'REF_TABLE':\n",
    "                    relationship.ref_table = row[property_value_col]\n",
    "                elif row[property_col] == 'TABLE':\n",
    "                    relationship.table = row[property_value_col]\n",
    "                elif row[property_col] == 'REF_KEY':\n",
    "                    relationship.ref_key = row[property_value_col]\n",
    "                elif row[property_col] == 'FOREIGN_KEY':\n",
    "                    relationship.foreign_key = row[property_value_col]\n",
    "            \n",
    "            # Only add if we have the essential information\n",
    "            if relationship.ref_table and relationship.table and relationship.ref_key and relationship.foreign_key:\n",
    "                self.relationships.append(relationship)\n",
    "    \n",
    "    def get_public_fields(self) -> List[SemanticField]:\n",
    "        \"\"\"Get only public fields\"\"\"\n",
    "        return [f for f in self.fields if f.access_modifier == 'PUBLIC']\n",
    "\n",
    "@dataclass\n",
    "class TDSField:\n",
    "    \"\"\"Represents a field in the TDS file\"\"\"\n",
    "    name: str\n",
    "    caption: str\n",
    "    data_type: str\n",
    "    local_type: str\n",
    "    role: str  # 'dimension' or 'measure'\n",
    "    aggregation: str = 'Sum'\n",
    "    semantic_role: str = None\n",
    "    contains_null: bool = True\n",
    "    precision: int = None\n",
    "    scale: int = None\n",
    "    width: int = None\n",
    "    table_name: str = \"\"  # Parent table name for folder organization\n",
    "\n",
    "class UDFTDSGenerator:\n",
    "    \"\"\"Full-featured TDS generator matching the working implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, semantic_view_name: str):\n",
    "        self.semantic_view_name = semantic_view_name\n",
    "        # Extract database and schema from fully qualified name\n",
    "        parts = semantic_view_name.split('.')\n",
    "        if len(parts) >= 3:\n",
    "            self.database = parts[0]\n",
    "            self.schema = parts[1]\n",
    "            self.view_name = parts[2]\n",
    "        else:\n",
    "            self.database = \"TABLEAU\"\n",
    "            self.schema = \"SEMANTICVIEWS\"\n",
    "            self.view_name = semantic_view_name\n",
    "        \n",
    "        # Generate IDs like Tableau does\n",
    "        self.connection_id = f\"snowflake.{str(uuid.uuid4()).replace('-', '')[:26]}\"\n",
    "        self.object_id = f\"SV ({semantic_view_name})_{str(uuid.uuid4()).replace('-', '').upper()[:32]}\"\n",
    "    \n",
    "    def generate_tds(self, tables: Dict[str, TableInfo], fields: List[SemanticField]) -> str:\n",
    "        \"\"\"Generate complete TDS XML matching working format\"\"\"\n",
    "        \n",
    "        # Process fields according to requirements\n",
    "        processed_fields = self._process_fields(fields)\n",
    "        \n",
    "        # Create XML structure\n",
    "        root = ET.Element('datasource', {\n",
    "            'formatted-name': f'federated.{str(uuid.uuid4()).replace(\"-\", \"\")[:26]}',\n",
    "            'inline': 'true',\n",
    "            'source-platform': 'mac',\n",
    "            'version': '18.1',\n",
    "            'xmlns:user': 'http://www.tableausoftware.com/xml/user'\n",
    "        })\n",
    "        \n",
    "        # Add document format change manifest\n",
    "        self._add_format_manifest(root)\n",
    "        \n",
    "        # Add connection\n",
    "        connection = self._create_connection(root)\n",
    "        \n",
    "        # Add metadata records\n",
    "        self._add_metadata_records(connection, processed_fields)\n",
    "        \n",
    "        # Add column definitions\n",
    "        self._add_column_definitions(root, processed_fields)\n",
    "        \n",
    "        # Add folder organization\n",
    "        self._add_folders(root, processed_fields, tables, fields)\n",
    "        \n",
    "        # Add layout and other elements\n",
    "        self._add_layout_and_misc(root)\n",
    "        \n",
    "        # Convert to formatted XML string\n",
    "        return self._prettify_xml(root)\n",
    "    \n",
    "    def _process_fields(self, fields: List[SemanticField]) -> List[TDSField]:\n",
    "        \"\"\"Process semantic fields into TDS fields according to requirements\"\"\"\n",
    "        tds_fields = []\n",
    "        \n",
    "        for field in fields:\n",
    "            # Skip private fields\n",
    "            if field.access_modifier == 'PRIVATE':\n",
    "                continue\n",
    "            \n",
    "            tds_field = TDSField(\n",
    "                name=f\"[{field.name}]\",\n",
    "                caption=self._format_field_caption(field.name),\n",
    "                data_type=field.data_type,\n",
    "                local_type=self._get_local_type(field.data_type),\n",
    "                role=self._determine_role(field),\n",
    "                aggregation=self._determine_aggregation(field),\n",
    "                semantic_role=self._get_semantic_role(field.name),\n",
    "                contains_null=True,\n",
    "                precision=self._extract_precision(field.data_type),\n",
    "                scale=self._extract_scale(field.data_type),\n",
    "                width=self._extract_width(field.data_type),\n",
    "                table_name=field.parent_table  # Track parent table for folder organization\n",
    "            )\n",
    "            \n",
    "            tds_fields.append(tds_field)\n",
    "        \n",
    "        return tds_fields\n",
    "    \n",
    "    def _format_field_caption(self, field_name: str) -> str:\n",
    "        \"\"\"Format field name into proper caption (CamelCase with spaces)\"\"\"\n",
    "        # Handle special prefixes like F_\n",
    "        if field_name.startswith('F_'):\n",
    "            field_name = field_name[2:]  # Remove F_ prefix\n",
    "        \n",
    "        # Split by underscores and capitalize each word\n",
    "        words = field_name.lower().split('_')\n",
    "        formatted_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word.upper() in ['LTV', 'ID', 'SK', 'URL', 'API', 'SQL', 'TAX']:\n",
    "                formatted_words.append(word.upper())\n",
    "            elif word == '30day':\n",
    "                formatted_words.append('30Day')\n",
    "            else:\n",
    "                formatted_words.append(word.capitalize())\n",
    "        \n",
    "        return ' '.join(formatted_words)\n",
    "    \n",
    "    def _add_format_manifest(self, root: ET.Element):\n",
    "        \"\"\"Add document format change manifest\"\"\"\n",
    "        manifest = ET.SubElement(root, 'document-format-change-manifest')\n",
    "        ET.SubElement(manifest, '_.fcp.ObjectModelEncapsulateLegacy.true...ObjectModelEncapsulateLegacy')\n",
    "        ET.SubElement(manifest, '_.fcp.ObjectModelTableType.true...ObjectModelTableType')\n",
    "        ET.SubElement(manifest, '_.fcp.SchemaViewerObjectModel.true...SchemaViewerObjectModel')\n",
    "    \n",
    "    def _create_connection(self, root: ET.Element) -> ET.Element:\n",
    "        \"\"\"Create connection element\"\"\"\n",
    "        connection = ET.SubElement(root, 'connection', {'class': 'federated'})\n",
    "        \n",
    "        # Named connections\n",
    "        named_connections = ET.SubElement(connection, 'named-connections')\n",
    "        named_conn = ET.SubElement(named_connections, 'named-connection', {\n",
    "            'caption': 'pm.snowflakecomputing.com',\n",
    "            'name': self.connection_id\n",
    "        })\n",
    "        \n",
    "        # Connection details\n",
    "        conn_details = ET.SubElement(named_conn, 'connection', {\n",
    "            'authentication': 'Username Password',\n",
    "            'class': 'snowflake',\n",
    "            'dbname': self.database,  # Use semantic view database\n",
    "            'odbc-connect-string-extras': '',\n",
    "            'one-time-sql': '',\n",
    "            'schema': self.schema,  # Use semantic view schema\n",
    "            'server': 'pm.snowflakecomputing.com',\n",
    "            'service': 'SYSADMIN',\n",
    "            'username': 'current_user',\n",
    "            'warehouse': 'COMPUTE_WH'\n",
    "        })\n",
    "        \n",
    "        # Connection customization\n",
    "        customization = ET.SubElement(conn_details, 'connection-customization', {\n",
    "            'class': 'snowflake',\n",
    "            'enabled': 'true',\n",
    "            'version': '18.1'\n",
    "        })\n",
    "        ET.SubElement(customization, 'vendor', {'name': 'snowflake'})\n",
    "        ET.SubElement(customization, 'driver', {'name': 'snowflake'})\n",
    "        \n",
    "        customizations = ET.SubElement(customization, 'customizations')\n",
    "        custom_opts = [\n",
    "            ('CAP_ODBC_METADATA_SUPPRESS_EXECUTED_QUERY', 'yes'),\n",
    "            ('CAP_ODBC_METADATA_SUPPRESS_PREPARED_QUERY', 'yes'),\n",
    "            ('CAP_ODBC_METADATA_SUPPRESS_SELECT_STAR', 'yes'),\n",
    "            ('CAP_ODBC_METADATA_SUPPRESS_SQLCOLUMNS_API', 'no'),\n",
    "            ('CAP_ESCAPE_UNDERSCORE_IN_NAMES', 'no'),\n",
    "            ('CAP_DISABLE_ESCAPE_UNDERSCORE_IN_CATALOG', 'yes')\n",
    "        ]\n",
    "        \n",
    "        for name, value in custom_opts:\n",
    "            ET.SubElement(customizations, 'customization', {'name': name, 'value': value})\n",
    "        \n",
    "        # Relation\n",
    "        ET.SubElement(connection, '_.fcp.ObjectModelEncapsulateLegacy.false...relation', {\n",
    "            'connection': self.connection_id,\n",
    "            'name': 'SV',\n",
    "            'table': f'[{self.database}].[{self.schema}].[{self.view_name}]',\n",
    "            'type': 'table'\n",
    "        })\n",
    "        \n",
    "        ET.SubElement(connection, '_.fcp.ObjectModelEncapsulateLegacy.true...relation', {\n",
    "            'connection': self.connection_id,\n",
    "            'name': 'SV',\n",
    "            'table': f'[{self.database}].[{self.schema}].[{self.view_name}]',\n",
    "            'type': 'table'\n",
    "        })\n",
    "        \n",
    "        return connection\n",
    "    \n",
    "    def _add_metadata_records(self, connection: ET.Element, fields: List[TDSField]):\n",
    "        \"\"\"Add metadata records for each field\"\"\"\n",
    "        metadata_records = ET.SubElement(connection, 'metadata-records')\n",
    "        \n",
    "        ordinal = 1\n",
    "        for field in fields:\n",
    "            record = ET.SubElement(metadata_records, 'metadata-record', {'class': 'column'})\n",
    "            \n",
    "            # Clean field name for remote-name\n",
    "            clean_name = field.name.strip('[]')\n",
    "            ET.SubElement(record, 'remote-name').text = clean_name\n",
    "            ET.SubElement(record, 'remote-type').text = self._get_remote_type(field.data_type)\n",
    "            ET.SubElement(record, 'local-name').text = field.name\n",
    "            ET.SubElement(record, 'parent-name').text = '[SV]'\n",
    "            ET.SubElement(record, 'remote-alias').text = clean_name\n",
    "            ET.SubElement(record, 'ordinal').text = str(ordinal)\n",
    "            ET.SubElement(record, 'local-type').text = field.local_type\n",
    "            ET.SubElement(record, 'aggregation').text = field.aggregation\n",
    "            \n",
    "            if field.precision is not None:\n",
    "                ET.SubElement(record, 'precision').text = str(field.precision)\n",
    "            if field.scale is not None:\n",
    "                ET.SubElement(record, 'scale').text = str(field.scale)\n",
    "            if field.width is not None:\n",
    "                ET.SubElement(record, 'width').text = str(field.width)\n",
    "            \n",
    "            ET.SubElement(record, 'contains-null').text = str(field.contains_null).lower()\n",
    "            \n",
    "            if field.local_type == 'string':\n",
    "                ET.SubElement(record, 'collation', {'flag': '0', 'name': 'binary'})\n",
    "            \n",
    "            # Attributes\n",
    "            attributes = ET.SubElement(record, 'attributes')\n",
    "            debug_remote = self._get_debug_remote_type(field.data_type)\n",
    "            debug_wire = self._get_debug_wire_type(field.data_type)\n",
    "            \n",
    "            ET.SubElement(attributes, 'attribute', {\n",
    "                'datatype': 'string',\n",
    "                'name': 'DebugRemoteType'\n",
    "            }).text = f'\"{debug_remote}\"'\n",
    "            \n",
    "            ET.SubElement(attributes, 'attribute', {\n",
    "                'datatype': 'string',\n",
    "                'name': 'DebugWireType'\n",
    "            }).text = f'\"{debug_wire}\"'\n",
    "            \n",
    "            if field.local_type == 'string':\n",
    "                ET.SubElement(attributes, 'attribute', {\n",
    "                    'datatype': 'string',\n",
    "                    'name': 'TypeIsVarchar'\n",
    "                }).text = '\"true\"'\n",
    "            \n",
    "            ET.SubElement(record, '_.fcp.ObjectModelEncapsulateLegacy.true...object-id').text = f'[{self.object_id}]'\n",
    "            \n",
    "            ordinal += 1\n",
    "    \n",
    "    def _add_column_definitions(self, root: ET.Element, fields: List[TDSField]):\n",
    "        \"\"\"Add column definitions\"\"\"\n",
    "        # Add aliases\n",
    "        ET.SubElement(root, 'aliases', {'enabled': 'yes'})\n",
    "        \n",
    "        # Add column definitions\n",
    "        # Note: We don't add 'table' attribute here because in single semantic view mode,\n",
    "        # the underlying tables (CAMPAIGN_PERFORMANCE_METRICS, etc.) are not defined as\n",
    "        # separate relations in the TDS. Folders are used for organization instead.\n",
    "        for field in fields:\n",
    "            attrs = {\n",
    "                'caption': field.caption,\n",
    "                'datatype': field.local_type,\n",
    "                'name': field.name,\n",
    "                'role': field.role,\n",
    "                'type': 'quantitative' if field.role == 'measure' else ('ordinal' if field.local_type == 'date' else 'nominal')\n",
    "            }\n",
    "            \n",
    "            if field.role == 'measure' and field.aggregation != 'Sum':\n",
    "                attrs['aggregation'] = field.aggregation\n",
    "            \n",
    "            if field.semantic_role:\n",
    "                attrs['semantic-role'] = field.semantic_role\n",
    "            \n",
    "            ET.SubElement(root, 'column', attrs)\n",
    "    \n",
    "    def _add_layout_and_misc(self, root: ET.Element):\n",
    "        \"\"\"Add layout and miscellaneous elements\"\"\"\n",
    "        # Layout\n",
    "        ET.SubElement(root, 'layout', {\n",
    "            '_.fcp.SchemaViewerObjectModel.false...dim-percentage': '0.5',\n",
    "            '_.fcp.SchemaViewerObjectModel.false...measure-percentage': '0.4',\n",
    "            'dim-ordering': 'alphabetic',\n",
    "            'measure-ordering': 'alphabetic',\n",
    "            'show-structure': 'false'\n",
    "        })\n",
    "        \n",
    "        # Object graph\n",
    "        object_graph = ET.SubElement(root, '_.fcp.ObjectModelEncapsulateLegacy.true...object-graph')\n",
    "        objects = ET.SubElement(object_graph, 'objects')\n",
    "        \n",
    "        obj = ET.SubElement(objects, 'object', {\n",
    "            'caption': 'SV',\n",
    "            'id': self.object_id\n",
    "        })\n",
    "        \n",
    "        properties = ET.SubElement(obj, 'properties', {'context': ''})\n",
    "        ET.SubElement(properties, 'relation', {\n",
    "            'connection': self.connection_id,\n",
    "            'name': 'SV',\n",
    "            'table': f'[{self.database}].[{self.schema}].[{self.view_name}]',\n",
    "            'type': 'table'\n",
    "        })\n",
    "    \n",
    "    def _prettify_xml(self, root: ET.Element) -> str:\n",
    "        \"\"\"Convert XML to prettified string\"\"\"\n",
    "        rough_string = ET.tostring(root, encoding='unicode')\n",
    "        reparsed = xml.dom.minidom.parseString(rough_string)\n",
    "        \n",
    "        # Add XML declaration\n",
    "        xml_str = reparsed.toprettyxml(indent='  ', encoding=None)\n",
    "        \n",
    "        # Clean up extra newlines\n",
    "        lines = [line for line in xml_str.split('\\n') if line.strip()]\n",
    "        \n",
    "        # Add build comment\n",
    "        lines.insert(1, '')\n",
    "        lines.insert(2, '<!-- build 20233.25.0610.1449                               -->')\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def _determine_role(self, field: SemanticField) -> str:\n",
    "        \"\"\"Determine if field should be dimension or measure based on requirements\"\"\"\n",
    "        # All METRICS are measures\n",
    "        if field.object_kind == 'METRIC':\n",
    "            return 'measure'\n",
    "        \n",
    "        # All numeric FACTS are measures\n",
    "        if field.object_kind == 'FACT' and self._is_numeric_type(field.data_type):\n",
    "            return 'measure'\n",
    "        \n",
    "        # Non-numeric FACTS are dimensions (grouped in folders with dimensions)\n",
    "        if field.object_kind == 'FACT' and not self._is_numeric_type(field.data_type):\n",
    "            return 'dimension'\n",
    "        \n",
    "        # ALL DIMENSIONS are dimensions, regardless of data type\n",
    "        if field.object_kind == 'DIMENSION':\n",
    "            return 'dimension'\n",
    "        \n",
    "        return 'dimension'  # Default\n",
    "    \n",
    "    def _determine_aggregation(self, field: SemanticField) -> str:\n",
    "        \"\"\"Determine aggregation based on requirements\"\"\"\n",
    "        role = self._determine_role(field)\n",
    "        \n",
    "        if role == 'dimension':\n",
    "            if self._is_numeric_type(field.data_type):\n",
    "                return 'Sum'\n",
    "            elif 'DATE' in field.data_type.upper():\n",
    "                return 'Year'\n",
    "            else:\n",
    "                return 'Count'\n",
    "        \n",
    "        # For measures\n",
    "        if field.object_kind == 'METRIC':\n",
    "            return 'Min'  # All metrics use Minimum aggregation\n",
    "        else:\n",
    "            return 'Sum'  # Facts default to Sum\n",
    "    \n",
    "    def _is_numeric_type(self, data_type: str) -> bool:\n",
    "        \"\"\"Check if data type is numeric\"\"\"\n",
    "        numeric_indicators = ['NUMBER', 'DECIMAL', 'INTEGER', 'FLOAT', 'REAL', 'DOUBLE']\n",
    "        return any(indicator in data_type.upper() for indicator in numeric_indicators)\n",
    "    \n",
    "    def _get_local_type(self, data_type: str) -> str:\n",
    "        \"\"\"Convert Snowflake data type to Tableau local type\"\"\"\n",
    "        data_type_upper = data_type.upper()\n",
    "        \n",
    "        if 'VARCHAR' in data_type_upper or 'STRING' in data_type_upper or 'TEXT' in data_type_upper:\n",
    "            return 'string'\n",
    "        elif 'DATE' in data_type_upper:\n",
    "            return 'date'\n",
    "        elif 'NUMBER' in data_type_upper or 'DECIMAL' in data_type_upper:\n",
    "            # Check if it has decimal places\n",
    "            if ',' in data_type and not data_type.endswith(',0)'):\n",
    "                return 'real'\n",
    "            else:\n",
    "                return 'integer'\n",
    "        elif 'FLOAT' in data_type_upper or 'REAL' in data_type_upper or 'DOUBLE' in data_type_upper:\n",
    "            return 'real'\n",
    "        else:\n",
    "            return 'string'  # Default\n",
    "    \n",
    "    def _get_semantic_role(self, field_name: str) -> str:\n",
    "        \"\"\"Get semantic role for geographic fields\"\"\"\n",
    "        if 'COUNTRY' in field_name.upper():\n",
    "            return '[Country].[Name]'\n",
    "        return None\n",
    "    \n",
    "    def _extract_precision(self, data_type: str) -> int:\n",
    "        \"\"\"Extract precision from data type like NUMBER(38,0)\"\"\"\n",
    "        if '(' in data_type and ')' in data_type:\n",
    "            parts = data_type.split('(')[1].split(')')[0].split(',')\n",
    "            if parts:\n",
    "                try:\n",
    "                    return int(parts[0])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        return None\n",
    "    \n",
    "    def _extract_scale(self, data_type: str) -> int:\n",
    "        \"\"\"Extract scale from data type like NUMBER(38,0)\"\"\"\n",
    "        if '(' in data_type and ')' in data_type and ',' in data_type:\n",
    "            parts = data_type.split('(')[1].split(')')[0].split(',')\n",
    "            if len(parts) > 1:\n",
    "                try:\n",
    "                    return int(parts[1])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        return None\n",
    "    \n",
    "    def _extract_width(self, data_type: str) -> int:\n",
    "        \"\"\"Extract width from data type like VARCHAR(50)\"\"\"\n",
    "        if 'VARCHAR' in data_type.upper() and '(' in data_type:\n",
    "            parts = data_type.split('(')[1].split(')')[0]\n",
    "            try:\n",
    "                return int(parts)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return None\n",
    "    \n",
    "    def _get_remote_type(self, data_type: str) -> str:\n",
    "        \"\"\"Map data type to remote type code\"\"\"\n",
    "        data_type_upper = data_type.upper()\n",
    "        \n",
    "        if 'VARCHAR' in data_type_upper or 'STRING' in data_type_upper:\n",
    "            return '129'  # SQL_VARCHAR\n",
    "        elif 'DATE' in data_type_upper:\n",
    "            return '7'    # SQL_TYPE_DATE\n",
    "        elif 'NUMBER' in data_type_upper or 'DECIMAL' in data_type_upper:\n",
    "            return '131'  # SQL_DECIMAL\n",
    "        else:\n",
    "            return '129'  # Default to VARCHAR\n",
    "    \n",
    "    def _get_debug_remote_type(self, data_type: str) -> str:\n",
    "        \"\"\"Get debug remote type string\"\"\"\n",
    "        data_type_upper = data_type.upper()\n",
    "        \n",
    "        if 'VARCHAR' in data_type_upper:\n",
    "            return 'SQL_VARCHAR'\n",
    "        elif 'DATE' in data_type_upper:\n",
    "            return 'SQL_TYPE_DATE'\n",
    "        elif 'NUMBER' in data_type_upper or 'DECIMAL' in data_type_upper:\n",
    "            return 'SQL_DECIMAL'\n",
    "        else:\n",
    "            return 'SQL_VARCHAR'\n",
    "    \n",
    "    def _get_debug_wire_type(self, data_type: str) -> str:\n",
    "        \"\"\"Get debug wire type string\"\"\"\n",
    "        data_type_upper = data_type.upper()\n",
    "        \n",
    "        if 'VARCHAR' in data_type_upper:\n",
    "            return 'SQL_C_CHAR'\n",
    "        elif 'DATE' in data_type_upper:\n",
    "            return 'SQL_C_TYPE_DATE'\n",
    "        elif 'NUMBER' in data_type_upper or 'DECIMAL' in data_type_upper:\n",
    "            return 'SQL_C_NUMERIC'\n",
    "        else:\n",
    "            return 'SQL_C_CHAR'\n",
    "    \n",
    "    def _add_folders(self, root: ET.Element, fields: List[TDSField], tables: Dict[str, TableInfo], semantic_fields: List[SemanticField]):\n",
    "        \"\"\"Add folder organization\"\"\"\n",
    "        field_groups = {}\n",
    "        \n",
    "        # Group fields by table - ONLY DIMENSIONS get folders\n",
    "        dimension_count = 0\n",
    "        for field in fields:\n",
    "            if field.role == 'dimension':  # Only group dimensions in folders\n",
    "                dimension_count += 1\n",
    "                \n",
    "                # Use table_name from TDSField, fall back to pattern matching if not set\n",
    "                table_name = field.table_name\n",
    "                if not table_name:\n",
    "                    field_name = field.name.strip('[]')\n",
    "                    table_name = self._determine_table_from_field_name(field_name)\n",
    "                \n",
    "                # Don't default to generic \"Dimensions\" - use pattern-based names\n",
    "                if not table_name or table_name == 'Other':\n",
    "                    table_name = 'Other'  # Keep as Other instead of Dimensions\n",
    "                \n",
    "                if table_name not in field_groups:\n",
    "                    field_groups[table_name] = {'dimensions': []}\n",
    "                \n",
    "                field_groups[table_name]['dimensions'].append(field)\n",
    "        \n",
    "        # Only create folders if we have dimensions\n",
    "        if dimension_count > 0:\n",
    "            # Legacy folder format - ONLY dimensions\n",
    "            for table_name, groups in field_groups.items():\n",
    "                if groups['dimensions']:\n",
    "                    folder = ET.SubElement(root, '_.fcp.SchemaViewerObjectModel.false...folder', {\n",
    "                        'name': table_name.title(),\n",
    "                        'role': 'dimensions'\n",
    "                    })\n",
    "                    for field in groups['dimensions']:\n",
    "                        ET.SubElement(folder, 'folder-item', {\n",
    "                            'name': field.name,\n",
    "                            'type': 'field'\n",
    "                        })\n",
    "            \n",
    "            # New folder format - ONLY dimensions\n",
    "            folders_common = ET.SubElement(root, '_.fcp.SchemaViewerObjectModel.true...folders-common')\n",
    "            \n",
    "            for table_name, groups in field_groups.items():\n",
    "                if groups['dimensions']:\n",
    "                    folder = ET.SubElement(folders_common, 'folder', {\n",
    "                        'name': table_name.title()\n",
    "                    })\n",
    "                    \n",
    "                    # Add only dimensions to folders\n",
    "                    for field in sorted(groups['dimensions'], key=lambda f: f.name):\n",
    "                        ET.SubElement(folder, 'folder-item', {\n",
    "                            'name': field.name,\n",
    "                            'type': 'field'\n",
    "                        })\n",
    "    \n",
    "    def _determine_table_from_field_name(self, field_name: str) -> str:\n",
    "        \"\"\"Determine table name from field name patterns based on TPCDS schema\"\"\"\n",
    "        field_upper = field_name.upper()\n",
    "        \n",
    "        # Customer dimension fields (from customer table)\n",
    "        if field_upper in ['AGE', 'AGE_BUCKET', 'BIRTHYEAR', 'COUNTRY', 'VALUE_BUCKET'] or 'C_CUSTOMER' in field_upper:\n",
    "            return 'Customer'\n",
    "        \n",
    "        # Date dimension fields (from date_dim table)\n",
    "        elif field_upper in ['DATE', 'MONTH', 'WEEK', 'YEAR'] or 'D_DATE' in field_upper:\n",
    "            return 'Date'\n",
    "        \n",
    "        # Customer demographics fields (from customer_demographics table)\n",
    "        elif field_upper in ['CREDIT_RATING', 'MARITAL_STATUS'] or 'CD_DEMO' in field_upper:\n",
    "            return 'Demographics'\n",
    "        \n",
    "        # Item dimension fields (from item table)\n",
    "        elif field_upper in ['BRAND', 'CATEGORY', 'CLASS'] or 'I_ITEM' in field_upper:\n",
    "            return 'Item'\n",
    "        \n",
    "        # Store dimension fields (from store table)\n",
    "        elif field_upper in ['MARKET', 'SQUAREFOOTAGE', 'STATE', 'STORECOUNTRY'] or 'S_STORE' in field_upper:\n",
    "            return 'Store'\n",
    "        \n",
    "        # Store sales foreign key fields (from store_sales table)\n",
    "        elif field_upper.startswith('SS_') and field_upper.endswith('_SK'):\n",
    "            return 'Store Sales'\n",
    "        \n",
    "        # Default grouping for unmatched fields\n",
    "        else:\n",
    "            return 'Other'\n",
    "\n",
    "def generate_tds_procedure(session, semantic_view_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Stored procedure to generate TDS from semantic view\n",
    "    \n",
    "    Args:\n",
    "        session: Snowflake session object (automatically provided)\n",
    "        semantic_view_name: Fully qualified semantic view name\n",
    "    \n",
    "    Returns:\n",
    "        str: TDS XML content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Execute DESCRIBE SEMANTIC VIEW\n",
    "        describe_sql = f\"DESCRIBE SEMANTIC VIEW {semantic_view_name}\"\n",
    "        metadata_df = session.sql(describe_sql).to_pandas()\n",
    "        \n",
    "        if metadata_df.empty:\n",
    "            return f\"<!-- Error: No metadata found for semantic view {semantic_view_name} -->\"\n",
    "        \n",
    "        # Debug: Check what columns we actually have\n",
    "        available_columns = list(metadata_df.columns)\n",
    "        if len(available_columns) == 0:\n",
    "            return f\"<!-- Error: DataFrame has no columns -->\"\n",
    "        \n",
    "        # Check if we have the required columns (handling quoted names)\n",
    "        required_cols = ['OBJECT_KIND', 'OBJECT_NAME', 'PROPERTY', 'PROPERTY_VALUE']\n",
    "        found_required = []\n",
    "        \n",
    "        for req_col in required_cols:\n",
    "            found = False\n",
    "            for avail_col in available_columns:\n",
    "                if avail_col.strip('\"').upper() == req_col:\n",
    "                    found_required.append(avail_col)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                return f\"<!-- Error: Required column '{req_col}' not found. Available columns: {', '.join(available_columns)} -->\"\n",
    "        \n",
    "        # Parse metadata\n",
    "        parser = UDFSemanticViewParser(metadata_df)\n",
    "        tables, fields, relationships = parser.parse()\n",
    "        \n",
    "        # Get only public fields\n",
    "        public_fields = parser.get_public_fields()\n",
    "        \n",
    "        if not public_fields:\n",
    "            return f\"<!-- Error: No public fields found in semantic view {semantic_view_name} -->\"\n",
    "        \n",
    "        # Generate TDS\n",
    "        generator = UDFTDSGenerator(semantic_view_name)\n",
    "        tds_content = generator.generate_tds(tables, public_fields)\n",
    "        \n",
    "        return tds_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"<!-- Error generating TDS: {str(e)} -->\"\n",
    "$$;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a62fd5e-079c-4e71-9192-3763c57b5746",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "## Step 6: Generate and Download the TDS File\n",
    "\n",
    "This Streamlit app provides an interactive interface to generate and download TDS files for any semantic view in your account.\n",
    "\n",
    "### How to Use:\n",
    "1. **Run the cell below** to launch the Streamlit interface\n",
    "2. **Enter the semantic view name** (default: `SVA_VHOL_DB.SVA_VHOL_SCHEMA.SVA_MARKETING_SV`)\n",
    "3. **Click \"Generate TDS\"** to create the file\n",
    "4. **Download the .tds file** and open it in Tableau Desktop\n",
    "\n",
    "### Features:\n",
    "- **Single TDS Mode**: Generate one TDS file at a time\n",
    "- **Batch TDS Mode**: Generate multiple TDS files and download as a ZIP\n",
    "\n",
    "### What Happens in Tableau:\n",
    "When you open the .tds file in Tableau:\n",
    "- Dimensions appear organized in folders by source table\n",
    "- Metrics are ready to use with proper aggregations\n",
    "- You can immediately start building visualizations using the semantic view\n",
    "\n",
    "> **Tip**: Make sure Tableau Desktop is configured with your Snowflake credentials before opening the .tds file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363d705-2fdf-454b-9940-5f65c34e6cd4",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "tds_downloader"
   },
   "outputs": [],
   "source": [
    "# Clean TDS Generator for Snowflake Notebook\n",
    "# Simple, minimal interface for developers\n",
    "\n",
    "import streamlit as st\n",
    "import zipfile\n",
    "import io\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "def generate_and_download_tds():\n",
    "    \"\"\"Clean interface for TDS generation and download\"\"\"\n",
    "    \n",
    "    st.title(\"TDS Generator\")\n",
    "    st.write(\"Generate Tableau Data Source files from Snowflake Semantic Views\")\n",
    "    \n",
    "    # Input section\n",
    "    semantic_view = st.text_input(\n",
    "        \"Semantic View Name\",\n",
    "        value=\"SVA_VHOL_DB.SVA_VHOL_SCHEMA.SVA_MARKETING_SV\",\n",
    "        help=\"Enter the fully qualified semantic view name\"\n",
    "    )\n",
    "    \n",
    "    if st.button(\"Generate TDS\", type=\"primary\"):\n",
    "        if not semantic_view:\n",
    "            st.error(\"Please enter a semantic view name\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Get Snowflake session\n",
    "            session = get_active_session()\n",
    "            \n",
    "            # Call the stored procedure\n",
    "            with st.spinner(\"Generating TDS file...\"):\n",
    "                result = session.sql(f\"CALL generate_tds_from_semantic_view('{semantic_view}')\").collect()\n",
    "                tds_content = result[0][0]\n",
    "            \n",
    "            # Check for errors\n",
    "            if tds_content.startswith(\"<!-- Error\"):\n",
    "                st.error(f\"Generation failed: {tds_content}\")\n",
    "                return\n",
    "            \n",
    "            # Success - show download\n",
    "            st.success(\"TDS file generated successfully\")\n",
    "            \n",
    "            # Create filename\n",
    "            view_name = semantic_view.split('.')[-1] if '.' in semantic_view else semantic_view\n",
    "            filename = f\"{view_name}_Semantic_View.tds\"\n",
    "            \n",
    "            # Download button\n",
    "            st.download_button(\n",
    "                label=\"Download TDS\",\n",
    "                data=tds_content,\n",
    "                file_name=filename,\n",
    "                mime=\"application/xml\"\n",
    "            )\n",
    "            \n",
    "            # Show file info\n",
    "            st.info(f\"File: {filename} ({len(tds_content):,} bytes)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error: {str(e)}\")\n",
    "\n",
    "def batch_generate_tds():\n",
    "    \"\"\"Generate multiple TDS files at once\"\"\"\n",
    "    \n",
    "    st.title(\"Batch TDS Generator\")\n",
    "    \n",
    "    # Text area for multiple semantic views\n",
    "    semantic_views = st.text_area(\n",
    "        \"Semantic View Names (one per line)\",\n",
    "        placeholder=\"DATABASE.SCHEMA.VIEW1\\nDATABASE.SCHEMA.VIEW2\\nDATABASE.SCHEMA.VIEW3\",\n",
    "        height=150\n",
    "    )\n",
    "    \n",
    "    if st.button(\"Generate All TDS Files\", type=\"primary\"):\n",
    "        if not semantic_views.strip():\n",
    "            st.error(\"Please enter at least one semantic view name\")\n",
    "            return\n",
    "            \n",
    "        view_list = [v.strip() for v in semantic_views.split('\\n') if v.strip()]\n",
    "        \n",
    "        try:\n",
    "            session = get_active_session()\n",
    "            zip_buffer = io.BytesIO()\n",
    "            \n",
    "            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "                progress_bar = st.progress(0)\n",
    "                \n",
    "                for i, semantic_view in enumerate(view_list):\n",
    "                    st.write(f\"Processing: {semantic_view}\")\n",
    "                    \n",
    "                    # Generate TDS\n",
    "                    result = session.sql(f\"CALL generate_tds_from_semantic_view('{semantic_view}')\").collect()\n",
    "                    tds_content = result[0][0]\n",
    "                    \n",
    "                    if not tds_content.startswith(\"<!-- Error\"):\n",
    "                        # Add to zip\n",
    "                        view_name = semantic_view.split('.')[-1] if '.' in semantic_view else semantic_view\n",
    "                        filename = f\"{view_name}_Semantic_View.tds\"\n",
    "                        zip_file.writestr(filename, tds_content)\n",
    "                        st.success(f\"âœ“ {filename}\")\n",
    "                    else:\n",
    "                        st.error(f\"âœ— Failed: {semantic_view}\")\n",
    "                    \n",
    "                    progress_bar.progress((i + 1) / len(view_list))\n",
    "            \n",
    "            # Download zip file\n",
    "            zip_buffer.seek(0)\n",
    "            st.download_button(\n",
    "                label=\"Download All TDS Files (ZIP)\",\n",
    "                data=zip_buffer.getvalue(),\n",
    "                file_name=\"semantic_view_tds_files.zip\",\n",
    "                mime=\"application/zip\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error: {str(e)}\")\n",
    "\n",
    "# Main interface\n",
    "def main():\n",
    "    \"\"\"Main application\"\"\"\n",
    "    \n",
    "    # Sidebar for mode selection\n",
    "    mode = st.sidebar.radio(\n",
    "        \"Mode\",\n",
    "        [\"Single TDS\", \"Batch TDS\"]\n",
    "    )\n",
    "    \n",
    "    if mode == \"Single TDS\":\n",
    "        generate_and_download_tds()\n",
    "    else:\n",
    "        batch_generate_tds()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5c19a",
   "metadata": {},
   "source": [
    "## Conclusion and Resources\n",
    "\n",
    "### What You Learned\n",
    "In this hands-on lab, you accomplished the following:\n",
    "- âœ… Created a marketing analytics data foundation with dimension and fact tables\n",
    "- âœ… Used **Semantic View Autopilot** to generate a semantic view from a Tableau workbook\n",
    "- âœ… Queried the semantic view using **Standard SQL** syntax\n",
    "- âœ… Created a stored procedure to generate **Tableau Data Source (.tds) files**\n",
    "- âœ… Built an interactive Streamlit app for TDS generation and download\n",
    "\n",
    "### Next Steps\n",
    "- Explore adding more verified queries to improve Autopilot suggestions\n",
    "- Try creating semantic views from other BI artifacts (Power BI, Looker)\n",
    "- Build dashboards in Tableau using your generated .tds files\n",
    "- Integrate semantic views with Cortex Analyst for natural language querying\n",
    "\n",
    "### Documentation\n",
    "- [Semantic Views SQL Documentation](https://docs.snowflake.com/en/user-guide/views-semantic/sql)\n",
    "- [Semantic View Autopilot](https://docs.snowflake.com/en/user-guide/views-semantic/autopilot)\n",
    "- [Standard SQL for Semantic Views](https://docs.snowflake.com/en/user-guide/views-semantic/standard-sql)\n",
    "\n",
    "### Related Tutorials\n",
    "- [Getting Started with Snowflake Semantic Views](https://quickstarts.snowflake.com/guide/getting-started-with-snowflake-semantic-views)\n",
    "- [Build Business-Ready Queries with Semantic Views](https://quickstarts.snowflake.com/guide/build-business-ready-queries-with-snowflake-semantic-views)\n",
    "- [Snowflake Semantic View and Agentic Analytics](https://github.com/Snowflake-Labs/snowflake-demo-notebooks/tree/main/Snowflake_Semantic_View_and_Agentic_Analytics)\n",
    "\n",
    "### GitHub Repository\n",
    "- [Snowflake AI Demo (Data Source)](https://github.com/NickAkincilar/Snowflake_AI_DEMO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "josh.klahr@snowflake.com",
   "authorId": "538700144337",
   "authorName": "JKLAHR",
   "lastEditTime": 1767989122997,
   "notebookId": "2pjllhf3worg5zcgstkn",
   "sessionId": "9fe441e9-6d15-43f9-a72c-f1a831e9b7b0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
