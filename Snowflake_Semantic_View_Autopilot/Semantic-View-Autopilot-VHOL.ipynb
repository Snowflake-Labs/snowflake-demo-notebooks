{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "culkod5attrrvlylaxgs",
   "authorId": "1871075387970",
   "authorName": "DATAPROFESSOR",
   "authorEmail": "hellodataprofessor@gmail.com",
   "sessionId": "a6b2b369-f54b-478a-9427-44783997e424",
   "lastEditTime": 1769542285487
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a483a81-fc9f-4ce1-9235-9083103011c8",
   "metadata": {
    "name": "intro",
    "collapsed": false
   },
   "source": "# ðŸš€ Virtual Hands-On Lab: Semantic View Autopilot\n\nWelcome to our **Virtual Hands-On Lab (VHOL)**! In this session, weâ€™ll guide you through an end-to-end, hands-on journey that shows how to go from raw campaign data âžœ to a fully usable **Snowflake Semantic View** âžœ to seamless consumption in **Tableau**.\n\nThis lab is designed to be practical, visual, and *fun*â€”youâ€™ll follow along in a Python notebook and see how each piece fits together in a modern analytics workflow.\n\n---\n\n## ðŸ§­ What Youâ€™ll Build\n\nBy the end of this lab, you will:\n\n* Load and explore campaign data in Snowflake\n* Automatically generate a Semantic View using the **Semantic View Wizard**\n* Query Semantic Views using **standard SQL**\n* Create a custom **UDF** to generate a Tableau `.tds` file\n* Connect Tableau to Snowflake using the generated Semantic View\n\n---\n\n## ðŸ”„ The Big Picture\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Campaign Data       â”‚                 â”‚ Existing Tableau Workbook  â”‚\nâ”‚  (Snowflake Tables)  â”‚                 â”‚ (.twb file)                â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚                                           â”‚\n           â–¼                                           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Semantic View Autopilot \"Fast-Gen\"                                   â”‚\nâ”‚ (Generate Semantic View from existing analytics)                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Snowflake Semantic   â”‚                 â”‚ Query History              â”‚\nâ”‚ View (Metrics + Dims)â”‚                 â”‚ (Real Usage Patterns)      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚                                           â”‚\n           â–¼                                           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Semantic View Autopilot \"Agentic Optimize\"                           â”‚\nâ”‚ (Refine semantics using observed query behavior)                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚                                    â”‚\n           â–¼                                    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” \nâ”‚ Standard SQL Queries â”‚                â”‚ Cortex Analyst       â”‚ \nâ”‚ (Human-Friendly!)    â”‚                â”‚ (Text to SQL)        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Tableau .tds File    â”‚\nâ”‚ (new system function)â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Tableau Dashboard    â”‚\nâ”‚ Powered by Semantics â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## ðŸ§ª Lab Flow (Step by Step)\n\n\n### 1ï¸âƒ£ Load the Base Campaign Data\nWe start by loading and validating the campaign data that will power everything else in the lab. This gives us a clean, trustworthy foundation to build on.\n\n\n---\n\n\n### 2ï¸âƒ£ Generate a Semantic View with Autopilot â€œFast-Genâ€\nNext, we use **Semantic View Autopilot â€œFast-Genâ€** to generate a Snowflake Semantic View from an **existing Tableau workbook (`.twb`)**. This step shows how business logic can be captured once and reused everywhere.\n\n\n> âœ¨ No manual modeling. No duplicated logic. Just semantics.\n\n\n---\n\n\n### 3ï¸âƒ£ Validate & Query the Semantic View\nOnce the Semantic View exists, we query it using **plain SQL**â€”no special syntax required. Metrics and dimensions are now first-class citizens.\n\n\n---\n\n\n### 4ï¸âƒ£ Improve Semantics with Autopilot â€œAgentic Optimizeâ€\nNow we take the Semantic View a step further using **Semantic View Autopilot â€œAgentic Optimizeâ€**.\n\n\nIt uses **Query History** (real usage patterns) to suggest improvements such as:\n- Better metric definitions and naming\n- Stronger dimension relationships\n- More intuitive, human-friendly semantics\n\n\nThe goal is simple: make the model *better the more itâ€™s used*.\n\n\n---\n\n\n### 5ï¸âƒ£ Generate a Tableau `.tds` via the TDS Generator System Function\nWe then generate a Tableau `.tds` file using our new **TDS Generator system function**. This bridges the gap between Snowflake semantics and Tableau consumption.\n\n\n---\n\n\n### 6ï¸âƒ£ Connect Tableau to the Semantic View\nFinally, we use the generated `.tds` file to connect Tableau directly to the Semantic Viewâ€”unlocking governed, consistent analytics.\n\n---\n\n## ðŸŽ¯ Why This Matters\n\n- **One semantic layer** â†’ many tools\n- **Consistent metrics** across teams\n- **Faster time to insight**\n- **Less manual modeling**\n\nThis lab demonstrates how Snowflake enables *semantic-driven analytics* without sacrificing flexibility or performance.\n\n---\n\n## ðŸ™Œ Letâ€™s Get Hands-On\n\nYouâ€™re now ready to dive in. Open the notebook, start running cells, and watch your Semantic View come to life.\n\nHappy querying! â„ï¸ðŸ“Š\n\n```\n"
  },
  {
   "cell_type": "code",
   "id": "d1f4fb42-f777-4a03-acc1-5e5757e6975c",
   "metadata": {
    "language": "sql",
    "name": "load_marketing_data",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--- this script borrows heavily from the Snowflake Intelligence end to end demo here: https://github.com/NickAkincilar/Snowflake_AI_DEMO\n\n--- should take around 1 minute to run completely\n\n-- Switch to accountadmin role to create db, schema, and load data\n    USE ROLE ACCOUNTADMIN;\n\n-- Create database and schema\n    CREATE OR REPLACE DATABASE SVA_VHOL_DB;\n    USE DATABASE SVA_VHOL_DB;\n\n    CREATE SCHEMA IF NOT EXISTS SVA_VHOL_SCHEMA;\n    USE SCHEMA SVA_VHOL_SCHEMA;\n\n-- Optional: allow anyone to see the agents in this schema\n    GRANT USAGE ON DATABASE SVA_VHOL_DB TO ROLE PUBLIC;\n    GRANT USAGE ON SCHEMA SVA_VHOL_DB.SVA_VHOL_SCHEMA TO ROLE PUBLIC;\n\n-- Create file format for CSV files\n    CREATE OR REPLACE FILE FORMAT CSV_FORMAT\n        TYPE = 'CSV'\n        FIELD_DELIMITER = ','\n        RECORD_DELIMITER = '\\n'\n        SKIP_HEADER = 1\n        FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n        TRIM_SPACE = TRUE\n        ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE\n        ESCAPE = 'NONE'\n        ESCAPE_UNENCLOSED_FIELD = '\\134'\n        DATE_FORMAT = 'YYYY-MM-DD'\n        TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS'\n        NULL_IF = ('NULL', 'null', '', 'N/A', 'n/a');\n\n-- Create API Integration for GitHub (public repository access)\n    CREATE OR REPLACE API INTEGRATION git_api_integration\n        API_PROVIDER = git_https_api\n        API_ALLOWED_PREFIXES = ('https://github.com/NickAkincilar/')\n        ENABLED = TRUE;\n        \n-- Create Git repository integration for the public demo repository\n    CREATE OR REPLACE GIT REPOSITORY SVA_VHOL_REPO\n        API_INTEGRATION = git_api_integration\n        ORIGIN = 'https://github.com/NickAkincilar/Snowflake_AI_DEMO.git';\n\n-- Create internal stage for copied data files\n    CREATE OR REPLACE STAGE INTERNAL_DATA_STAGE\n        FILE_FORMAT = CSV_FORMAT\n        COMMENT = 'Internal stage for copied demo data files'\n        DIRECTORY = ( ENABLE = TRUE)\n        ENCRYPTION = (   TYPE = 'SNOWFLAKE_SSE');\n\n    ALTER GIT REPOSITORY SVA_VHOL_REPO FETCH;\n\n    -- ========================================================================\n    -- COPY DATA FROM GIT TO INTERNAL STAGE\n    -- ========================================================================\n\n    -- Copy all CSV files from Git repository demo_data folder to internal stage\n    COPY FILES\n    INTO @INTERNAL_DATA_STAGE/demo_data/\n    FROM @SVA_VHOL_REPO/branches/main/demo_data/;\n\n\n    COPY FILES\n    INTO @INTERNAL_DATA_STAGE/unstructured_docs/\n    FROM @SVA_VHOL_REPO/branches/main/unstructured_docs/;\n\n    -- Verify files were copied\n    LS @INTERNAL_DATA_STAGE;\n\n    ALTER STAGE INTERNAL_DATA_STAGE refresh;\n\n  \n\n    -- ========================================================================\n    -- DIMENSION TABLES\n    -- ========================================================================\n\n    -- Product Dimension\n    CREATE OR REPLACE TABLE product_dim (\n        product_key INT PRIMARY KEY,\n        product_name VARCHAR(200),\n        category_key INT,\n        category_name VARCHAR(100),\n        vertical VARCHAR(50)\n    );\n\n    -- Region Dimension\n    CREATE OR REPLACE TABLE region_dim (\n        region_key INT PRIMARY KEY,\n        region_name VARCHAR(100)\n    );\n\n    -- Campaign Dimension (Marketing)\n    CREATE OR REPLACE TABLE campaign_dim (\n        campaign_key INT PRIMARY KEY,\n        campaign_name VARCHAR(300),\n        objective VARCHAR(100)\n    );\n\n    -- Channel Dimension (Marketing)\n    CREATE OR REPLACE TABLE channel_dim (\n        channel_key INT PRIMARY KEY,\n        channel_name VARCHAR(100)\n    );\n\n    -- ========================================================================\n    -- FACT TABLE\n    -- ========================================================================\n\n    -- Marketing Campaign Fact Table\n    CREATE OR REPLACE TABLE marketing_campaign_fact (\n        campaign_fact_id INT PRIMARY KEY,\n        date DATE ,\n        campaign_key INT,\n        product_key INT ,\n        channel_key INT ,\n        region_key INT ,\n        spend DECIMAL(10,2) ,\n        leads_generated INT ,\n        impressions INT \n    );\n\n    -- ========================================================================\n    -- LOAD DIMENSION DATA FROM INTERNAL STAGE\n    -- ========================================================================\n\n    -- Load Product Dimension\n    COPY INTO product_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/product_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Region Dimension\n    COPY INTO region_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/region_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Campaign Dimension\n    COPY INTO campaign_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/campaign_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- Load Channel Dimension\n    COPY INTO channel_dim\n    FROM @INTERNAL_DATA_STAGE/demo_data/channel_dim.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- ========================================================================\n    -- LOAD FACT DATA FROM INTERNAL STAGE\n    -- ========================================================================\n\n    -- Load Marketing Campaign Fact\n    COPY INTO marketing_campaign_fact\n    FROM @INTERNAL_DATA_STAGE/demo_data/marketing_campaign_fact.csv\n    FILE_FORMAT = CSV_FORMAT\n    ON_ERROR = 'CONTINUE';\n\n    -- ========================================================================\n    -- VERIFICATION\n    -- ========================================================================\n\n    -- Verify Git integration and file copy\n    SHOW GIT REPOSITORIES;\n  -- SELECT 'Internal Stage Files' as stage_type, COUNT(*) as file_count FROM (LS @INTERNAL_DATA_STAGE);\n\n    -- Verify data loads\n    SELECT 'DIMENSION TABLES' as category, '' as table_name, NULL as row_count\n    UNION ALL\n    SELECT '', 'product_dim', COUNT(*) FROM product_dim\n    UNION ALL\n    SELECT '', 'campaign_dim', COUNT(*) FROM campaign_dim\n    UNION ALL\n    SELECT '', 'channel_dim', COUNT(*) FROM channel_dim\n    UNION ALL\n    SELECT 'FACT TABLES', '', NULL\n    UNION ALL\n    SELECT '', 'marketing_campaign_fact', COUNT(*) FROM marketing_campaign_fact;\n    ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e82c65b4-8bb4-4f6a-b511-0f41aa242118",
   "metadata": {
    "language": "sql",
    "name": "show_the_tables",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Switch to accountadmin role to create db, schema, and load data\n    USE ROLE ACCOUNTADMIN;\n\n-- Create database and schema\n    USE DATABASE SVA_VHOL_DB;\n    USE SCHEMA SVA_VHOL_SCHEMA;\n\n--- list the tables\n    SHOW TABLES;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a38f7b66-d637-4f7e-bb19-27beea1c0dca",
   "metadata": {
    "language": "sql",
    "name": "data_science_team_queries",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--- The semantic view autopilot will look at query history and use this to suggest model improvements and verified queries. If you want to see this in action when you create your semantic view, run this cell.\n\n--- MARKETING SEMANTIC VIEW â€“ ANALYTIC QUERY PLAYBOOK\n--- Semantic View: SVA_MARKETING_DASHBOARD_SV\n\n---\n\n--- Query 1: Overall marketing performance by month\n--- SQL:\n\nSELECT\nDATE_TRUNC('month', mcf.date) AS month,\nSUM(mcf.spend) AS total_spend,\nSUM(mcf.impressions) AS total_impressions,\nSUM(mcf.leads_generated) AS total_leads,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0) AS cost_per_lead,\nSUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0) AS lead_conversion_rate\nFROM MARKETING_CAMPAIGN_FACT mcf\nGROUP BY 1\nORDER BY 1;\n\n---\n\n--- Query 2: Which channels are most efficient\n--- SQL:\n\nSELECT\ncd.channel_name,\nSUM(mcf.spend) AS total_spend,\nSUM(mcf.leads_generated) AS total_leads,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0) AS cost_per_lead\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN CHANNEL_DIM cd ON mcf.channel_key = cd.channel_key\nGROUP BY 1\nHAVING SUM(mcf.leads_generated) > 0\nORDER BY cost_per_lead ASC, total_leads DESC;\n\n---\n\n--- Query 3: Channel trend â€“ month over month\n--- SQL:\n\nSELECT\nDATE_TRUNC('month', mcf.date) AS month,\ncd.channel_name,\nSUM(mcf.spend) AS total_spend,\nSUM(mcf.impressions) AS total_impressions,\nSUM(mcf.leads_generated) AS total_leads,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0) AS cpl,\nSUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0) AS conv_rate\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN CHANNEL_DIM cd ON mcf.channel_key = cd.channel_key\nGROUP BY 1,2\nORDER BY 1,2;\n\n---\n\n--- Query 4: Top campaigns by leads\n--- SQL:\n\nSELECT\nc.campaign_name,\nc.objective,\nSUM(mcf.leads_generated) AS total_leads,\nSUM(mcf.spend) AS total_spend,\nSUM(mcf.impressions) AS total_impressions,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0) AS cpl,\nSUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0) AS conv_rate\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN CAMPAIGN_DIM c ON mcf.campaign_key = c.campaign_key\nGROUP BY 1,2\nORDER BY total_leads DESC\nLIMIT 20;\n\n---\n\n--- Query 5: Objectives performance\n--- SQL:\n\nSELECT\nc.objective,\nSUM(mcf.spend) AS total_spend,\nSUM(mcf.impressions) AS total_impressions,\nSUM(mcf.leads_generated) AS total_leads,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0) AS cpl,\nSUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0) AS conv_rate\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN CAMPAIGN_DIM c ON mcf.campaign_key = c.campaign_key\nGROUP BY 1\nORDER BY cpl ASC;\n\n---\n\n--- Query 6: Performance by region\n--- SQL:\n\nWITH totals AS (\nSELECT SUM(spend) AS all_spend, SUM(leads_generated) AS all_leads FROM MARKETING_CAMPAIGN_FACT\n)\nSELECT\nrd.region_name,\nSUM(mcf.spend) AS spend,\nSUM(mcf.leads_generated) AS leads,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated),0) AS cpl,\nSUM(mcf.spend) / NULLIF(t.all_spend,0) AS spend_share,\nSUM(mcf.leads_generated) / NULLIF(t.all_leads,0) AS leads_share\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN REGION_DIM rd ON mcf.region_key = rd.region_key\nCROSS JOIN totals t\nGROUP BY 1, t.all_spend, t.all_leads\nORDER BY leads DESC;\n\n---\n\n--- Query 7: Products with best conversion\n--- SQL:\n\nSELECT\npd.product_name,\npd.category_name,\npd.vertical,\nSUM(mcf.impressions) AS impressions,\nSUM(mcf.leads_generated) AS leads,\nSUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions),0) AS conv_rate\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN PRODUCT_DIM pd ON mcf.product_key = pd.product_key\nGROUP BY 1,2,3\nHAVING SUM(mcf.impressions) > 0\nORDER BY conv_rate DESC\nLIMIT 20;\n\n---\n\n--- Query 8: Category x Channel CPL heatmap\n--- SQL:\n\nSELECT\npd.category_name,\ncd.channel_name,\nSUM(mcf.spend) AS total_spend,\nSUM(mcf.leads_generated) AS total_leads,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated),0) AS cpl\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN PRODUCT_DIM pd ON mcf.product_key = pd.product_key\nJOIN CHANNEL_DIM cd ON mcf.channel_key = cd.channel_key\nGROUP BY 1,2\nORDER BY pd.category_name, cpl ASC;\n\n---\n\n--- Query 9: Vertical by region leads\n--- SQL:\n\nSELECT\npd.vertical,\nrd.region_name,\nSUM(mcf.leads_generated) AS total_leads,\nSUM(mcf.spend) AS total_spend,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated),0) AS cpl\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN PRODUCT_DIM pd ON mcf.product_key = pd.product_key\nJOIN REGION_DIM rd ON mcf.region_key = rd.region_key\nGROUP BY 1,2\nORDER BY total_leads DESC;\n\n---\n\n--- Query 10: Worst CPL segments\n--- SQL:\n\nSELECT\nc.campaign_name,\ncd.channel_name,\nSUM(mcf.leads_generated) AS leads,\nSUM(mcf.spend) AS spend,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated),0) AS cpl\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN CAMPAIGN_DIM c ON mcf.campaign_key = c.campaign_key\nJOIN CHANNEL_DIM cd ON mcf.channel_key = cd.channel_key\nGROUP BY 1,2\nHAVING SUM(mcf.leads_generated) >= 50\nORDER BY cpl DESC\nLIMIT 10;\n\n--- Query 11: Budget allocation by objective and channel\n--- SQL:\n\nWITH obj_totals AS (\nSELECT\nc.objective,\nSUM(mcf.spend) AS objective_spend\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN CAMPAIGN_DIM c\nON mcf.campaign_key = c.campaign_key\nGROUP BY 1\n)\nSELECT\nc.objective,\ncd.channel_name,\nSUM(mcf.spend) AS spend,\nSUM(mcf.spend) / NULLIF(ot.objective_spend, 0) AS spend_share_within_objective\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN CAMPAIGN_DIM c\nON mcf.campaign_key = c.campaign_key\nJOIN CHANNEL_DIM cd\nON mcf.channel_key = cd.channel_key\nJOIN obj_totals ot\nON c.objective = ot.objective\nGROUP BY 1, 2, ot.objective_spend\nORDER BY c.objective, spend_share_within_objective DESC;\n\n--- Query 12: Facebook vs non-Facebook efficiency over time\n--- SQL:\n\nSELECT\nDATE_TRUNC('month', mcf.date) AS month,\nSUM(CASE WHEN cd.channel_name = 'Facebook' THEN mcf.spend ELSE 0 END) AS facebook_spend,\nSUM(CASE WHEN cd.channel_name <> 'Facebook' THEN mcf.spend ELSE 0 END) AS non_facebook_spend,\nSUM(CASE WHEN cd.channel_name = 'Facebook' THEN mcf.leads_generated ELSE 0 END) AS facebook_leads,\nSUM(CASE WHEN cd.channel_name <> 'Facebook' THEN mcf.leads_generated ELSE 0 END) AS non_facebook_leads,\n(SUM(CASE WHEN cd.channel_name = 'Facebook' THEN mcf.spend ELSE 0 END)\n/ NULLIF(SUM(CASE WHEN cd.channel_name = 'Facebook' THEN mcf.leads_generated ELSE 0 END), 0)\n) AS facebook_cpl,\n(SUM(CASE WHEN cd.channel_name <> 'Facebook' THEN mcf.spend ELSE 0 END)\n/ NULLIF(SUM(CASE WHEN cd.channel_name <> 'Facebook' THEN mcf.leads_generated ELSE 0 END), 0)\n) AS non_facebook_cpl\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN CHANNEL_DIM cd\nON mcf.channel_key = cd.channel_key\nGROUP BY 1\nORDER BY 1;\n\n--- Query 13: Campaign flighting (first/last active date, duration, totals)\n--- SQL:\n\nSELECT\nc.campaign_name,\nc.objective,\nMIN(mcf.date) AS first_active_date,\nMAX(mcf.date) AS last_active_date,\nDATEDIFF('day', MIN(mcf.date), MAX(mcf.date)) + 1 AS active_days,\nSUM(mcf.spend) AS total_spend,\nSUM(mcf.leads_generated) AS total_leads,\nSUM(mcf.impressions) AS total_impressions,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0) AS cpl,\nSUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0) AS conv_rate\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN CAMPAIGN_DIM c\nON mcf.campaign_key = c.campaign_key\nGROUP BY 1, 2\nORDER BY last_active_date DESC, total_spend DESC;\n\n--- Query 14: Week-over-week growth (spend and leads with WoW % change)\n--- SQL:\n\nWITH weekly AS (\nSELECT\nDATE_TRUNC('week', date) AS week,\nSUM(spend) AS spend,\nSUM(leads_generated) AS leads\nFROM MARKETING_CAMPAIGN_FACT\nGROUP BY 1\n)\nSELECT\nweek,\nspend,\nleads,\n(spend - LAG(spend) OVER (ORDER BY week))\n/ NULLIF(LAG(spend) OVER (ORDER BY week), 0) AS spend_wow_pct,\n(leads - LAG(leads) OVER (ORDER BY week))\n/ NULLIF(LAG(leads) OVER (ORDER BY week), 0) AS leads_wow_pct\nFROM weekly\nORDER BY week;\n\n--- Query 15: Best â€˜scaledâ€™ segments (lowest CPL with high lead volume, region x channel)\n--- SQL:\n\nSELECT\nrd.region_name,\ncd.channel_name,\nSUM(mcf.leads_generated) AS leads,\nSUM(mcf.spend) AS spend,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0) AS cpl\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN REGION_DIM rd\nON mcf.region_key = rd.region_key\nJOIN CHANNEL_DIM cd\nON mcf.channel_key = cd.channel_key\nGROUP BY 1, 2\nHAVING SUM(mcf.leads_generated) >= 100\nORDER BY cpl ASC, leads DESC\nLIMIT 20;\n\n--- Query 16: Objective mix by region (what are we running where?)\n--- SQL:\n\nSELECT\nrd.region_name,\nc.objective,\nSUM(mcf.spend) AS spend,\nSUM(mcf.leads_generated) AS leads,\nSUM(mcf.impressions) AS impressions,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0) AS cpl,\nSUM(mcf.leads_generated) / NULLIF(SUM(mcf.impressions), 0) AS conv_rate\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN REGION_DIM rd\nON mcf.region_key = rd.region_key\nJOIN CAMPAIGN_DIM c\nON mcf.campaign_key = c.campaign_key\nGROUP BY 1, 2\nORDER BY rd.region_name, spend DESC;\n\n--- Query 17: Pareto view (cumulative leads vs cumulative spend by campaign)\n--- SQL:\n\nWITH by_campaign AS (\nSELECT\nc.campaign_name,\nSUM(mcf.leads_generated) AS leads,\nSUM(mcf.spend) AS spend\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN CAMPAIGN_DIM c\nON mcf.campaign_key = c.campaign_key\nGROUP BY 1\n),\nranked AS (\nSELECT\n*,\nSUM(leads) OVER () AS total_leads,\nSUM(spend) OVER () AS total_spend,\nSUM(leads) OVER (ORDER BY leads DESC) AS cum_leads,\nSUM(spend) OVER (ORDER BY leads DESC) AS cum_spend\nFROM by_campaign\n)\nSELECT\ncampaign_name,\nleads,\nspend,\ncum_leads / NULLIF(total_leads, 0) AS cumulative_leads_share,\ncum_spend / NULLIF(total_spend, 0) AS cumulative_spend_share\nFROM ranked\nORDER BY leads DESC;\n\n--- Query 18: Anomaly detection-lite (days with unusually high CPL via z-score)\n--- SQL:\n\nWITH daily AS (\nSELECT\ndate,\nSUM(spend) AS spend,\nSUM(leads_generated) AS leads,\nSUM(spend) / NULLIF(SUM(leads_generated), 0) AS cpl\nFROM MARKETING_CAMPAIGN_FACT\nGROUP BY 1\n),\nstats AS (\nSELECT\nAVG(cpl) AS avg_cpl,\nSTDDEV_SAMP(cpl) AS std_cpl\nFROM daily\nWHERE cpl IS NOT NULL\n),\nscored AS (\nSELECT\nd.date,\nd.spend,\nd.leads,\nd.cpl,\n(d.cpl - s.avg_cpl) / NULLIF(s.std_cpl, 0) AS cpl_zscore\nFROM daily d\nCROSS JOIN stats s\nWHERE d.cpl IS NOT NULL\n)\nSELECT\n*\nFROM scored\nWHERE ABS(cpl_zscore) >= 2\nORDER BY ABS(cpl_zscore) DESC, date DESC;\n\n--- Query 19: Diminishing returns curve (CPL by daily spend bucket)\n--- SQL:\n\nWITH daily AS (\nSELECT\ndate,\nSUM(spend) AS spend,\nSUM(leads_generated) AS leads\nFROM MARKETING_CAMPAIGN_FACT\nGROUP BY 1\n),\nbucketed AS (\nSELECT\n*,\nWIDTH_BUCKET(spend, 0, (SELECT MAX(spend) FROM daily), 10) AS spend_bucket\nFROM daily\n)\nSELECT\nspend_bucket,\nMIN(spend) AS min_spend_in_bucket,\nMAX(spend) AS max_spend_in_bucket,\nSUM(spend) AS total_spend,\nSUM(leads) AS total_leads,\nSUM(spend) / NULLIF(SUM(leads), 0) AS cpl\nFROM bucketed\nGROUP BY 1\nORDER BY 1;\n\n--- Query 20: Efficiency frontier (top 5 campaigns per channel by lowest CPL, min lead threshold)\n--- SQL:\n\nWITH campaign_channel AS (\nSELECT\ncd.channel_name,\nc.campaign_name,\nSUM(mcf.spend) AS spend,\nSUM(mcf.leads_generated) AS leads,\nSUM(mcf.spend) / NULLIF(SUM(mcf.leads_generated), 0) AS cpl\nFROM MARKETING_CAMPAIGN_FACT mcf\nJOIN CHANNEL_DIM cd\nON mcf.channel_key = cd.channel_key\nJOIN CAMPAIGN_DIM c\nON mcf.campaign_key = c.campaign_key\nGROUP BY 1, 2\nHAVING SUM(mcf.leads_generated) >= 50\n),\nranked AS (\nSELECT\n*,\nROW_NUMBER() OVER (\nPARTITION BY channel_name\nORDER BY cpl ASC NULLS LAST, leads DESC\n) AS rn\nFROM campaign_channel\n)\nSELECT\nchannel_name,\ncampaign_name,\nspend,\nleads,\ncpl\nFROM ranked\nWHERE rn <= 5\nORDER BY channel_name, rn;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7fcef61a-8236-4f57-9698-45452918b1de",
   "metadata": {
    "name": "use_sva",
    "collapsed": false
   },
   "source": "### Use the Semantic View Wizard to Build a Semantic View from a Tableau Dashbard\n\n- Navigate to the Semantic View wizard by going to **\"AI & ML > Analyst\"** from the left side menu in Snowflake, pick the `SVA_VHOL_DB.SVA_VHOL_SCHEMA` from the drop down, and select **\"Create New Semantic View\"**. *Note: be sure to use the `ACCOUNTADMIN` role or a role that has ownership rights on `SVA_VHOL_DB.SVA_VHOL_SCHEMA.`*\n- Name your semantic view `SVA_MARKETING_SV` and click **\"Next\"**. In the following screen, select \"Tableu Files\" as your context. Pick `SVA_VHOL_SCHEMA.INTERNAL_DATA_STAGE` and navigate to the `/unstructured_docs/BI_dashboards` folder (when in `/unstructured_docs/` folder you may need to scroll down to find `BI_dashboards` sub-folder. Select the `CampaignMetricsDash.twb` file and click \"Create and Save\".\n- On the next screen you will now see a \"first pass\" version of a Snowflake Semantic view that is based on the imported `.twb` file.  Test it out by going to the **\"Playground\"** tab in the right-hand panel and asking: *\"Show me the top 10 most expensive products based on cost per lead\"*.\n- Congratulations! You just went from \"zero-to-semantic-view\" in 1 minute!\n"
  },
  {
   "cell_type": "markdown",
   "id": "54e578bf-e43b-4eea-b0fa-23e1aece7e2a",
   "metadata": {
    "name": "use_standard_sql_and_tableau",
    "collapsed": false
   },
   "source": "### Use Semantic View \"Standard SQL\" to Query Semantic Views, Access Using Tableau\n\nSemantic View [Standard SQL](https://docs.snowflake.com/en/user-guide/views-semantic/querying#specifying-the-name-of-the-semantic-view-in-the-from-clause) in Snowflake allows you to right ANSI-style SQL queries against semantic views. In this section we will:\n- Write and run a Standard SQL query against the `SVA_MARKETING_SV` semantic view\n- Generate a Tableau Data Source (`.tds`) file for the semantic view\n- Query the semantic view from Tableau\n\n"
  },
  {
   "cell_type": "code",
   "id": "2c902f32-d80d-4138-9506-edf6fbf1802e",
   "metadata": {
    "language": "sql",
    "name": "sv_standard_sql"
   },
   "outputs": [],
   "source": "-- Switch to accountadmin role to create db, schema, and load data\n    USE ROLE ACCOUNTADMIN;\n\n-- Create database and schema\n    USE DATABASE SVA_VHOL_DB;\n    USE SCHEMA SVA_VHOL_SCHEMA;\n\n-- Standard SQL query\n\nSELECT\n   -- dimensions and facts can be selected directly\n   product_name,\n   --- metrics must be aggregated using AGG, MIN, MAX, or ANYVALUE\n   AGG(cost_per_lead) as total_cost_per_lead\nFROM\n   --- directly reference the semantic view name\n   SVA_MARKETING_SV\n    --- you must include a GROUP BY if your SELECT statement includes any metrics\n    GROUP BY ALL\nORDER BY total_cost_per_lead DESC\nLIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4a62fd5e-079c-4e71-9192-3763c57b5746",
   "metadata": {
    "name": "generate_tds",
    "collapsed": false
   },
   "source": "### Generate and download the .tds file\n\nSnowflake now has a system function that automatically generates a Tableau Data Source (`.tds`) file that points directly to the relevant Snowflake Semantic View with a pre-configured connection, folders for organization, and well-mapped metric and dimensions.\n\nYou can also try the function yourself with the statement below:\n\n```select SYSTEM$EXPORT_TDS_FROM_SEMANTIC_VIEW('my_test_sv');```\n\nRun this cell and click **\"Generate TDS\"**, then open the `.tds` file using Tableau!"
  },
  {
   "cell_type": "code",
   "id": "51d83e18-1add-4706-a6ed-59968f7773de",
   "metadata": {
    "language": "python",
    "name": "system_function_tds_download",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Clean TDS Generator for Snowflake Notebook\n# Simple, minimal interface for developers\n\nimport streamlit as st\nimport zipfile\nimport io\nfrom snowflake.snowpark.context import get_active_session\n\ndef generate_and_download_tds():\n    \"\"\"Clean interface for TDS generation and download\"\"\"\n    \n    st.title(\"TDS Generator\")\n    st.write(\"Generate Tableau Data Source files from Snowflake Semantic Views\")\n    \n    # Input section\n    semantic_view = st.text_input(\n        \"Semantic View Name\",\n        value=\"SVA_VHOL_DB.SVA_VHOL_SCHEMA.SVA_MARKETING_SV\",\n        help=\"Enter the fully qualified semantic view name\"\n    )\n    \n    if st.button(\"Generate TDS\", type=\"primary\"):\n        if not semantic_view:\n            st.error(\"Please enter a semantic view name\")\n            return\n            \n        try:\n            # Get Snowflake session\n            session = get_active_session()\n            \n            # Call the stored procedure\n            with st.spinner(\"Generating TDS file...\"):\n                result = session.sql(f\"select SYSTEM$EXPORT_TDS_FROM_SEMANTIC_VIEW('{semantic_view}');\").collect()\n                tds_content = result[0][0]\n            \n            # Check for errors\n            if tds_content.startswith(\"<!-- Error\"):\n                st.error(f\"Generation failed: {tds_content}\")\n                return\n            \n            # Success - show download\n            st.success(\"TDS file generated successfully\")\n            \n            # Create filename\n            view_name = semantic_view.split('.')[-1] if '.' in semantic_view else semantic_view\n            filename = f\"{view_name}_Semantic_View.tds\"\n            \n            # Download button\n            st.download_button(\n                label=\"Download TDS\",\n                data=tds_content,\n                file_name=filename,\n                mime=\"application/xml\"\n            )\n            \n            # Show file info\n            st.info(f\"File: {filename} ({len(tds_content):,} bytes)\")\n            \n        except Exception as e:\n            st.error(f\"Error: {str(e)}\")\n\ndef batch_generate_tds():\n    \"\"\"Generate multiple TDS files at once\"\"\"\n    \n    st.title(\"Batch TDS Generator\")\n    \n    # Text area for multiple semantic views\n    semantic_views = st.text_area(\n        \"Semantic View Names (one per line)\",\n        placeholder=\"DATABASE.SCHEMA.VIEW1\\nDATABASE.SCHEMA.VIEW2\\nDATABASE.SCHEMA.VIEW3\",\n        height=150\n    )\n    \n    if st.button(\"Generate All TDS Files\", type=\"primary\"):\n        if not semantic_views.strip():\n            st.error(\"Please enter at least one semantic view name\")\n            return\n            \n        view_list = [v.strip() for v in semantic_views.split('\\n') if v.strip()]\n        \n        try:\n            session = get_active_session()\n            zip_buffer = io.BytesIO()\n            \n            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n                progress_bar = st.progress(0)\n                \n                for i, semantic_view in enumerate(view_list):\n                    st.write(f\"Processing: {semantic_view}\")\n                    \n                    # Generate TDS\n                    result = session.sql(f\"select SYSTEM$EXPORT_TDS_FROM_SEMANTIC_VIEW('{semantic_view}');\").collect()\n                    tds_content = result[0][0]\n                    \n                    if not tds_content.startswith(\"<!-- Error\"):\n                        # Add to zip\n                        view_name = semantic_view.split('.')[-1] if '.' in semantic_view else semantic_view\n                        filename = f\"{view_name}_Semantic_View.tds\"\n                        zip_file.writestr(filename, tds_content)\n                        st.success(f\"âœ“ {filename}\")\n                    else:\n                        st.error(f\"âœ— Failed: {semantic_view}\")\n                    \n                    progress_bar.progress((i + 1) / len(view_list))\n            \n            # Download zip file\n            zip_buffer.seek(0)\n            st.download_button(\n                label=\"Download All TDS Files (ZIP)\",\n                data=zip_buffer.getvalue(),\n                file_name=\"semantic_view_tds_files.zip\",\n                mime=\"application/zip\"\n            )\n            \n        except Exception as e:\n            st.error(f\"Error: {str(e)}\")\n\n# Main interface\ndef main():\n    \"\"\"Main application\"\"\"\n    \n    # Sidebar for mode selection\n    mode = st.sidebar.radio(\n        \"Mode\",\n        [\"Single TDS\", \"Batch TDS\"]\n    )\n    \n    if mode == \"Single TDS\":\n        generate_and_download_tds()\n    else:\n        batch_generate_tds()\n\nif __name__ == \"__main__\":\n    main()\n",
   "execution_count": null
  }
 ]
}