{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3775908f-ca36-4846-8f38-5adca39217f2",
      "metadata": {
        "name": "cell1",
        "collapsed": false
      },
      "source": "# \ud83d\udcc8 Telco Churn Model\n\nIn this Quickstart guide, we will play the role of a data scientist at a telecom company that wants to identify users who are at high risk of churning. To accomplish this, we need to build a model that can learn how to identify such users. We will demonstrate how to use Snowflake Notebook in conjunction with Snowflake/Snowpark to build a Random Forest Classifier to help us with this task.\n\n\n### Prerequisites\n\n- Familiarity with basic Python and SQL\n- Familiarity with training ML models\n- Familiarity with data science notebooks\n- Go to the [Snowflake](https://signup.snowflake.com/) sign-up page and register for a free account. After registration, you will receive an email containing a link that will take you to Snowflake, where you can sign in.\n\n### What You'll Learn\n\n- How to import/load data with Snowflake Notebook\n- How to train a Random Forest with Snowpark ML model\n- How to visualize the predicted results from the forecasting model\n- How to build an interactive web app and make predictions on new users\n"
    },
    {
      "cell_type": "markdown",
      "id": "c112f9d6-c914-4094-8de1-e8f128a64274",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell2"
      },
      "source": [
        "First, add the `imbalanced-learn` and `snowflake-ml-python` package from the package picker on the top right. We will be using these packages later in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cafe2625-7659-4d9f-89dc-eca109d16bb8",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell3"
      },
      "source": [
        "## Importing Data\n",
        "To pull our churn dataset into SnowSight notebooks, we will pull some parquet data from AWS S3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7abdd41-0957-485d-864a-deb3a1ed8339",
      "metadata": {
        "codeCollapsed": false,
        "language": "sql",
        "name": "cell4",
        "collapsed": false
      },
      "outputs": [],
      "source": "CREATE OR REPLACE STAGE TELCO_CHURN_EXTERNAL_STAGE_DEMO\n    URL = 's3://sfquickstarts/notebook_demos/churn/' "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc72488-fc2f-4359-ba27-06c431c9e457",
      "metadata": {
        "language": "sql",
        "name": "cell5",
        "codeCollapsed": false,
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "CREATE FILE FORMAT IF NOT EXISTS MY_PARQUET_FORMAT TYPE = PARQUET COMPRESSION = SNAPPY;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bc6d6c4-7714-44e6-979c-78b669621171",
      "metadata": {
        "language": "sql",
        "name": "cell6",
        "codeCollapsed": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "LS @TELCO_CHURN_EXTERNAL_STAGE_DEMO;"
    },
    {
      "cell_type": "code",
      "id": "a75ae25e-e012-47e8-97b5-693cc475060b",
      "metadata": {
        "language": "sql",
        "name": "cell7"
      },
      "outputs": [],
      "source": "CREATE TABLE if not exists TELCO_CHURN_RAW_DATA_DEMO USING TEMPLATE ( \n    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*)) \n    FROM \n        TABLE( INFER_SCHEMA( \n        LOCATION => '@TELCO_CHURN_EXTERNAL_STAGE_DEMO', \n        FILE_FORMAT => 'MY_PARQUET_FORMAT',\n        FILES => 'telco_churn.parquet'\n        ) \n    ) \n);",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f21fdaf-86d0-42bb-affa-e164316964b1",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "sql",
        "name": "cell8"
      },
      "outputs": [],
      "source": "COPY INTO TELCO_CHURN_RAW_DATA_DEMO\nFROM @TELCO_CHURN_EXTERNAL_STAGE_DEMO\nFILES = ('telco_churn.parquet')\nFILE_FORMAT = (\n    TYPE=PARQUET,\n    REPLACE_INVALID_CHARACTERS=TRUE,\n    BINARY_AS_TEXT=FALSE\n)\nMATCH_BY_COLUMN_NAME=CASE_INSENSITIVE\nON_ERROR=ABORT_STATEMENT;"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbfde78a-b5fc-4f7c-a413-fa8c076458f9",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "sql",
        "name": "cell9"
      },
      "outputs": [],
      "source": "SELECT * FROM TELCO_CHURN_RAW_DATA_DEMO;"
    },
    {
      "cell_type": "markdown",
      "id": "430ebdf6-dab3-4096-aec8-26c9fe17f603",
      "metadata": {
        "name": "cell10",
        "collapsed": false
      },
      "source": [
        "# Working with Data\n",
        "\n",
        "Now that we have our data loaded in, we can start working with the data using our familiar data science libraries in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "language": "python",
        "name": "cell11"
      },
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\nimport streamlit as st\nimport altair as alt\nfrom imblearn.over_sampling import SMOTE \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712182bf-38f8-4b54-8511-494f139ad508",
      "metadata": {
        "language": "python",
        "name": "cell12",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "from snowflake.snowpark.context import get_active_session\nsession = get_active_session()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c34ca1e0-434f-40ba-97e5-3210c74b192c",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell13"
      },
      "outputs": [],
      "source": "telco_churn_snow_df = cell9.to_df()\ntelco_churn_snow_df"
    },
    {
      "cell_type": "markdown",
      "id": "181caa90-d298-4763-998b-38a04764bb6a",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell14"
      },
      "source": "## Exploratory Data Analysis (EDA)\n\nMachine learning models thrive on clean and well-organized data. To ensure our models perform at their best, we'll investigate our dataset to address any missing values and visualize the distributions of each column."
    },
    {
      "cell_type": "markdown",
      "id": "8371a133-8d23-4330-8992-b15f56b111cd",
      "metadata": {
        "name": "cell15",
        "collapsed": false
      },
      "source": "### Basic Summary Statistics"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb5fa2b7-fb41-411f-af93-5b8990a1cd7f",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell16",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "telco_churn_snow_df.describe()"
    },
    {
      "cell_type": "markdown",
      "id": "a9282527-2b63-4b6a-9c7b-085525fd9367",
      "metadata": {
        "name": "cell17",
        "collapsed": false
      },
      "source": [
        "### Checking nulls with Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf8ed78d-c98d-4119-8f20-8924b90c4b67",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell18"
      },
      "outputs": [],
      "source": "telco_churn_pdf = telco_churn_snow_df.to_pandas()\ntelco_churn_pdf.isnull().sum()"
    },
    {
      "cell_type": "markdown",
      "id": "7e641dee-1fa7-428c-a04f-88019983436d",
      "metadata": {
        "name": "cell19",
        "collapsed": false
      },
      "source": [
        "As can be seen, there is no null value in any of the feature columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25eafe5f-f32f-4af7-bb38-ffca9f3f71f1",
      "metadata": {
        "name": "cell20",
        "collapsed": false
      },
      "source": [
        "### Visualizing Feature Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf78bb87-047e-4706-9268-0ec01a26bb93",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell21"
      },
      "outputs": [],
      "source": "columns = telco_churn_pdf.columns\nnum_columns_for_display = 3\ncol1, col2 , col3 = st.columns(num_columns_for_display)\nindex = 0\nfor col in columns:\n    source = pd.DataFrame(telco_churn_pdf[col])\n    chrt = alt.Chart(source).mark_bar().encode(\n    alt.X(f\"{col}:Q\", bin=True),\n    y='count()',\n    )\n    if index % num_columns_for_display == 0:\n        with col1: \n            st.altair_chart(chrt)\n    elif index % num_columns_for_display == 1:\n        with col2: \n            st.altair_chart(chrt)\n    elif index % num_columns_for_display == 2:\n        with col3: \n            st.altair_chart(chrt)\n    index = index + 1"
    },
    {
      "cell_type": "markdown",
      "id": "f126aef0-a085-4525-8ce7-c976e21332b3",
      "metadata": {
        "name": "cell22",
        "collapsed": false
      },
      "source": [
        "### Understanding Churn Rate - Imbalanced dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "152d1847-a0d9-4f39-bf24-4d340db5b891",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell23"
      },
      "outputs": [],
      "source": "telco_churn_snow_df.group_by('\"Churn\"').count()"
    },
    {
      "cell_type": "markdown",
      "id": "4d8eeb31-8a03-435d-a3dd-67d210db0fe0",
      "metadata": {
        "name": "cell24",
        "collapsed": false
      },
      "source": [
        "If you want to understand a model, you need to know its weaknesses. When the target variable has one class that is much more frequent than the other, your data is imbalanced. This causes issues when evaluating models since both classes don't get equal attention.\n",
        "\n",
        "In contrast to modeling an imbalanced dataset, a model trained on balanced data sees an equal amount of observations per class. By eliminating the imbalance, we also eliminate the model's potential to achieve high metric scores due to bias towards a majority class. This means that when we evaluate our model, the metrics can capture a better representation of how well the model does at making valuable predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "352ad1cb-86c4-4228-88f6-ab11c7c0fa13",
      "metadata": {
        "name": "cell25",
        "collapsed": false
      },
      "source": "#### Comparing Big data processing with pandas v.s. Snowpark Dataframes\n\nFor the groupby aggregation query above, we used Snowpark dataframes to perform the operation. Snowpark's Dataframe API allows you to query and process data at scale in Snowflake. With Snowpark, you no longer have to convert your dataframes to pandas in memory. Snowpark lets process data in Snowflake without moving data to the system where your application code runs, and process at scale as part of the elastic and serverless Snowflake engine.\n\nBelow we look at how the query performance of the groupby aggregation with Snowpark v.s. pandas.\n"
    },
    {
      "cell_type": "code",
      "id": "27268649-5b9c-42d8-b5df-fa8d442d869f",
      "metadata": {
        "language": "python",
        "name": "cell26",
        "collapsed": false,
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "import time\nstart = time.time()\ntelco_churn_snow_df.group_by('\"Churn\"').count()\nend = time.time()\nst.markdown(f\"Total Time with Snowpark: {end-start}\")",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b54387a5-1d61-4c80-a2b4-33a1e7b32c5c",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell27",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "start = time.time()\ntelco_churn_snow_pdf = telco_churn_snow_df.to_pandas()\nend_mid = time.time()\ntelco_churn_snow_pdf.groupby(\"Churn\").count()\nend = time.time()\nst.markdown(f\"Total Time with Pandas: {end-start}\")"
    },
    {
      "cell_type": "markdown",
      "id": "294550ee-81df-418a-89f7-343eb86de87b",
      "metadata": {
        "name": "cell28",
        "collapsed": false
      },
      "source": [
        "We can see that Snowpark runs much faster. This is because of the I/O overhead for converting a Snowpark dataframe to pandas. We can see that the bulk of the time spent is on I/O."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a975a94e-d2cf-4988-a186-bf671755cf5d",
      "metadata": {
        "name": "cell29",
        "language": "python",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "st.markdown(f\"I/O time to convert to Pandas dataframe: {end_mid-start}\")\nst.markdown(f\"Processing time with Pandas dataframe: {end-end_mid}\")\nst.markdown(f\"I/O account for {(end_mid-start)/(end-start)*100:.2f}% of processing time\")"
    },
    {
      "cell_type": "markdown",
      "id": "7d4eadf4-0922-400d-aa16-f8cc7c3a12e2",
      "metadata": {
        "name": "cell30",
        "collapsed": false
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "To prepare our data for our model, we'll need to handle the imbalanced data problem by upsampling our dataset. \n",
        "\n",
        "For this, we'll be using the `SMOTE` algorithm from the `imblearn` package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4489e32-a3ab-435d-abee-60396748dcec",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell31"
      },
      "outputs": [],
      "source": [
        "# Extract the training features\n",
        "features_names = [col for col in telco_churn_pdf.columns if col not in ['Churn']]\n",
        "features = telco_churn_pdf[features_names]\n",
        "\n",
        "# extract the target\n",
        "target = telco_churn_pdf['Churn']\n",
        "st.markdown(\"## Lets balance the dataset.\")\n",
        "# upsample the minority class in the dataset\n",
        "upsampler = SMOTE(random_state = 111)\n",
        "features, target = upsampler.fit_resample(features, target)\n",
        "st.dataframe(features.head())\n",
        "\n",
        "st.markdown(\"## Upsampled data.\")\n",
        "upsampled_data = pd.concat([features, target], axis=1)\n",
        "upsampled_data.reset_index(inplace=True)\n",
        "upsampled_data.rename(columns={'index': 'INDEX'}, inplace=True)\n",
        "st.dataframe(upsampled_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc051113-6bef-40a5-908b-38b4de5ff8c4",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell32",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": [
        "upsampled_data = session.create_dataframe(upsampled_data)\n",
        "# Get the list of column names from the dataset\n",
        "feature_names_input = [c for c in upsampled_data.columns if c != '\"Churn\"' and c != \"INDEX\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b33f5b5c-1181-4d68-90d3-342060389fec",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell33",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "upsampled_data[feature_names_input]"
    },
    {
      "cell_type": "markdown",
      "id": "9a1f5f7f-777a-4f26-82cc-213b6b3c04e0",
      "metadata": {
        "name": "cell34",
        "collapsed": false
      },
      "source": [
        "Once that's taken care of, we'll use scikit-learn to preprocess our data into a format that the model expects. This means scaling our features and splitting our data into training and testing datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6a09e11-586a-45dd-ba17-aff7d2f08fd4",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell35"
      },
      "source": [
        "We can perform StandardScaler preprocessing via sklearn to process in-memory or Snowpark ML preprocessing for pushdown compute. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e4e5b0-a52c-4468-929c-0624148753c0",
      "metadata": {
        "name": "cell36",
        "collapsed": false
      },
      "source": [
        "## Sci-kit learn Preprocessing with Pandas DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9533336-f5d3-461c-aa75-b604e23e44c1",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell37",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "import sklearn.preprocessing as pp_original\n# Initialize a StandardScaler object with input and output column names\nscaler = pp_original.StandardScaler()\nfeatures_pdf = upsampled_data[feature_names_input].to_pandas()\n\n# Fit the scaler to the dataset\nscaler.fit(features_pdf)\n\n# Transform the dataset using the fitted scaler\nscaled_features = scaler.transform(features_pdf)\nscaled_features = pd.DataFrame(scaled_features, columns = features_names)\nscaled_features"
    },
    {
      "cell_type": "markdown",
      "id": "83a35603-423c-4da8-9e4c-cb9d77c2e081",
      "metadata": {
        "name": "cell38",
        "collapsed": false
      },
      "source": [
        "## Snowpark ML preprocessing with Snowpark DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94cf9f2d-021b-4df0-904a-5ad1bdfde9a9",
      "metadata": {
        "name": "cell39"
      },
      "source": [
        "Note the similarity between the APIs used for sklearn and Snowpark ML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa37e57b-9410-47d8-9d88-d3d3438f3469",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell40",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "import snowflake.ml.modeling.preprocessing as pp\n\n# Initialize a StandardScaler object with input and output column names\nscaler = pp.StandardScaler(\n    input_cols=feature_names_input,\n    output_cols=feature_names_input\n)\n\n# Fit the scaler to the dataset\nscaler.fit(upsampled_data)\n\n# Transform the dataset using the fitted scaler\nscaled_features = scaler.transform(upsampled_data)\nscaled_features"
    },
    {
      "cell_type": "markdown",
      "id": "410681cf-c3b1-47dd-9b58-85c2b2afbc8f",
      "metadata": {
        "name": "cell41",
        "collapsed": false
      },
      "source": [
        "## Let's perform the train test split using 80/20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589337dc-d674-40d6-9b73-a9d35fa28086",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell42"
      },
      "outputs": [],
      "source": "# Split the scaled_features dataset into training and testing sets with an 80/20 ratio\ntraining, testing = scaled_features.random_split(weights=[0.8, 0.2], seed=111)"
    },
    {
      "cell_type": "markdown",
      "id": "fd84cf31-4ae6-4f9b-a805-cad7f0d1c38e",
      "metadata": {
        "name": "cell43",
        "collapsed": false
      },
      "source": [
        "# Model Training - Random Forest Classifier \n",
        "\n",
        "The mystery model of the day is a [random forest classifier](https://towardsdatascience.com/understanding-random-forest-58381e0602d2). I'll spare you the details on how it works, but in short, it creates an ensemble of smaller models that all make predictions on the same data. Whichever prediction has the most votes is the final prediction that the model goes with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15803558-604a-4312-b838-46c71ec74bf3",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell44"
      },
      "outputs": [],
      "source": "from snowflake.ml.modeling.ensemble import RandomForestClassifier\n\n# Define the target variable (label) column name\nlabel = ['\"Churn\"']\n\n# Define the output column name for the predicted label\noutput_label = ['\"predicted_churn\"']\n\n# Initialize a RandomForestClassifier object with input, label, and output column names\nmodel = RandomForestClassifier(\n    input_cols=feature_names_input,\n    label_cols=label,\n    output_cols=output_label,\n)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aba2aca-6227-443e-ae16-0fa69661cca7",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell45",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "# Train the RandomForestClassifier model using the training set\n_ = model.fit(training)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9acc6eba-17cf-4d3c-8343-4023612b693f",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell46",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": [
        "# Predict the target variable (churn) for the testing set using the trained model\n",
        "results = model.predict(testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6303824-d5b7-457d-9106-70cb821cc4d6",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell47",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": [
        "testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "701cf188-e74f-4de6-8f56-1569879ac2d4",
      "metadata": {
        "name": "cell48",
        "collapsed": false
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "Model evaluation is all about checking how well our machine learning model is doing by comparing its predictions to the actual outcomes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc8f6c8-5d53-4608-a56d-e0e959a27bd0",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell49"
      },
      "outputs": [],
      "source": [
        "# return only the predicted churn values\n",
        "predictions = results.to_pandas().sort_values(\"INDEX\")[output_label].astype(int).to_numpy().flatten()\n",
        "actual = testing.to_pandas().sort_values(\"INDEX\")[['Churn']].to_numpy().flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9fd435b-b357-4d86-bc00-976884b9d590",
      "metadata": {
        "name": "cell50",
        "collapsed": false
      },
      "source": [
        "## Feature Importance\n",
        "\n",
        "Feature importance is all about figuring out which input variables are the real MVPs when it comes to making predictions with our machine learning model. We'll find out which features are the most important by looking at how much they contribute to the model's overall performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "418c65c8-c6f2-4a63-b8cd-9c139a395a11",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell51"
      },
      "outputs": [],
      "source": [
        "rf = model.to_sklearn()\n",
        "importances = pd.DataFrame(\n",
        "    list(zip(features.columns, rf.feature_importances_)),\n",
        "    columns=[\"feature\", \"importance\"],\n",
        ")\n",
        "\n",
        "bar_chart = alt.Chart(importances).mark_bar().encode(\n",
        "    x=\"importance:Q\",\n",
        "    y=alt.Y(\"feature:N\", sort=\"-x\")\n",
        ")\n",
        "st.altair_chart(bar_chart, use_container_width=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ec2b044-eb45-4797-9032-8e48a01e9dee",
      "metadata": {
        "name": "cell52",
        "collapsed": false
      },
      "source": [
        "## Predicting churn for a new user\n",
        "Using our trained random forest model, we can make predictions that tell us whether a new customer will churn or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d0b210a-71f9-43f3-9047-e1d11afbb98f",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell53"
      },
      "outputs": [],
      "source": [
        "account_weeks = \"10\"\n",
        "data_usage = \"1.7\"\n",
        "mins_per_month = \"82\"\n",
        "daytime_calls = \"67\"\n",
        "customer_service_calls = \"4\"\n",
        "monthly_charge = \"37\"\n",
        "roam_mins = \"0\"\n",
        "overage_fee = \"9.5\"\n",
        "renewed_contract = \"true\"\n",
        "has_data_plan = \"true\"\n",
        "user_vector = np.array([\n",
        "    account_weeks,\n",
        "    1 if renewed_contract else 0,\n",
        "    1 if has_data_plan else 0,\n",
        "    data_usage,\n",
        "    customer_service_calls,\n",
        "    mins_per_month,\n",
        "    daytime_calls,\n",
        "    monthly_charge,\n",
        "    overage_fee,\n",
        "    roam_mins,\n",
        "]).reshape(1,-1)\n",
        "\n",
        "user_dataframe = pd.DataFrame(user_vector, columns=[f'\"{_}\"' for _ in features.columns])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "720ee104-ef6a-4aba-9292-22e8a8f2a971",
      "metadata": {
        "name": "cell54",
        "collapsed": false
      },
      "source": "#### Input dataframe for new user"
    },
    {
      "cell_type": "code",
      "id": "955ba6e7-38df-43a7-8a8e-8702bd03c491",
      "metadata": {
        "language": "python",
        "name": "cell55",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "user_dataframe",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2899f3dd-ba96-486d-aae1-ef956dc3a0a6",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell56",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "user_vector = scaler.transform(user_dataframe)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3ffec0f-1ad1-44a7-ba16-b7d332d85cf3",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell57",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "model.predict(user_vector)[['\"predicted_churn\"']].values"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9267afe2-0972-488a-b01a-554e63f5f38a",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell58",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "st.markdown(\"#### Scaled dataframe for new user\")\nst.dataframe(user_vector)\nst.markdown(\"#### Prediction\")\npredicted_value = model.predict(user_vector)[['\"predicted_churn\"']].values.astype(int).flatten()\nuser_probability = model.predict_proba(user_vector)\nprobability_of_prediction = max(user_probability[user_probability.columns[-2:]].values[0]) * 100\nprediction = 'churn' if predicted_value == 1 else 'not churn'\nst.markdown(prediction)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa7d04cd-eed2-4e42-8ac1-d70db0c53e4b",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell59",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "col1, col2 = st.columns(2)\n\nwith col1: \n    account_weeks = st.slider(\"AccountWeeks\", int(features[\"AccountWeeks\"].min()) , int(features[\"AccountWeeks\"].max()))\n    data_usage = st.slider(\"DataUsage\", int(features[\"DataUsage\"].min()) , int(features[\"DataUsage\"].max()))\n    mins_per_month = st.slider(\"DayMins\", int(features[\"DayMins\"].min()) , int(features[\"DayMins\"].max()))\n    daytime_calls = st.slider(\"DayCalls\", int(features[\"DayCalls\"].min()) , int(features[\"DayCalls\"].max()))\n    renewed_contract =  st.selectbox(\"Renewed Contract?\",('true','false'))\n    \nwith col2: \n    monthly_charge = st.slider(\"MonthlyCharge\", int(features[\"MonthlyCharge\"].min()) , int(features[\"MonthlyCharge\"].max()))\n    roam_mins = st.slider(\"RoamMins\", int(features[\"RoamMins\"].min()) , int(features[\"RoamMins\"].max()))\n    customer_service_calls = st.slider(\"CustServCalls\", int(features[\"CustServCalls\"].min()) , int(features[\"CustServCalls\"].max()))\n    overage_fee = st.slider(\"OverageFee\", int(features[\"OverageFee\"].min()) , int(features[\"OverageFee\"].max()))\n    has_data_plan = st.selectbox(\"Has Data Plan?\",('true','false'))\n\nuser_vector = np.array([\n    account_weeks,\n    1 if renewed_contract else 0,\n    1 if has_data_plan else 0,\n    data_usage,\n    customer_service_calls,\n    mins_per_month,\n    daytime_calls,\n    monthly_charge,\n    overage_fee,\n    roam_mins,\n]).reshape(1,-1)\n\nuser_dataframe = pd.DataFrame(user_vector, columns=[f'\"{_}\"' for _ in features.columns])\nuser_vector = scaler.transform(user_dataframe)\nwith col1: \n    st.markdown(\"#### Input dataframe for new user\")\n    st.dataframe(user_dataframe)\nwith col2:\n    st.markdown(\"#### Scaled dataframe for new user\")\n    st.dataframe(user_vector)\n\nst.markdown(\"#### Prediction\")\npredicted_value = model.predict(user_vector)[['\"predicted_churn\"']].values.astype(int).flatten()\nuser_probability = model.predict_proba(user_vector)\nprobability_of_prediction = max(user_probability[user_probability.columns[-2:]].values[0]) * 100\nprediction = 'churn' if predicted_value == 1 else 'not churn'\nst.markdown(prediction)"
    },
    {
      "cell_type": "markdown",
      "id": "d2e7e9b5-b23f-4b20-800e-65237530e4ec",
      "metadata": {
        "name": "cell60",
        "collapsed": false
      },
      "source": [
        "## Exporting Model with Timestamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baed87c3-3bd1-40cb-b8d3-7b4be39702fe",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell61"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import datetime\n",
        "filename = f'telco-eda-model-{datetime.datetime.now()}.pkl'\n",
        "\n",
        "pickle.dump(model, open(filename,'wb'))\n",
        "print(f\"Saved to {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "176f5afc-6838-44b0-9bc0-61612a691b58",
      "metadata": {
        "name": "cell62",
        "collapsed": false
      },
      "source": "Congratulations on making it to the end of this Lab where we explored churn modeling using Snowflake Notebooks! We learned how to import/load data to Snowflake, train a Random Forest model, visualize predictions, and build an interactive data app, and make predictions for new users.\n"
    }
  ]
}