{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell1"
      },
      "source": [
        "# Select Packages\n",
        "\n",
        "To get started, let's select a few packages that we will need. In the **Packages** drop-down picker in the top right of the UI, search for and add the following packages by clicking on them:\n",
        "\n",
        "- snowflake-ml-python\n",
        "\n",
        "Once you add the packages, click the **Start** button! Once it says **Active**, you're ready to run the rest of the Notebook."
      ],
      "id": "34f846ea-b4b0-4657-b499-cd8b5b1eb380"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell2"
      },
      "source": [
        "## 1. Data Ingestion\n",
        "\n",
        "The `diamonds` dataset has been widely used in data science and machine learning. We will use it to demonstrate Snowflake's native data science transformers in terms of database functionality and Spark & Pandas comportablity, using non-synthetic and statistically appropriate data that is well known to the ML community.\n",
        "\n"
      ],
      "id": "1f4adaa3-8ef6-4c8f-8057-1d12bf900962"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell3"
      },
      "source": [
        "### Import Libraries"
      ],
      "id": "6fe2a4b4-821f-410c-8d2c-e527829b106e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell4"
      },
      "outputs": [],
      "source": "# Snowpark for Python\nfrom snowflake.snowpark.types import DoubleType\nimport snowflake.snowpark.functions as F",
      "id": "7eed4494-06ab-43b2-86e8-879c99c1a2c0"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell5"
      },
      "source": [
        "### Setup and establish Secure Connection to Snowflake\n",
        "\n",
        "Notebooks establish a Snowpark Session when the notebook is attached to the kernel. We create a new warehouse, database, and schema that will be used throughout this tutorial."
      ],
      "id": "39326625-0d17-438f-a847-8d1e2c1488da"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "sql",
        "name": "cell6"
      },
      "outputs": [],
      "source": [
        "CREATE OR REPLACE WAREHOUSE ML_HOL_WH; --by default, this creates an XS Standard Warehouse\n",
        "CREATE OR REPLACE DATABASE ML_HOL_DB;\n",
        "CREATE OR REPLACE SCHEMA ML_HOL_SCHEMA;\n",
        "CREATE OR REPLACE STAGE ML_HOL_ASSETS; --to store model assets"
      ],
      "id": "f2c3d0f5-309d-43f8-9f31-9ace562f4e1e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell7"
      },
      "outputs": [],
      "source": [
        "# Get Snowflake Session object\n",
        "session = get_active_session()\n",
        "session.sql_simplifier_enabled = True\n",
        "\n",
        "# Current Environment Details\n",
        "print('Connection Established with the following parameters:')\n",
        "print('User      : {}'.format(session.get_current_user()))\n",
        "print('Role      : {}'.format(session.get_current_role()))\n",
        "print('Database  : {}'.format(session.get_current_database()))\n",
        "print('Schema    : {}'.format(session.get_current_schema()))\n",
        "print('Warehouse : {}'.format(session.get_current_warehouse()))"
      ],
      "id": "046fa3ea-5ea9-4af6-a4dd-88c7101dcf0d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell8"
      },
      "source": [
        "### Setup and Load CSV data to stage\n",
        " Then we create a external S3 stage for the `diamonds.csv` file stored in a public S3 bucket."
      ],
      "id": "570d6438-bcb0-44be-b397-471a3ba0eb0a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "sql",
        "name": "cell9"
      },
      "outputs": [],
      "source": [
        "-- Create csv format\n",
        "CREATE FILE FORMAT IF NOT EXISTS CSVFORMAT \n",
        "    SKIP_HEADER = 1 \n",
        "    TYPE = 'CSV';\n",
        "\n",
        "-- Create external stage with the csv format to stage the diamonds dataset\n",
        "CREATE STAGE IF NOT EXISTS DIAMONDS_ASSETS \n",
        "    FILE_FORMAT =  CSVFORMAT \n",
        "    URL = 's3://sfquickstarts/intro-to-machine-learning-with-snowpark-ml-for-python/diamonds.csv';\n",
        "    \n",
        "-- Inspect content of stage\n",
        "LS @DIAMONDS_ASSETS;"
      ],
      "id": "edf8e804-ef4e-471a-96fc-560eb97467e6"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell10"
      },
      "source": [
        "### Use the Snowpark DataFrame Reader to read in data from the externally staged `diamonds` CSV file \n",
        "\n",
        "In setup.sql, we staged the `diamonds.csv` file from an external s3 bucket. Now, we can read it in.\n",
        "\n",
        "For more information on loading data, see documentation on [snowflake.snowpark.DataFrameReader](https://docs.snowflake.com/ko/developer-guide/snowpark/reference/python/api/snowflake.snowpark.DataFrameReader.html).\n",
        "\n",
        "\n"
      ],
      "id": "e9f22ce6-da50-4d76-98e1-3ff6246ed220"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell11"
      },
      "outputs": [],
      "source": [
        "# Create a Snowpark DataFrame that is configured to load data from the CSV file\n",
        "# We can now infer schema from CSV files.\n",
        "diamonds_df = session.read.options({\"field_delimiter\": \",\",\n",
        "                                    \"field_optionally_enclosed_by\": '\"',\n",
        "                                    \"infer_schema\": True,\n",
        "                                    \"parse_header\": True}).csv(\"@DIAMONDS_ASSETS\")\n",
        "\n",
        "diamonds_df"
      ],
      "id": "6080dc92-595e-48b6-b4fc-667e2346bed9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell12"
      },
      "outputs": [],
      "source": [
        "# Look at descriptive stats on the DataFrame\n",
        "diamonds_df.describe()"
      ],
      "id": "d04f72db-d8b4-4f25-aaf0-928475009895"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell13"
      },
      "outputs": [],
      "source": [
        "diamonds_df.columns"
      ],
      "id": "8205f888-1a4a-4033-b4f4-ddbb1bc30e67"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell14"
      },
      "source": [
        "### Data cleaning\n",
        "\n",
        "First, let's force headers to uppercase using Snowpark DataFrame operations for standardization when columns are later written to a Snowflake table."
      ],
      "id": "8babcdbe-083d-4e4e-b7ec-ac86982e775d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell15"
      },
      "outputs": [],
      "source": [
        "# Force headers to uppercase\n",
        "for colname in diamonds_df.columns:\n",
        "    if colname == '\"table\"':\n",
        "       new_colname = \"TABLE_PCT\"\n",
        "    else:\n",
        "        new_colname = str.upper(colname)\n",
        "    diamonds_df = diamonds_df.with_column_renamed(colname, new_colname)\n",
        "\n",
        "diamonds_df"
      ],
      "id": "fcd0ce8f-0f7d-4cc9-ad07-f59d5c8e67a2"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell16"
      },
      "source": [
        "Next, we standardize the category formatting for `CUT` using Snowpark DataFrame operations.\n",
        "\n",
        "This way, when we write to a Snowflake table, there will be no inconsistencies in how the Snowpark DataFrame will read in the category names. Secondly, the feature transformations on categoricals will be easier to encode."
      ],
      "id": "93794112-57e4-442e-b598-8c87aa31594e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell17"
      },
      "outputs": [],
      "source": [
        "def fix_values(columnn):\n",
        "    return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n",
        "\n",
        "for col in [\"CUT\"]:\n",
        "    diamonds_df = diamonds_df.with_column(col, fix_values(col))\n",
        "\n",
        "diamonds_df"
      ],
      "id": "b03328f0-f85a-4296-bfce-923a44aeadf7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell18"
      },
      "source": [
        "Check the schema."
      ],
      "id": "654c8a95-271b-440b-a579-8f42d8f6f135"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell19"
      },
      "outputs": [],
      "source": [
        "list(diamonds_df.schema)"
      ],
      "id": "989ec169-24e9-477f-8170-a5c332249269"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell20"
      },
      "source": [
        "Finally, let's cast the decimal types to DoubleType() since DecimalType() isn't support by Snowpark ML at the moment."
      ],
      "id": "a09273ca-c6a9-461f-8697-c703b62b986f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell21"
      },
      "outputs": [],
      "source": [
        "for colname in [\"CARAT\", \"X\", \"Y\", \"Z\", \"DEPTH\", \"TABLE_PCT\"]:\n",
        "    diamonds_df = diamonds_df.with_column(colname, diamonds_df[colname].cast(DoubleType()))\n",
        "\n",
        "diamonds_df"
      ],
      "id": "988261a8-7619-48c8-8241-6aa490e6d2d4"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell22"
      },
      "source": [
        "### Write cleaned data to a Snowflake table"
      ],
      "id": "54aea408-5f24-4fc1-9add-de83caed2b05"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell23"
      },
      "outputs": [],
      "source": [
        "diamonds_df.write.mode('overwrite').save_as_table('diamonds')"
      ],
      "id": "0d00d0c5-157e-461b-b3e7-6577e081935d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell24"
      },
      "source": [
        "In the next notebook, we will perform data transformations with the Snowpark ML Preprocessing API for feature engineering. "
      ],
      "id": "57a0fb14-608b-49c0-87e8-721c71cb5688"
    }
  ]
}