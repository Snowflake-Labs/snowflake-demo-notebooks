{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell1"
      },
      "source": [
        "# Select Packages\n",
        "\n",
        "To get started, let's select a few packages that we will need. In the **Packages** drop-down picker in the top right of the UI, search for and add the following packages by clicking on them:\n",
        "\n",
        "- snowflake-ml-python\n",
        "\n",
        "Once you add the packages, click the **Start** button! Once it says **Active**, you're ready to run the rest of the Notebook."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell2"
      },
      "source": [
        "## 1. Data Ingestion\n",
        "\n",
        "The `diamonds` dataset has been widely used in data science and machine learning. We will use it to demonstrate Snowflake's native data science transformers in terms of database functionality and Spark & Pandas comportablity, using non-synthetic and statistically appropriate data that is well known to the ML community.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell3"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell4"
      },
      "outputs": [],
      "source": [
        "# Snowpark for Python\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.snowpark.version import VERSION\n",
        "from snowflake.snowpark.types import StructType, StructField, DoubleType, StringType\n",
        "import snowflake.snowpark.functions as F"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell5"
      },
      "source": [
        "### Setup and establish Secure Connection to Snowflake\n",
        "\n",
        "Notebooks establish a Snowpark Session when the notebook is attached to the kernel. We create a new warehouse, database, and schema that will be used throughout this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "sql",
        "name": "cell6"
      },
      "outputs": [],
      "source": [
        "CREATE OR REPLACE WAREHOUSE ML_HOL_WH; --by default, this creates an XS Standard Warehouse\n",
        "CREATE OR REPLACE DATABASE ML_HOL_DB;\n",
        "CREATE OR REPLACE SCHEMA ML_HOL_SCHEMA;\n",
        "CREATE OR REPLACE STAGE ML_HOL_ASSETS; --to store model assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell7"
      },
      "outputs": [],
      "source": [
        "# Get Snowflake Session object\n",
        "session = get_active_session()\n",
        "session.sql_simplifier_enabled = True\n",
        "\n",
        "# Current Environment Details\n",
        "print('Connection Established with the following parameters:')\n",
        "print('User      : {}'.format(session.get_current_user()))\n",
        "print('Role      : {}'.format(session.get_current_role()))\n",
        "print('Database  : {}'.format(session.get_current_database()))\n",
        "print('Schema    : {}'.format(session.get_current_schema()))\n",
        "print('Warehouse : {}'.format(session.get_current_warehouse()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell8"
      },
      "source": [
        "### Setup and Load CSV data to stage\n",
        " Then we create a external S3 stage for the `diamonds.csv` file stored in a public S3 bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "sql",
        "name": "cell9"
      },
      "outputs": [],
      "source": [
        "-- Create csv format\n",
        "CREATE FILE FORMAT IF NOT EXISTS CSVFORMAT \n",
        "    SKIP_HEADER = 1 \n",
        "    TYPE = 'CSV';\n",
        "\n",
        "-- Create external stage with the csv format to stage the diamonds dataset\n",
        "CREATE STAGE IF NOT EXISTS DIAMONDS_ASSETS \n",
        "    FILE_FORMAT =  CSVFORMAT \n",
        "    URL = 's3://sfquickstarts/intro-to-machine-learning-with-snowpark-ml-for-python/diamonds.csv';\n",
        "    \n",
        "-- Inspect content of stage\n",
        "LS @DIAMONDS_ASSETS;"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell10"
      },
      "source": [
        "### Use the Snowpark DataFrame Reader to read in data from the externally staged `diamonds` CSV file \n",
        "\n",
        "In setup.sql, we staged the `diamonds.csv` file from an external s3 bucket. Now, we can read it in.\n",
        "\n",
        "For more information on loading data, see documentation on [snowflake.snowpark.DataFrameReader](https://docs.snowflake.com/ko/developer-guide/snowpark/reference/python/api/snowflake.snowpark.DataFrameReader.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell11"
      },
      "outputs": [],
      "source": [
        "# Create a Snowpark DataFrame that is configured to load data from the CSV file\n",
        "# We can now infer schema from CSV files.\n",
        "diamonds_df = session.read.options({\"field_delimiter\": \",\",\n",
        "                                    \"field_optionally_enclosed_by\": '\"',\n",
        "                                    \"infer_schema\": True,\n",
        "                                    \"parse_header\": True}).csv(\"@DIAMONDS_ASSETS\")\n",
        "\n",
        "diamonds_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell12"
      },
      "outputs": [],
      "source": [
        "# Look at descriptive stats on the DataFrame\n",
        "diamonds_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell13"
      },
      "outputs": [],
      "source": [
        "diamonds_df.columns"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell14"
      },
      "source": [
        "### Data cleaning\n",
        "\n",
        "First, let's force headers to uppercase using Snowpark DataFrame operations for standardization when columns are later written to a Snowflake table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell15"
      },
      "outputs": [],
      "source": [
        "# Force headers to uppercase\n",
        "for colname in diamonds_df.columns:\n",
        "    if colname == '\"table\"':\n",
        "       new_colname = \"TABLE_PCT\"\n",
        "    else:\n",
        "        new_colname = str.upper(colname)\n",
        "    diamonds_df = diamonds_df.with_column_renamed(colname, new_colname)\n",
        "\n",
        "diamonds_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell16"
      },
      "source": [
        "Next, we standardize the category formatting for `CUT` using Snowpark DataFrame operations.\n",
        "\n",
        "This way, when we write to a Snowflake table, there will be no inconsistencies in how the Snowpark DataFrame will read in the category names. Secondly, the feature transformations on categoricals will be easier to encode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell17"
      },
      "outputs": [],
      "source": [
        "def fix_values(columnn):\n",
        "    return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n",
        "\n",
        "for col in [\"CUT\"]:\n",
        "    diamonds_df = diamonds_df.with_column(col, fix_values(col))\n",
        "\n",
        "diamonds_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell18"
      },
      "source": [
        "Check the schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell19"
      },
      "outputs": [],
      "source": [
        "list(diamonds_df.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell20"
      },
      "source": [
        "Finally, let's cast the decimal types to DoubleType() since DecimalType() isn't support by Snowpark ML at the moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell21"
      },
      "outputs": [],
      "source": [
        "for colname in [\"CARAT\", \"X\", \"Y\", \"Z\", \"DEPTH\", \"TABLE_PCT\"]:\n",
        "    diamonds_df = diamonds_df.with_column(colname, diamonds_df[colname].cast(DoubleType()))\n",
        "\n",
        "diamonds_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell22"
      },
      "source": [
        "### Write cleaned data to a Snowflake table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell23"
      },
      "outputs": [],
      "source": [
        "diamonds_df.write.mode('overwrite').save_as_table('diamonds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell24"
      },
      "source": [
        "In the next notebook, we will perform data transformations with the Snowpark ML Preprocessing API for feature engineering. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}