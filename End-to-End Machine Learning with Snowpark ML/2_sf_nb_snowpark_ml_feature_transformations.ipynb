{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell1"
      },
      "source": [
        "# Select Packages\n",
        "\n",
        "To get started, let's select a few packages that we will need. In the **Packages** drop-down picker in the top right of the UI, search for and add the following packages by clicking on them:\n",
        "\n",
        "- snowflake-ml-python\n",
        "- seaborn\n",
        "- matplotlib\n",
        "\n",
        "Once you add the packages, click the **Start** button! Once it says **Active**, you're ready to run the rest of the Notebook."
      ],
      "id": "a335757a-b965-469a-959a-a1ae5752e20e"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell2"
      },
      "source": [
        "## 2. ML Feature Transformations\n",
        "\n",
        "- In this notebook, we will walk through a few transformations that are included in the Snowpark ML Preprocessing API. \n",
        "- We will also build a preprocessing pipeline to be used in the ML modeling notebook.\n",
        "\n",
        "***Note: All feature transformations using Snowpark ML are distributed operations in the same way that Snowpark DataFrame operations are.***"
      ],
      "id": "c0f7cf92-65a9-48b2-9212-d8807ba454bc"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell3"
      },
      "source": [
        "### Import Libraries"
      ],
      "id": "96992c21-d30a-400e-a034-0bfd96a1200f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell4"
      },
      "outputs": [],
      "source": "# Snowpark for Python\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.types import DecimalType\n\n# Snowpark ML\nimport snowflake.ml.modeling.preprocessing as snowml\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.metrics.correlation import correlation\n\n# Data Science Libs\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Misc\nimport json\nimport joblib\n\n# warning suppresion\nimport warnings; warnings.simplefilter('ignore')",
      "id": "5fd506e2-20a7-43fd-9630-7fc0468c9652"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell5"
      },
      "source": [
        "### Establish Secure Connection to Snowflake\n",
        "\n",
        "Notebooks establish a Snowpark Session when the notebook is attached to the kernel. Let's use that Session object to validate our connection."
      ],
      "id": "69454482-73d4-4d81-90ad-374152e5bcb9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "name": "cell6"
      },
      "outputs": [],
      "source": [
        "-- Using Warehouse, Database, and Schema created in tutorial #1\n",
        "USE WAREHOUSE ML_HOL_WH;\n",
        "USE DATABASE ML_HOL_DB;\n",
        "USE SCHEMA ML_HOL_SCHEMA;"
      ],
      "id": "0414c504-5aa4-48ed-bcee-c8aab2c5105c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell7"
      },
      "outputs": [],
      "source": [
        "session = get_active_session()\n",
        "session"
      ],
      "id": "a6aeaefb-b580-47d3-8382-e915e33b1d4c"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell8"
      },
      "source": [
        "### Data Loading"
      ],
      "id": "71ac9d3f-0e4e-466c-b500-868ab63b69e9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell9"
      },
      "outputs": [],
      "source": [
        "# First, we read in the data from a Snowflake table into a Snowpark DataFrame\n",
        "# **Change this only if you named your table something else in the data ingest notebook **\n",
        "diamonds_df = session.table(\"DIAMONDS\")"
      ],
      "id": "29aa8b77-5ab8-4953-b7b9-2c023a1d03cc"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell10"
      },
      "source": [
        "### Feature Transformations\n",
        "\n",
        "We will illustrate a few of the transformation functions here, but the rest can be found in the [documentation](https://docs.snowflake.com/LIMITEDACCESS/snowflake-ml-preprocessing)."
      ],
      "id": "7a052fa9-922e-4d72-a41d-f1795bfac6ea"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell11"
      },
      "source": [
        "##### Let's use the `MinMaxScaler` to normalize the `CARAT` column."
      ],
      "id": "265507af-8e9c-4d8a-987d-914d249309c2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell12"
      },
      "outputs": [],
      "source": [
        "# Normalize the CARAT column\n",
        "snowml_mms = snowml.MinMaxScaler(input_cols=[\"CARAT\"], output_cols=[\"CARAT_NORM\"])\n",
        "normalized_diamonds_df = snowml_mms.fit(diamonds_df).transform(diamonds_df)\n",
        "\n",
        "# Reduce the number of decimals\n",
        "new_col = normalized_diamonds_df.col(\"CARAT_NORM\").cast(DecimalType(7, 6))\n",
        "normalized_diamonds_df = normalized_diamonds_df.with_column(\"CARAT_NORM\", new_col)\n",
        "\n",
        "normalized_diamonds_df"
      ],
      "id": "00c962dc-1724-4eec-a3b6-bd255e9df4bf"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell13"
      },
      "source": [
        "##### Let's use the `OrdinalEncoder` to transform `COLOR` and `CLARITY` from categorical to numerical values so they are more meaningful."
      ],
      "id": "3d5292e4-613f-4f4c-9579-aa338cf0f6eb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell14"
      },
      "outputs": [],
      "source": [
        "# Encode CUT and CLARITY preserve ordinal importance\n",
        "categories = {\n",
        "    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n",
        "    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n",
        "}\n",
        "snowml_oe = snowml.OrdinalEncoder(input_cols=[\"CUT\", \"CLARITY\"], output_cols=[\"CUT_OE\", \"CLARITY_OE\"], categories=categories)\n",
        "ord_encoded_diamonds_df = snowml_oe.fit(normalized_diamonds_df).transform(normalized_diamonds_df)\n",
        "\n",
        "# Show the encoding\n",
        "print(snowml_oe._state_pandas)\n",
        "\n",
        "ord_encoded_diamonds_df"
      ],
      "id": "4cf97705-c30b-48e9-b152-76e0ccc3a818"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell15"
      },
      "source": [
        "##### Let's use the `OneHotEncoder` to transform the categorical columns to numerical columns.\n",
        "\n",
        "This is more for illustration purposes. Using the OrdinalEncoder makes more sense for the diamonds dataset since `CARAT`, `COLOR`, and `CLARITY` all follow a natural ranking order."
      ],
      "id": "412bfa35-f9dd-459a-99b3-5dd5d846f9e6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell16"
      },
      "outputs": [],
      "source": [
        "# Encode categoricals to numeric columns\n",
        "snowml_ohe = snowml.OneHotEncoder(input_cols=[\"CUT\", \"COLOR\", \"CLARITY\"], output_cols=[\"CUT_OHE\", \"COLOR_OHE\", \"CLARITY_OHE\"])\n",
        "transformed_diamonds_df = snowml_ohe.fit(ord_encoded_diamonds_df).transform(ord_encoded_diamonds_df)\n",
        "\n",
        "np.array(transformed_diamonds_df.columns)"
      ],
      "id": "95781b03-1996-4171-8ebd-0a41f676a359"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell17"
      },
      "source": [
        "##### Finally, we can also build out a full preprocessing `Pipeline`.\n",
        "\n",
        "This will be useful for both the ML training & inference steps to have standarized feature transformations."
      ],
      "id": "950944f6-de76-4820-a185-18feb4b24462"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell18"
      },
      "outputs": [],
      "source": [
        "# Categorize all the features for processing\n",
        "CATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\n",
        "CATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\n",
        "NUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n",
        "\n",
        "categories = {\n",
        "    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n",
        "    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n",
        "    \"COLOR\": np.array(['D', 'E', 'F', 'G', 'H', 'I', 'J']),\n",
        "}"
      ],
      "id": "e1cd00f7-46ac-46d4-804e-2894b01a1197"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell19"
      },
      "outputs": [],
      "source": [
        "# Build the pipeline\n",
        "preprocessing_pipeline = Pipeline(\n",
        "    steps=[\n",
        "            (\n",
        "                \"OE\",\n",
        "                snowml.OrdinalEncoder(\n",
        "                    input_cols=CATEGORICAL_COLUMNS,\n",
        "                    output_cols=CATEGORICAL_COLUMNS_OE,\n",
        "                    categories=categories,\n",
        "                )\n",
        "            ),\n",
        "            (\n",
        "                \"MMS\",\n",
        "                snowml.MinMaxScaler(\n",
        "                    clip=True,\n",
        "                    input_cols=NUMERICAL_COLUMNS,\n",
        "                    output_cols=NUMERICAL_COLUMNS,\n",
        "                )\n",
        "            )\n",
        "    ]\n",
        ")\n",
        "\n",
        "PIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib'\n",
        "joblib.dump(preprocessing_pipeline, PIPELINE_FILE) # We are just pickling it locally first\n",
        "\n",
        "transformed_diamonds_df = preprocessing_pipeline.fit(diamonds_df).transform(diamonds_df)\n",
        "transformed_diamonds_df"
      ],
      "id": "4082f2e2-a2ae-4858-9e4c-a0fd1540dd4d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell20"
      },
      "outputs": [],
      "source": [
        "# You can also save the pickled object into the stage we created earlier for deployment\n",
        "session.file.put(PIPELINE_FILE, \"@ML_HOL_ASSETS\", overwrite=True)"
      ],
      "id": "160f2c47-584a-4270-9c53-0a401eccc720"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell21"
      },
      "source": [
        "### Data Exploration\n",
        "\n",
        "Now that we've transformed our features, let's calculate the correlation using Snowpark ML's `correlation()` function between each pair to better understand their relationships.\n",
        "\n",
        "*Note: Snowpark ML's pearson correlation function returns a Pandas DataFrame*"
      ],
      "id": "d4bd6167-3f3f-4236-a962-fd5fd019bc12"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell22"
      },
      "outputs": [],
      "source": [
        "corr_diamonds_df = correlation(df=transformed_diamonds_df)\n",
        "corr_diamonds_df # This is a Pandas DataFrame"
      ],
      "id": "0b5fb644-2cf4-4721-82e4-311465d14ad2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell23"
      },
      "outputs": [],
      "source": [
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr_diamonds_df, dtype=bool))\n",
        "\n",
        "# Create a heatmap with the features\n",
        "plt.figure(figsize=(7, 7))\n",
        "heatmap = sns.heatmap(corr_diamonds_df, mask=mask, cmap=\"YlGnBu\", annot=True, vmin=-1, vmax=1)"
      ],
      "id": "b8d8c572-5119-4288-9d32-c42f157aaf37"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell24"
      },
      "source": [
        "We note that `CARAT` and `PRICE` are highly correlated, which makes sense! Let's take a look at their relationship a bit closer.\n",
        "\n",
        "*Note: You will have to convert your Snowpark DF to a Pandas DF in order to use matplotlib & seaborn.*"
      ],
      "id": "ba002b6c-9819-46cf-8b6a-7ff589150e49"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell25"
      },
      "outputs": [],
      "source": [
        "# Set up a plot to look at CARAT and PRICE\n",
        "counts = transformed_diamonds_df.to_pandas().groupby(['PRICE', 'CARAT']).size().reset_index(name='Count')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax = sns.scatterplot(data=counts, x='CARAT', y='PRICE', size='Count', markers='o', alpha=(0.1, .25, 0.5, 0.75, 1))\n",
        "ax.grid(axis='y')\n",
        "\n",
        "# The relationship is not linear - it appears exponential which makes sense given the rarity of the large diamonds\n",
        "sns.move_legend(ax, \"upper left\")\n",
        "sns.despine(left=True, bottom=True)"
      ],
      "id": "9cb10563-3acc-4db4-98fa-4e5b0dc9d553"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "cell26"
      },
      "source": [
        "In the next notebook, we will look at how you can train an XGBoost model with the diamonds dataset using the Snowpark ML Modeling API."
      ],
      "id": "a29501c1-3653-4b51-81bb-439c35383db7"
    }
  ]
}