{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell1"
      },
      "source": [
        "# Select Packages\n",
        "\n",
        "To get started, let's select a few packages that we will need. In the **Packages** drop-down picker in the top right of the UI, search for and add the following packages by clicking on them:\n",
        "\n",
        "- snowflake-ml-python\n",
        "- seaborn\n",
        "- matplotlib\n",
        "\n",
        "Once you add the packages, click the **Start** button! Once it says **Active**, you're ready to run the rest of the Notebook."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell2"
      },
      "source": [
        "## 2. ML Feature Transformations\n",
        "\n",
        "- In this notebook, we will walk through a few transformations that are included in the Snowpark ML Preprocessing API. \n",
        "- We will also build a preprocessing pipeline to be used in the ML modeling notebook.\n",
        "\n",
        "***Note: All feature transformations using Snowpark ML are distributed operations in the same way that Snowpark DataFrame operations are.***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell3"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell4"
      },
      "outputs": [],
      "source": [
        "# Snowpark for Python\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.snowpark.version import VERSION\n",
        "import snowflake.snowpark.functions as F\n",
        "from snowflake.snowpark.types import DecimalType\n",
        "\n",
        "# Snowpark ML\n",
        "import snowflake.ml.modeling.preprocessing as snowml\n",
        "from snowflake.ml.modeling.pipeline import Pipeline\n",
        "from snowflake.ml.modeling.metrics.correlation import correlation\n",
        "\n",
        "# Data Science Libs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Misc\n",
        "import json\n",
        "import joblib\n",
        "\n",
        "# warning suppresion\n",
        "import warnings; warnings.simplefilter('ignore')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell5"
      },
      "source": [
        "### Establish Secure Connection to Snowflake\n",
        "\n",
        "Notebooks establish a Snowpark Session when the notebook is attached to the kernel. Let's use that Session object to validate our connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "name": "cell6"
      },
      "outputs": [],
      "source": [
        "-- Using Warehouse, Database, and Schema created in tutorial #1\n",
        "USE WAREHOUSE ML_HOL_WH;\n",
        "USE DATABASE ML_HOL_DB;\n",
        "USE SCHEMA ML_HOL_SCHEMA;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell7"
      },
      "outputs": [],
      "source": [
        "session = get_active_session()\n",
        "session"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "name": "cell8"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell9"
      },
      "outputs": [],
      "source": [
        "# First, we read in the data from a Snowflake table into a Snowpark DataFrame\n",
        "# **Change this only if you named your table something else in the data ingest notebook **\n",
        "diamonds_df = session.table(\"DIAMONDS\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell10"
      },
      "source": [
        "### Feature Transformations\n",
        "\n",
        "We will illustrate a few of the transformation functions here, but the rest can be found in the [documentation](https://docs.snowflake.com/LIMITEDACCESS/snowflake-ml-preprocessing)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell11"
      },
      "source": [
        "##### Let's use the `MinMaxScaler` to normalize the `CARAT` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell12"
      },
      "outputs": [],
      "source": [
        "# Normalize the CARAT column\n",
        "snowml_mms = snowml.MinMaxScaler(input_cols=[\"CARAT\"], output_cols=[\"CARAT_NORM\"])\n",
        "normalized_diamonds_df = snowml_mms.fit(diamonds_df).transform(diamonds_df)\n",
        "\n",
        "# Reduce the number of decimals\n",
        "new_col = normalized_diamonds_df.col(\"CARAT_NORM\").cast(DecimalType(7, 6))\n",
        "normalized_diamonds_df = normalized_diamonds_df.with_column(\"CARAT_NORM\", new_col)\n",
        "\n",
        "normalized_diamonds_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell13"
      },
      "source": [
        "##### Let's use the `OrdinalEncoder` to transform `COLOR` and `CLARITY` from categorical to numerical values so they are more meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "cell14"
      },
      "outputs": [],
      "source": [
        "# Encode CUT and CLARITY preserve ordinal importance\n",
        "categories = {\n",
        "    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n",
        "    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n",
        "}\n",
        "snowml_oe = snowml.OrdinalEncoder(input_cols=[\"CUT\", \"CLARITY\"], output_cols=[\"CUT_OE\", \"CLARITY_OE\"], categories=categories)\n",
        "ord_encoded_diamonds_df = snowml_oe.fit(normalized_diamonds_df).transform(normalized_diamonds_df)\n",
        "\n",
        "# Show the encoding\n",
        "print(snowml_oe._state_pandas)\n",
        "\n",
        "ord_encoded_diamonds_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell15"
      },
      "source": [
        "##### Let's use the `OneHotEncoder` to transform the categorical columns to numerical columns.\n",
        "\n",
        "This is more for illustration purposes. Using the OrdinalEncoder makes more sense for the diamonds dataset since `CARAT`, `COLOR`, and `CLARITY` all follow a natural ranking order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell16"
      },
      "outputs": [],
      "source": [
        "# Encode categoricals to numeric columns\n",
        "snowml_ohe = snowml.OneHotEncoder(input_cols=[\"CUT\", \"COLOR\", \"CLARITY\"], output_cols=[\"CUT_OHE\", \"COLOR_OHE\", \"CLARITY_OHE\"])\n",
        "transformed_diamonds_df = snowml_ohe.fit(ord_encoded_diamonds_df).transform(ord_encoded_diamonds_df)\n",
        "\n",
        "np.array(transformed_diamonds_df.columns)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell17"
      },
      "source": [
        "##### Finally, we can also build out a full preprocessing `Pipeline`.\n",
        "\n",
        "This will be useful for both the ML training & inference steps to have standarized feature transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell18"
      },
      "outputs": [],
      "source": [
        "# Categorize all the features for processing\n",
        "CATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\n",
        "CATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\n",
        "NUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n",
        "\n",
        "categories = {\n",
        "    \"CUT\": np.array([\"IDEAL\", \"PREMIUM\", \"VERY_GOOD\", \"GOOD\", \"FAIR\"]),\n",
        "    \"CLARITY\": np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]),\n",
        "    \"COLOR\": np.array(['D', 'E', 'F', 'G', 'H', 'I', 'J']),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell19"
      },
      "outputs": [],
      "source": [
        "# Build the pipeline\n",
        "preprocessing_pipeline = Pipeline(\n",
        "    steps=[\n",
        "            (\n",
        "                \"OE\",\n",
        "                snowml.OrdinalEncoder(\n",
        "                    input_cols=CATEGORICAL_COLUMNS,\n",
        "                    output_cols=CATEGORICAL_COLUMNS_OE,\n",
        "                    categories=categories,\n",
        "                )\n",
        "            ),\n",
        "            (\n",
        "                \"MMS\",\n",
        "                snowml.MinMaxScaler(\n",
        "                    clip=True,\n",
        "                    input_cols=NUMERICAL_COLUMNS,\n",
        "                    output_cols=NUMERICAL_COLUMNS,\n",
        "                )\n",
        "            )\n",
        "    ]\n",
        ")\n",
        "\n",
        "PIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib'\n",
        "joblib.dump(preprocessing_pipeline, PIPELINE_FILE) # We are just pickling it locally first\n",
        "\n",
        "transformed_diamonds_df = preprocessing_pipeline.fit(diamonds_df).transform(diamonds_df)\n",
        "transformed_diamonds_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell20"
      },
      "outputs": [],
      "source": [
        "# You can also save the pickled object into the stage we created earlier for deployment\n",
        "session.file.put(PIPELINE_FILE, \"@ML_HOL_ASSETS\", overwrite=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell21"
      },
      "source": [
        "### Data Exploration\n",
        "\n",
        "Now that we've transformed our features, let's calculate the correlation using Snowpark ML's `correlation()` function between each pair to better understand their relationships.\n",
        "\n",
        "*Note: Snowpark ML's pearson correlation function returns a Pandas DataFrame*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell22"
      },
      "outputs": [],
      "source": [
        "corr_diamonds_df = correlation(df=transformed_diamonds_df)\n",
        "corr_diamonds_df # This is a Pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell23"
      },
      "outputs": [],
      "source": [
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr_diamonds_df, dtype=bool))\n",
        "\n",
        "# Create a heatmap with the features\n",
        "plt.figure(figsize=(7, 7))\n",
        "heatmap = sns.heatmap(corr_diamonds_df, mask=mask, cmap=\"YlGnBu\", annot=True, vmin=-1, vmax=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell24"
      },
      "source": [
        "We note that `CARAT` and `PRICE` are highly correlated, which makes sense! Let's take a look at their relationship a bit closer.\n",
        "\n",
        "*Note: You will have to convert your Snowpark DF to a Pandas DF in order to use matplotlib & seaborn.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell25"
      },
      "outputs": [],
      "source": [
        "# Set up a plot to look at CARAT and PRICE\n",
        "counts = transformed_diamonds_df.to_pandas().groupby(['PRICE', 'CARAT']).size().reset_index(name='Count')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax = sns.scatterplot(data=counts, x='CARAT', y='PRICE', size='Count', markers='o', alpha=(0.1, .25, 0.5, 0.75, 1))\n",
        "ax.grid(axis='y')\n",
        "\n",
        "# The relationship is not linear - it appears exponential which makes sense given the rarity of the large diamonds\n",
        "sns.move_legend(ax, \"upper left\")\n",
        "sns.despine(left=True, bottom=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "cell26"
      },
      "source": [
        "In the next notebook, we will look at how you can train an XGBoost model with the diamonds dataset using the Snowpark ML Modeling API."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}